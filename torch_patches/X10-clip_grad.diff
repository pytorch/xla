diff --git a/torch/nn/utils/clip_grad.py b/torch/nn/utils/clip_grad.py
index ec5f62179e..6b860a9274 100644
--- a/torch/nn/utils/clip_grad.py
+++ b/torch/nn/utils/clip_grad.py
@@ -24,14 +24,18 @@ def clip_grad_norm_(parameters, max_norm, norm_type=2):
     parameters = list(filter(lambda p: p.grad is not None, parameters))
     max_norm = float(max_norm)
     norm_type = float(norm_type)
+    device = parameters[0].device if parameters else torch.device('cpu')
     if norm_type == inf:
         total_norm = max(p.grad.detach().abs().max() for p in parameters)
     else:
-        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]), norm_type)
-    clip_coef = max_norm / (total_norm + 1e-6)
-    if clip_coef < 1:
+        total_norm = torch.zeros([], device=device if parameters else None)
         for p in parameters:
-            p.grad.detach().mul_(clip_coef)
+            param_norm = p.grad.data.norm(norm_type) ** norm_type
+            total_norm.add_(param_norm)
+        total_norm = (total_norm ** (1. / norm_type))
+    clip_coef = torch.tensor(max_norm, device=device) / (total_norm + 1e-6)
+    for p in parameters:
+        p.grad.data.mul_(torch.where(clip_coef < 1, clip_coef, torch.tensor(1., device=device)))
     return total_norm
 
 
