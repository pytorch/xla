commit fdaec2ec9dc9df3c00e2071a95fced233f6a13f8
Author: Davide Libenzi <dlibenzi@google.com>
Date:   Thu Mar 14 16:50:27 2019 -0700

    Enable autograd to recognize the XLA backend as one providing multiple devices.

diff --git a/aten/src/ATen/Context.h b/aten/src/ATen/Context.h
index ff75ba3a7..de828975e 100644
--- a/aten/src/ATen/Context.h
+++ b/aten/src/ATen/Context.h
@@ -12,6 +12,7 @@
 #include <ATen/core/VariableHooksInterface.h>
 #include <ATen/detail/CUDAHooksInterface.h>
 #include <ATen/detail/HIPHooksInterface.h>
+#include <ATen/detail/XLAHooksInterface.h>
 #include <ATen/detail/ComplexHooksInterface.h>
 #include <c10/util/Exception.h>
 
@@ -76,6 +77,9 @@ class CAFFE2_API Context {
   bool hasHIP() const {
     return detail::getHIPHooks().hasHIP();
   }
+  bool hasXLA() const {
+    return detail::getXLAHooks().hasXLA();
+  }
   // defined in header so that getNonVariableType has ability to inline
   // call_once check. getNonVariableType is called fairly frequently
   THCState* lazyInitCUDA() {
@@ -204,7 +208,14 @@ static inline bool hasHIP() {
   return globalContext().hasHIP();
 }
 
+static inline bool hasXLA() {
+  return globalContext().hasXLA();
+}
+
 static inline size_t getNumGPUs() {
+  if (hasXLA()) {
+    return detail::getXLAHooks().getNumDevices();
+  }
   if (hasCUDA()) {
     return detail::getCUDAHooks().getNumGPUs();
   }
diff --git a/aten/src/ATen/detail/XLAHooksInterface.cpp b/aten/src/ATen/detail/XLAHooksInterface.cpp
new file mode 100644
index 000000000..1dc309dcb
--- /dev/null
+++ b/aten/src/ATen/detail/XLAHooksInterface.cpp
@@ -0,0 +1,22 @@
+#include <ATen/detail/XLAHooksInterface.h>
+
+#include <atomic>
+
+namespace at {
+namespace detail {
+namespace {
+
+std::atomic<XLAHooksInterface*> g_xla_hooks(new XLAHooksInterface());
+
+}  // namespace
+
+const XLAHooksInterface& getXLAHooks() {
+  return *g_xla_hooks.load();
+}
+
+void setXLAHooks(XLAHooksInterface* xla_hooks) {
+  g_xla_hooks = xla_hooks;
+}
+
+} // namespace detail
+} // namespace at
diff --git a/aten/src/ATen/detail/XLAHooksInterface.h b/aten/src/ATen/detail/XLAHooksInterface.h
new file mode 100644
index 000000000..3dbf848ac
--- /dev/null
+++ b/aten/src/ATen/detail/XLAHooksInterface.h
@@ -0,0 +1,25 @@
+#pragma once
+
+#include <c10/macros/Export.h>
+
+namespace at {
+
+struct CAFFE2_API XLAHooksInterface {
+  virtual ~XLAHooksInterface() {}
+
+  virtual bool hasXLA() const {
+    return false;
+  }
+
+  virtual int getNumDevices() const {
+    return 0;
+  }
+};
+
+namespace detail {
+
+CAFFE2_API const XLAHooksInterface& getXLAHooks();
+CAFFE2_API void setXLAHooks(XLAHooksInterface* xla_hooks);
+
+} // namespace detail
+} // namespace at
diff --git a/tools/build_pytorch_libs.py b/tools/build_pytorch_libs.py
index c1f08f1cd..de4f7a9b7 100644
--- a/tools/build_pytorch_libs.py
+++ b/tools/build_pytorch_libs.py
@@ -215,7 +215,11 @@ def run_cmake(version,
         if env_var_name.startswith('gh'):
             # github env vars use utf-8, on windows, non-ascii code may
             # cause problem, so encode first
-            my_env[env_var_name] = str(my_env[env_var_name].encode("utf-8"))
+            try:
+                my_env[env_var_name] = str(my_env[env_var_name].encode("utf-8"))
+            except UnicodeDecodeError as e:
+                shex = ':'.join('{:02x}'.format(ord(c)) for c in my_env[env_var_name])
+                sys.stderr.write('Invalid ENV[{}] = {}\n'.format(env_var_name, shex))
     check_call(cmake_args, cwd=build_dir, env=my_env)
 
 
diff --git a/torch/csrc/autograd/engine.cpp b/torch/csrc/autograd/engine.cpp
index 44ab9c535..480f321b0 100644
--- a/torch/csrc/autograd/engine.cpp
+++ b/torch/csrc/autograd/engine.cpp
@@ -220,6 +220,9 @@ auto Engine::thread_init(int device) -> void {
     if (at::hasHIP()) {
       guard.reset_device(at::Device(at::DeviceType::HIP, device));
     }
+    if (at::hasXLA()) {
+      guard.reset_device(at::Device(at::DeviceType::XLA, device));
+    }
   }
   worker_device = device;
   thread_main(nullptr);
@@ -352,7 +355,7 @@ static void validate_outputs(const edge_list& edges, variable_list& grads, const
       ss << metadata.type() << " but got " << grads[i].type();
       AT_ERROR(format_error(ss.str()));
     }
-    const auto output_device = output.is_cuda() ? output.get_device() : -1;
+    const auto output_device = get_tensor_device(output);
     if (output_device != metadata.device()) {
       std::stringstream ss;
       ss << "invalid gradient at index " << i << " - expected device ";
diff --git a/torch/csrc/autograd/input_buffer.cpp b/torch/csrc/autograd/input_buffer.cpp
index 5322bee67..d14eab743 100644
--- a/torch/csrc/autograd/input_buffer.cpp
+++ b/torch/csrc/autograd/input_buffer.cpp
@@ -1,6 +1,7 @@
 #include <torch/csrc/autograd/input_buffer.h>
 
 #include <torch/csrc/autograd/functions/basic_ops.h>
+#include <torch/csrc/autograd/input_metadata.h>
 
 #include <ATen/DeviceGuard.h>
 
@@ -42,8 +43,11 @@ void InputBuffer::add(size_t pos, Variable var) {
 
 auto InputBuffer::device() const -> int {
   for (auto& var : buffer) {
-    if (var.defined() && var.is_cuda()) {
-      return var.get_device();
+    if (var.defined()) {
+      int device = get_tensor_device(var);
+      if (device >= 0) {
+        return device;
+      }
     }
   }
   return -1;
diff --git a/torch/csrc/autograd/input_metadata.h b/torch/csrc/autograd/input_metadata.h
index 678e7a1ed..e0c08a52c 100644
--- a/torch/csrc/autograd/input_metadata.h
+++ b/torch/csrc/autograd/input_metadata.h
@@ -6,6 +6,14 @@
 
 namespace torch { namespace autograd {
 
+inline int get_tensor_device(const at::Tensor& tensor) {
+  if (tensor.is_cuda() ||
+      (tensor.type().backend() == at::Backend::XLA && at::hasXLA())) {
+    return tensor.get_device();
+  }
+  return -1;
+}
+
 /// A tensor's type and shape. Each Function records the required type and
 /// shape of its inputs. If is_valid() is false, then the corresponding input
 /// is not used and may be an undefined tensor.
@@ -16,7 +24,7 @@ struct InputMetadata {
   : type_{&type} , shape_{shape}, device_{device} { }
 
   InputMetadata(const at::Tensor& t)
-  : InputMetadata(t.type(), t.sizes(), t.is_cuda() ? t.get_device() : - 1) { }
+  : InputMetadata(t.type(), t.sizes(), get_tensor_device(t)) { }
 
   bool is_valid() const {
     return type_ != nullptr;
diff --git a/torch/csrc/autograd/variable.cpp b/torch/csrc/autograd/variable.cpp
index 44bd52b72..9b1aa2c79 100644
--- a/torch/csrc/autograd/variable.cpp
+++ b/torch/csrc/autograd/variable.cpp
@@ -164,7 +164,7 @@ void Variable::Impl::set_data(const at::Tensor &new_data) {
   auto prior_accumulator = autograd_meta->grad_accumulator_.lock();
   if (prior_accumulator) {
     const auto prior_device = prior_accumulator->input_metadata(0).device();
-    const auto new_device = new_data.is_cuda() ? new_data.get_device() : -1;
+    const auto new_device = get_tensor_device(new_data);
 
     if (new_data.type() != data_.type() || prior_device != new_device) {
       autograd_meta->grad_accumulator_.reset();
@@ -217,7 +217,7 @@ const std::shared_ptr<Function>& Variable::grad_fn() const {
       fn->add_input_metadata(
         diff_view_meta->base_.type()
       , sizes() // Note: sizes(), not base_.sizes(), is intentional
-      , diff_view_meta->base_.is_cuda() ? diff_view_meta->base_.get_device() : -1);
+      , get_tensor_device(diff_view_meta->base_));
       diff_view_meta->grad_fn_ = std::move(fn);
       diff_view_meta->attr_version = current_version;
     }
