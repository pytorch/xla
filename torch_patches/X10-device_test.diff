diff --git a/torch/testing/_internal/common_device_type.py b/torch/testing/_internal/common_device_type.py
index a23696c29c..d7eb93373b 100644
--- a/torch/testing/_internal/common_device_type.py
+++ b/torch/testing/_internal/common_device_type.py
@@ -3,6 +3,7 @@ import threading
 from functools import wraps
 import unittest
 import torch
+import copy
 from torch.testing._internal.common_utils import TestCase, TEST_WITH_ROCM, TEST_MKL, \
     skipCUDANonDefaultStreamIf

@@ -191,7 +192,7 @@ class DeviceTypeTestBase(TestCase):
                     # Sets precision and runs test
                     # Note: precision is reset after the test is run
                     guard_precision = self.precision
-                    try :
+                    try:
                         self.precision = self._get_precision_override(test, dtype)
                         result = test(self, device_arg, dtype)
                     finally:
@@ -242,10 +243,103 @@ class CUDATestBase(DeviceTypeTestBase):
         cls.primary_device = 'cuda:{0}'.format(torch.cuda.current_device())


+import torch_xla
+import torch_xla.core.xla_model as xm
+
+# Acquires XLA test metadata
+import os
+from runpy import run_path
+assert 'XLA_TEST_DIR' in os.environ, "XLA_TEST_DIR environment variable must be set"
+xla_test_path = os.environ['XLA_TEST_DIR']
+xla_meta_path = xla_test_path + "/torch_test_meta.py"
+xla_meta = run_path(xla_meta_path)
+assert xla_meta, "XLA metadata not found!"
+disabled_torch_tests = xla_meta.get('disabled_torch_tests', None)
+torch_test_precisions = xla_meta.get('torch_test_precisions', None)
+DEFAULT_FLOATING_PRECISION = xla_meta.get('DEFAULT_FLOATING_PRECISION', None)
+assert disabled_torch_tests is not None, "XLA tests not found!"
+assert torch_test_precisions is not None, "XLA test precisions not found!"
+assert DEFAULT_FLOATING_PRECISION is not None, "DEFAULT_FLOATING_PRECISION not found!"
+
+class XLATestBase(DeviceTypeTestBase):
+    device_type = 'xla'
+    unsupported_dtypes = {torch.half}
+    precision = DEFAULT_FLOATING_PRECISION
+
+    # Overrides to instantiate tests that are known to run quickly
+    # and correctly on XLA.
+    @classmethod
+    def instantiate_test(cls, name, test):
+        test_name = name + "_" + cls.device_type
+        if test_name in disabled_torch_tests or test.__name__ in disabled_torch_tests:
+            assert not hasattr(cls, test_name), "Redefinition of test {0}".format(test_name)
+
+            @wraps(test)
+            def disallowed_test(self, test=test):
+                raise unittest.SkipTest("skipped on XLA")
+                return test(self, cls.device_type)
+
+            setattr(cls, test_name, disallowed_test)
+        else:  # Test is allowed
+            dtypes = cls._get_dtypes(test)
+            if dtypes is None:  # Tests without dtype variants are instantiated as usual
+                super().instantiate_test(name, test)
+            else:  # Tests with dtype variants have unsupported dtypes skipped
+                skipped_dtypes = [dtype for dtype in dtypes if dtype in cls.unsupported_dtypes]
+
+                # Skips unsupported dtypes
+                for dtype in skipped_dtypes:
+                    dtype_str = str(dtype).split('.')[1]
+                    dtype_test_name = test_name + "_" + dtype_str
+                    reason = "XLA does not support dtype {0}".format(str(dtype))
+
+                    @wraps(test)
+                    def skipped_test(self, *args, reason=reason, **kwargs):
+                        raise unittest.SkipTest(reason)
+
+                    assert not hasattr(cls, dtype_test_name), "Redefinition of test {0}".format(dtype_test_name)
+                    setattr(cls, dtype_test_name, skipped_test)
+
+                # Instantiates supported dtypes
+                xla_dtypes = [dtype for dtype in dtypes if dtype not in cls.unsupported_dtypes]
+
+                # Sets default precision for floating types to bfloat16 precision
+                if not hasattr(test, 'precision_overrides'):
+                    test.precision_overrides = {}
+
+                floating_precision = torch_test_precisions.get(test_name, torch_test_precisions.get(test.__name__, DEFAULT_FLOATING_PRECISION))
+                test.precision_overrides[torch.float] = max(test.precision_overrides.get(torch.float, 0), floating_precision)
+                test.precision_overrides[torch.double] = max(test.precision_overrides.get(torch.double, 0), floating_precision)
+                test.precision_overrides[torch.bfloat16] = max(test.precision_overrides.get(torch.bfloat16, 0), floating_precision)
+
+                if len(xla_dtypes) != 0:
+                    test.dtypes[cls.device_type] = xla_dtypes
+                    super().instantiate_test(name, test)
+
+    @classmethod
+    def get_primary_device(cls):
+        return cls.primary_device
+
+    @classmethod
+    def setUpClass(cls):
+        # Sets the primary test device to the xla_device (CPU or TPU)
+        cls.primary_device = str(xm.xla_device())
+        torch_xla._XLAC._xla_set_use_full_mat_mul_precision(use_full_mat_mul_precision=True)
+
+    # Overrides assertEqual to popular custom precision
+    def assertEqual(self, x, y, prec=None, message='', allow_inf=False):
+        if prec is None:
+            prec = self.precision
+        else:
+            prec = max(self.precision, prec)
+        return DeviceTypeTestBase.assertEqual(self, x, y, prec, message, allow_inf)
+
+
 # Adds available device-type-specific test base classes
 device_type_test_bases.append(CPUTestBase)
 if torch.cuda.is_available():
     device_type_test_bases.append(CUDATestBase)
+device_type_test_bases.append(XLATestBase)


 # Adds 'instantiated' device-specific test cases to the given scope.
@@ -289,7 +383,7 @@ def instantiate_device_type_tests(generic_test_class, scope, except_for=None):
                 assert inspect.isfunction(test), "Couldn't extract function from '{0}'".format(name)

                 # Instantiates the device-specific tests
-                device_type_test_class.instantiate_test(name, test)
+                device_type_test_class.instantiate_test(name, copy.deepcopy(test))
             else:  # Ports non-test member
                 assert not hasattr(device_type_test_class, name), "Redefinition of non-test member {0}".format(name)

