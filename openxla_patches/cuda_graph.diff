Revert 8aa20d47ea5c863b9edeb072489ffb7c38b6e8eb
diff --git a/xla/service/gpu/runtime/BUILD b/xla/service/gpu/runtime/BUILD
index c37013f4b5c..f9fb8e17b42 100644
--- a/xla/service/gpu/runtime/BUILD
+++ b/xla/service/gpu/runtime/BUILD
@@ -375,9 +375,7 @@ cc_library(
         "//xla/stream_executor",
         "@com_google_absl//absl/container:node_hash_map",
         "@com_google_absl//absl/synchronization",
-    ] + if_cuda_is_configured([
-        "//xla/stream_executor/cuda:cuda_graph",
-    ]),
+    ],
 )
 
 cc_library(
diff --git a/xla/service/gpu/runtime/executable.cc b/xla/service/gpu/runtime/executable.cc
index 3cc546f7d5e..f8f7018b859 100644
--- a/xla/service/gpu/runtime/executable.cc
+++ b/xla/service/gpu/runtime/executable.cc
@@ -371,6 +371,7 @@ Status GpuRuntimeExecutable::Execute(
       graph_instances_(executor)->snapshot();
   CapturedFunctionExecutionCount::Snapshot execution_count =
       captured_function_counts_(executor)->snapshot();
+  CapturingCudaGraph capturing_cuda_graph(false);
 #endif  // GOOGLE_CUDA
 
   // Kernels in concurrent regions should be launched on borrowed stream, so
@@ -398,7 +399,7 @@ Status GpuRuntimeExecutable::Execute(
       &collectives_, &fft_plans, &send_recv_events, &gpu_lock,
 #if GOOGLE_CUDA
       // Auxiliary data that is available only if compiled with CUDA support.
-      &matmul_plans, &graph_instances, &execution_count,
+      &matmul_plans, &graph_instances, &execution_count, &capturing_cuda_graph,
 #endif  // GOOGLE_CUDA
       &concurrent_region_status,
       // Null pointer will be interpreted as an absence of async collectives
diff --git a/xla/service/gpu/runtime/graph_launch.cc b/xla/service/gpu/runtime/graph_launch.cc
index c70e39a5a91..0391a41b6ba 100644
--- a/xla/service/gpu/runtime/graph_launch.cc
+++ b/xla/service/gpu/runtime/graph_launch.cc
@@ -279,11 +279,18 @@ static absl::Status LaunchGraph(
   // Compute the hash of the buffer arguments.
   size_t ptrs_hash = absl::HashOf(RemainingArgsPtrs{fwd_args, temp_buffer});
 
+  CapturingCudaGraph not_capturing(false);
+  CapturingCudaGraph capturing(true);
   // Forwards user data required for launching kernels.
-  auto user_data = [&] {
+  auto user_data_no_capture = [&] {
     return CustomCall::UserData(run_options, debug_options, ptx, cubin,
                                 temp_buffer, kernels, convs, executable,
-                                gemm_config, gpu_lock);
+                                gemm_config, gpu_lock, &not_capturing);
+  };
+  auto user_data_capture = [&] {
+    return CustomCall::UserData(run_options, debug_options, ptx, cubin,
+                                temp_buffer, kernels, convs, executable,
+                                gemm_config, gpu_lock, &capturing);
   };
 
   absl::StatusOr<std::unique_ptr<std::atomic<uint64_t>>*> get_count =
@@ -298,15 +305,16 @@ static absl::Status LaunchGraph(
       debug_options->xla_gpu_cuda_graph_instantiation_threshold();
   if (count < instantiation_threshold) {
     // Run captured graph directly.
-    absl::Status result = RunGraphWithoutCapture(run_options, function_ref,
-                                                 fwd_args, user_data());
+    absl::Status result = RunGraphWithoutCapture(
+        run_options, function_ref, fwd_args, user_data_no_capture());
     if (!result.ok()) return result;
     return absl::OkStatus();
   }
 
   absl::StatusOr<GraphInstance*> instance = instances->GetOrCreate(
       capture.ordinal, [&]() -> absl::StatusOr<GraphInstance> {
-        auto g = CaptureGraph(run_options, function_ref, fwd_args, user_data());
+        auto g = CaptureGraph(run_options, function_ref, fwd_args,
+                              user_data_capture());
         if (!g.ok()) return g.status();
 
         auto e = se::gpu::InstantiateCudaGraph(std::move(*g));
@@ -330,7 +338,8 @@ static absl::Status LaunchGraph(
   VLOG(3) << "Update cached graph instance";
 
   // Capture CUDA graph by running capture function.
-  auto g = CaptureGraph(run_options, function_ref, fwd_args, user_data());
+  auto g =
+      CaptureGraph(run_options, function_ref, fwd_args, user_data_capture());
   if (!g.ok()) return g.status();
 
   // Update captured graph executable.
diff --git a/xla/service/gpu/runtime/kernel_launch.cc b/xla/service/gpu/runtime/kernel_launch.cc
index ceef00ae284..3f7606bebdc 100644
--- a/xla/service/gpu/runtime/kernel_launch.cc
+++ b/xla/service/gpu/runtime/kernel_launch.cc
@@ -30,10 +30,6 @@ limitations under the License.
 #include "xla/service/service_executable_run_options.h"
 #include "xla/stream_executor/kernel.h"
 
-#if GOOGLE_CUDA
-#include "xla/stream_executor/cuda/cuda_graph.h"
-#endif  // #if GOOGLE_CUDA
-
 namespace xla {
 namespace gpu {
 
@@ -54,6 +50,9 @@ StreamExecutorKernels* GpuExecutableKernels::operator()(
 static absl::Status LaunchImpl(
     const ServiceExecutableRunOptions* run_options, const std::string* ptx,
     const std::vector<uint8_t>* cubin, se::DeviceMemoryBase* temp_buffer,
+#if GOOGLE_CUDA
+    CapturingCudaGraph* capturing_cuda_graph,
+#endif
     State<std::unique_ptr<se::KernelBase>> device_kernel,
     int32_t shared_memory_bytes, int32_t grid_size_x, int32_t grid_size_y,
     int32_t grid_size_z, int32_t block_size_x, int32_t block_size_y,
@@ -79,9 +78,7 @@ static absl::Status LaunchImpl(
   assert((**kernel)->name() == name && "unexpected loaded kernel");
 
 #if GOOGLE_CUDA
-  absl::StatusOr<bool> is_capturing = se::gpu::IsStreamCapturing(stream);
-  if (!is_capturing.ok()) return is_capturing.status();
-  if (is_capturing.value()) {
+  if (capturing_cuda_graph->capturing()) {
     VLOG(3) << "Launching " << (**kernel)->name()
             << "during CUDA graph capture";
   } else {
@@ -127,6 +124,9 @@ XLA_RUNTIME_DEFINE_CUSTOM_CALL(
         .UserData<const std::string*>()
         .UserData<const std::vector<uint8_t>*>()
         .UserData<se::DeviceMemoryBase*>()
+#if GOOGLE_CUDA
+        .UserData<CapturingCudaGraph*>()
+#endif
         .State<std::unique_ptr<se::KernelBase>>("uid")
         .Arg<int32_t>()   // shared_memory_bytes
         .Arg<int32_t>()   // grid_size_x
diff --git a/xla/service/gpu/runtime/support.h b/xla/service/gpu/runtime/support.h
index 4f5c722db9e..b5026b95837 100644
--- a/xla/service/gpu/runtime/support.h
+++ b/xla/service/gpu/runtime/support.h
@@ -116,6 +116,15 @@ inline void PopulateDotDimsAttrEncoding(
           .Add("rhs_contract", &DotDimsAttr::getRhsContractingDimensions));
 }
 
+class CapturingCudaGraph {
+ public:
+  explicit CapturingCudaGraph(bool capturing) : capturing_(capturing) {}
+  bool capturing() { return capturing_; }
+
+ private:
+  bool capturing_ = false;
+};
+
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/stream_executor/cuda/cuda_graph.cc b/xla/stream_executor/cuda/cuda_graph.cc
index 5a0b9e099ea..282e89d30bf 100644
--- a/xla/stream_executor/cuda/cuda_graph.cc
+++ b/xla/stream_executor/cuda/cuda_graph.cc
@@ -175,17 +175,5 @@ tsl::StatusOr<OwnedCudaGraphExec> InstantiateCudaGraph(OwnedCudaGraph graph) {
   return OwnedCudaGraphExec(exec);
 }
 
-tsl::StatusOr<bool> IsStreamCapturing(stream_executor::Stream* stream) {
-  cudaStreamCaptureStatus capture_status;
-  cudaError_t err = cudaStreamIsCapturing(
-      stream_executor::gpu::AsGpuStreamValue(stream), &capture_status);
-  if (err != cudaSuccess) {
-    return InternalError("Failed to get stream's capture status: %s",
-                         cudaGetErrorString(err));
-  }
-
-  return capture_status == cudaStreamCaptureStatusActive;
-}
-
 }  // namespace gpu
 }  // namespace stream_executor
diff --git a/xla/stream_executor/cuda/cuda_graph.h b/xla/stream_executor/cuda/cuda_graph.h
index 3480e96c095..f790b1cff3b 100644
--- a/xla/stream_executor/cuda/cuda_graph.h
+++ b/xla/stream_executor/cuda/cuda_graph.h
@@ -85,9 +85,6 @@ tsl::StatusOr<OwnedCudaGraph> CaptureCudaGraph(
 // Instantiates a captured cuda graph instance into a cuda graph executable.
 tsl::StatusOr<OwnedCudaGraphExec> InstantiateCudaGraph(OwnedCudaGraph graph);
 
-// Returns true if the stream is in graph capture mode
-tsl::StatusOr<bool> IsStreamCapturing(stream_executor ::Stream* stream);
-
 }  // namespace gpu
 }  // namespace stream_executor
 