


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>PyTorch on XLA Devices &mdash; PyTorch/XLA master documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.6 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">PyTorch on XLA Devices</a><ul>
<li><a class="reference internal" href="#creating-an-xla-tensor">Creating an XLA Tensor</a></li>
<li><a class="reference internal" href="#xla-tensors-are-pytorch-tensors">XLA Tensors are PyTorch Tensors</a></li>
<li><a class="reference internal" href="#running-models-on-xla-devices">Running Models on XLA Devices</a><ul>
<li><a class="reference internal" href="#running-on-a-single-xla-device">Running on a Single XLA Device</a></li>
<li><a class="reference internal" href="#running-on-multiple-xla-devices-with-multiprocessing">Running on Multiple XLA Devices with MultiProcessing</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id1">XLA Tensor Deep Dive</a><ul>
<li><a class="reference internal" href="#xla-tensors-are-lazy">XLA Tensors are Lazy</a></li>
<li><a class="reference internal" href="#xla-tensors-and-bfloat16">XLA Tensors and bFloat16</a></li>
<li><a class="reference internal" href="#memory-layout">Memory Layout</a></li>
<li><a class="reference internal" href="#moving-xla-tensors-to-and-from-the-cpu">Moving XLA Tensors to and from the CPU</a></li>
<li><a class="reference internal" href="#saving-and-loading-xla-tensors">Saving and Loading XLA Tensors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#further-reading">Further Reading</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch-xla-api">PyTorch/XLA API</a><ul>
<li><a class="reference internal" href="#module-torch_xla.core.xla_model">xla_model</a></li>
<li><a class="reference internal" href="#module-torch_xla.distributed.parallel_loader">distributed</a></li>
<li><a class="reference internal" href="#module-torch_xla.utils.tf_record_reader">utils</a></li>
</ul>
</li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="#">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>PyTorch on XLA Devices</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="pytorch-on-xla-devices">
<h1>PyTorch on XLA Devices<a class="headerlink" href="#pytorch-on-xla-devices" title="Permalink to this headline">¶</a></h1>
<p>PyTorch runs on XLA devices, like TPUs, with the
<a class="reference external" href="https://github.com/pytorch/xla/">torch_xla package</a>. This document describes
how to run your models on these devices.</p>
<div class="section" id="creating-an-xla-tensor">
<h2>Creating an XLA Tensor<a class="headerlink" href="#creating-an-xla-tensor" title="Permalink to this headline">¶</a></h2>
<p>PyTorch/XLA adds a new <code class="docutils literal notranslate"><span class="pre">xla</span></code> device type to PyTorch. This device type works just
like other PyTorch device types. For example, here’s how to create and
print an XLA tensor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<p>This code should look familiar. PyTorch/XLA uses the same interface as regular
PyTorch with a few additions. Importing <code class="docutils literal notranslate"><span class="pre">torch_xla</span></code> initializes PyTorch/XLA, and
<code class="docutils literal notranslate"><span class="pre">xm.xla_device()</span></code> returns the current XLA device. This may be a CPU or TPU
depending on your environment.</p>
</div>
<div class="section" id="xla-tensors-are-pytorch-tensors">
<h2>XLA Tensors are PyTorch Tensors<a class="headerlink" href="#xla-tensors-are-pytorch-tensors" title="Permalink to this headline">¶</a></h2>
<p>PyTorch operations can be performed on XLA tensors just like CPU or CUDA tensors.</p>
<p>For example, XLA tensors can be added together:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t0</span> <span class="o">+</span> <span class="n">t1</span><span class="p">)</span>
</pre></div>
</div>
<p>Or matrix multiplied:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">t1</span><span class="p">))</span>
</pre></div>
</div>
<p>Or used with neural network modules:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">l_out</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">l_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">l_out</span><span class="p">)</span>
</pre></div>
</div>
<p>Like other device types, XLA tensors only work with other XLA tensors on the
same device. So code like</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">l_out</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">l_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">l_out</span><span class="p">)</span>
<span class="c1"># Input tensor is not an XLA tensor: torch.FloatTensor</span>
</pre></div>
</div>
<p>will throw an error since the torch.nn.Linear module is on the CPU.</p>
</div>
<div class="section" id="running-models-on-xla-devices">
<h2>Running Models on XLA Devices<a class="headerlink" href="#running-models-on-xla-devices" title="Permalink to this headline">¶</a></h2>
<p>Building a new PyTorch network or converting an existing one to run on XLA
devices requires only a few lines of XLA-specific code. The following snippets
highlight these lines when running on a single device and multiple devices with XLA
multiprocessing.</p>
<div class="section" id="running-on-a-single-xla-device">
<h3>Running on a Single XLA Device<a class="headerlink" href="#running-on-a-single-xla-device" title="Permalink to this headline">¶</a></h3>
<p>The following snippet shows a network training on a single XLA device:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
  <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="n">xm</span><span class="o">.</span><span class="n">optimizer_step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">barrier</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>This snippet highlights how easy it is to switch your model to run on XLA. The
model definition, dataloader, optimizer and training loop can work on any device.
The only XLA-specific code is a couple lines that acquire the XLA device and
step the optimizer with a <span class="raw-html-m2r"><b>barrier</b></span>. Calling
<code class="docutils literal notranslate"><span class="pre">xm.optimizer_step(optimizer,</span> <span class="pre">barrier=True)</span></code> at the end of each training
iteration causes XLA to execute its current graph and update the model’s
parameters. See <a class="reference external" href="#xla-tensor-deep-dive">XLA Tensor Deep Dive</a> for more on
how XLA creates graphs and runs operations.</p>
</div>
<div class="section" id="running-on-multiple-xla-devices-with-multiprocessing">
<h3>Running on Multiple XLA Devices with MultiProcessing<a class="headerlink" href="#running-on-multiple-xla-devices-with-multiprocessing" title="Permalink to this headline">¶</a></h3>
<p>PyTorch/XLA makes it easy to accelerate training by running on multiple XLA
devices. The following snippet shows how:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.parallel_loader</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_multiprocessing</span> <span class="k">as</span> <span class="nn">xmp</span>

<span class="k">def</span> <span class="nf">_mp_fn</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
  <span class="n">para_loader</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">ParallelLoader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="p">[</span><span class="n">device</span><span class="p">])</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">para_loader</span><span class="o">.</span><span class="n">per_device_loader</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">xm</span><span class="o">.</span><span class="n">optimizer_step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="n">xmp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">_mp_fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">())</span>
</pre></div>
</div>
<p>There are three differences between this multidevice snippet and the previous
single device snippet:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">xmp.spawn()</span></code> creates the processes that each run an XLA device.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ParallelLoader</span></code> loads the training data onto each device.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">xm.optimizer_step(optimizer)</span></code> no longer needs a barrier. ParallelLoader
automatically creates an XLA barrier that evalutes the graph.</p></li>
</ul>
<p>The model definition, optimizer definition and training loop remain the same.</p>
<blockquote>
<div><p><strong>NOTE:</strong> It is important to note that, when using multi-processing, the user can start
retrieving and accessing XLA devices only from within the target function of
<code class="docutils literal notranslate"><span class="pre">xmp.spawn()</span></code> (or any function which has <code class="docutils literal notranslate"><span class="pre">xmp.spawn()</span></code> as parent in the call
stack).</p>
</div></blockquote>
<p>See the
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_train_mp_mnist.py">full multiprocessing example</a>
for more on training a network on multiple XLA devices with multiprocessing.</p>
</div>
</div>
<div class="section" id="id1">
<h2>XLA Tensor Deep Dive<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Using XLA tensors and devices requires changing only a few lines of code. But
even though XLA tensors act a lot like CPU and CUDA tensors their internals are
different. This section describes what makes XLA tensors unique.</p>
<div class="section" id="xla-tensors-are-lazy">
<h3>XLA Tensors are Lazy<a class="headerlink" href="#xla-tensors-are-lazy" title="Permalink to this headline">¶</a></h3>
<p>CPU and CUDA tensors launch operations immediately or <span class="raw-html-m2r"><b>eagerly</b></span>. XLA tensors,
on the other hand, are <span class="raw-html-m2r"><b>lazy</b></span>. They record operations in a graph until the
results are needed. Deferring execution like this lets XLA optimize it. A graph
of multiple separate operations might be fused into a single optimized
operation, for example.</p>
<p>Lazy execution is generally invisible to the caller. PyTorch/XLA automatically
constructs the graphs, sends them to XLA devices, and synchronizes when
copying data between an XLA device and the CPU. Inserting a barrier when
taking an optimizer step explicitly synchronizes the CPU and the XLA device.</p>
</div>
<div class="section" id="xla-tensors-and-bfloat16">
<h3>XLA Tensors and bFloat16<a class="headerlink" href="#xla-tensors-and-bfloat16" title="Permalink to this headline">¶</a></h3>
<p>PyTorch/XLA can use the
<a class="reference external" href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16</a>
datatype when running on TPUs. In fact, PyTorch/XLA handles float types
(<code class="docutils literal notranslate"><span class="pre">torch.float</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.double</span></code>) differently on TPUs. This behavior is
controlled by the <code class="docutils literal notranslate"><span class="pre">XLA_USE_BF16</span></code> environment variable:</p>
<ul class="simple">
<li><p>By default both <code class="docutils literal notranslate"><span class="pre">torch.float</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.double</span></code> are
<code class="docutils literal notranslate"><span class="pre">torch.float</span></code> on TPUs.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">XLA_USE_BF16</span></code> is set, then <code class="docutils literal notranslate"><span class="pre">torch.float</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.double</span></code> are both
<code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> on TPUs.</p></li>
<li><p>If a PyTorch tensor has <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code> data type, this will be directly
mapped to the TPU <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> (XLA <code class="docutils literal notranslate"><span class="pre">BF16</span></code> primitive type).</p></li>
</ul>
<p>XLA tensors on TPUs will always report their PyTorch datatype regardless of
the actual datatype they’re using. This conversion is automatic and opaque.
If an XLA tensor on a TPU is moved back to the CPU it will be converted
from its actual datatype to its PyTorch datatype.</p>
</div>
<div class="section" id="memory-layout">
<h3>Memory Layout<a class="headerlink" href="#memory-layout" title="Permalink to this headline">¶</a></h3>
<p>The internal data representation of XLA tensors is opaque to the user. They
do not expose their storage and they always appear to be contiguous, unlike
CPU and CUDA tensors. This allows XLA to adjust a tensor’s memory layout for
better performance.</p>
</div>
<div class="section" id="moving-xla-tensors-to-and-from-the-cpu">
<h3>Moving XLA Tensors to and from the CPU<a class="headerlink" href="#moving-xla-tensors-to-and-from-the-cpu" title="Permalink to this headline">¶</a></h3>
<p>XLA tensors can be moved from the CPU to an XLA device and from an XLA device
to the CPU. If a view is moved then the data its viewing is copied to the
other device and the view relationship is not preserved. Put another way,
once data is copied to another device it has no relationship with its
previous device or any tensors on it.</p>
</div>
<div class="section" id="saving-and-loading-xla-tensors">
<h3>Saving and Loading XLA Tensors<a class="headerlink" href="#saving-and-loading-xla-tensors" title="Permalink to this headline">¶</a></h3>
<p>XLA tensors should be moved to the CPU before saving, as in the following
snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">tensors</span> <span class="o">=</span> <span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">t1</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>

<span class="n">tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>This lets you put the loaded tensors on any available device.</p>
<p>Per the above note on moving XLA tensors to the CPU, care must be taken when
working with views. Instead of saving views it’s recommended that you recreate
them after the tensors have been loaded and moved to their destination device(s).</p>
<p>A utility API is provided to save data by taking care of previously moving it
to CPU:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">xm</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
<p>In case of multple devices, the above API will only save the data for the master
device ordinal (0).</p>
<p>Directly saving XLA tensors is possible but not recommended. XLA
tensors are always loaded back to the device they were saved from, and if
that device is unavailable the load will fail. PyTorch/XLA, like all of PyTorch,
is under active development and this behavior may change in the future.</p>
</div>
</div>
<div class="section" id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h2>
<p>Additional documentation is available at the
<a class="reference external" href="https://github.com/pytorch/xla/">PyTorch/XLA repo</a>. More examples of running
networks on TPUs are available
<a class="reference external" href="https://github.com/pytorch-tpu/examples">here</a>.</p>
</div>
</div>
<div class="section" id="pytorch-xla-api">
<h1>PyTorch/XLA API<a class="headerlink" href="#pytorch-xla-api" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-torch_xla.core.xla_model">
<span id="xla-model"></span><h2>xla_model<a class="headerlink" href="#module-torch_xla.core.xla_model" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch_xla.core.xla_model.xla_device">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">xla_device</code><span class="sig-paren">(</span><em class="sig-param">n=None</em>, <em class="sig-param">devkind=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#xla_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.xla_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a given instance of an XLA device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The specific instance (ordinal) to be returned. If
specified, the specific XLA device instance will be returned. Otherwise
the first device of <cite>devkind</cite> will be returned.</p></li>
<li><p><strong>devkind</strong> (<em>string...</em><em>, </em><em>optional</em>) – If specified, one of <cite>TPU</cite>, <cite>GPU</cite> or <cite>CPU</cite>
(the ‘GPU’ XLA device is currently not implemented).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>torch.device</cite> with the requested instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.get_xla_supported_devices">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">get_xla_supported_devices</code><span class="sig-paren">(</span><em class="sig-param">devkind=None</em>, <em class="sig-param">max_devices=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_xla_supported_devices"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_xla_supported_devices" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of supported devices of a given kind.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>devkind</strong> (<em>string...</em><em>, </em><em>optional</em>) – If specified, one of <cite>TPU</cite>, <cite>GPU</cite> or <cite>CPU</cite>
(the ‘GPU’ XLA device is currently not implemented).</p></li>
<li><p><strong>max_devices</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The maximum number of devices to be returned of
that kind.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The list of device strings.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.xla_device_hw">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">xla_device_hw</code><span class="sig-paren">(</span><em class="sig-param">device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#xla_device_hw"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.xla_device_hw" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the hardware type of the given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>string</em><em> or </em><em>torch.device</em>) – The xla device that will be mapped to the
real device.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A string representation of the hardware type (<cite>CPU</cite>, <cite>TPU</cite>, <cite>GPU</cite>) of the
given device.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.get_ordinal">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">get_ordinal</code><span class="sig-paren">(</span><em class="sig-param">defval=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_ordinal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_ordinal" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the replication ordinal of the current process.</p>
<p>The ordinals range from 0 to <cite>xrt_world_size()</cite> minus 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>defval</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The default value to be returned in case there is no
replication information available.
Default: 0</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The replication ordinal of the current process.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.get_local_ordinal">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">get_local_ordinal</code><span class="sig-paren">(</span><em class="sig-param">defval=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_local_ordinal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_local_ordinal" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the replication local ordinal of the current process.</p>
<p>The local ordinals range from 0 to the number of local devices minus 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>defval</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The default value to be returned in case there is no
replication information available.
Default: 0</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The replication local ordinal of the current process.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.is_master_ordinal">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">is_master_ordinal</code><span class="sig-paren">(</span><em class="sig-param">local=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#is_master_ordinal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.is_master_ordinal" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks whether the current process is the master ordinal (0).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>local</strong> (<em>bool</em>) – Whether the local or global master ordinal should be checked.
In case of multi-host replication, there is only one global master ordinal
(host 0, device 0), while there are NUM_HOSTS local master ordinals.
Default: True</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A boolean indicating whether the current process is the master ordinal.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.xrt_world_size">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">xrt_world_size</code><span class="sig-paren">(</span><em class="sig-param">defval=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#xrt_world_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.xrt_world_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the number of devices which is taking part of the replication.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>defval</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The default value to be returned in case there is no
replication information available.
Default: 1</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The number of devices which is taking part of the replication.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.all_reduce">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">all_reduce</code><span class="sig-paren">(</span><em class="sig-param">reduce_type</em>, <em class="sig-param">inputs</em>, <em class="sig-param">scale=1.0</em>, <em class="sig-param">groups=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#all_reduce"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.all_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an inplace reduce operation on the input tensor(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>reduce_type</strong> (<em>string</em>) – One of <code class="docutils literal notranslate"><span class="pre">REDUCE_SUM</span></code>, <code class="docutils literal notranslate"><span class="pre">REDUCE_MUL</span></code>, <code class="docutils literal notranslate"><span class="pre">REDUCE_AND</span></code>,
<code class="docutils literal notranslate"><span class="pre">REDUCE_OR</span></code>, <code class="docutils literal notranslate"><span class="pre">REDUCE_MIN</span></code> and <code class="docutils literal notranslate"><span class="pre">REDUCE_MIN</span></code>.</p></li>
<li><p><strong>inputs</strong> – Either a single <cite>torch.Tensor</cite> or a list of <cite>torch.Tensor</cite> to
perform the all reduce op to.</p></li>
<li><p><strong>scale</strong> (<em>python:float</em>) – A default scaling value to be applied after the reduce.
Default: 1.0</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_reduce()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>If a single <cite>torch.Tensor</cite> is passed, the return value is a <cite>torch.Tensor</cite>
holding the reduced value (across the replicas). If a list/tuple is passed,
this function performs an inplace all-reduce op on the input tensors, and
returns the list/tuple itself.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.all_gather">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">all_gather</code><span class="sig-paren">(</span><em class="sig-param">value</em>, <em class="sig-param">dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#all_gather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an all-gather operation along a given dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – The input tensor.</p></li>
<li><p><strong>dim</strong> (<em>python:int</em>) – The gather dimension.
Default: 0</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor which has, in the <code class="docutils literal notranslate"><span class="pre">dim</span></code> dimension, all the values from the
participating replicas.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.all_to_all">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">all_to_all</code><span class="sig-paren">(</span><em class="sig-param">value</em>, <em class="sig-param">split_dimension</em>, <em class="sig-param">concat_dimension</em>, <em class="sig-param">split_count</em>, <em class="sig-param">groups=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#all_to_all"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.all_to_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an XLA <cite>AllToAll()</cite> operation on the input tensor.</p>
<p>See: <a class="reference external" href="https://www.tensorflow.org/xla/operation_semantics#alltoall">https://www.tensorflow.org/xla/operation_semantics#alltoall</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – The input tensor.</p></li>
<li><p><strong>split_dimension</strong> (<em>python:int</em>) – The dimension upon which the split should happen.</p></li>
<li><p><strong>concat_dimension</strong> (<em>python:int</em>) – The dimension upon which the concat should happen.</p></li>
<li><p><strong>split_count</strong> (<em>python:int</em>) – The split count.</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_reduce()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The result <cite>torch.Tensor</cite> of the <cite>all_to_all()</cite> operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.collective_permute">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">collective_permute</code><span class="sig-paren">(</span><em class="sig-param">value</em>, <em class="sig-param">pairs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#collective_permute"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.collective_permute" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a XLA <cite>CollectivePermute()</cite> operation on the input tensor.</p>
<p>See: <a class="reference external" href="https://www.tensorflow.org/xla/operation_semantics#collectivepermute">https://www.tensorflow.org/xla/operation_semantics#collectivepermute</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – The input tensor.</p></li>
<li><p><strong>pairs</strong> (<em>list</em>) – <p>A list of (source_replica_id, target_replica_id) pairs,
representing the sender and receiver for the <cite>collective_permute()</cite>
operation. Example: <cite>[[0, 1], [1, 2], [2, 0]]</cite> defines three pairs. The</p>
<blockquote>
<div><p>tensor will be send from replidca 0 to replidca 1, replidca 1 to
replidca 2, and replidca 2 to replidca 0.</p>
</div></blockquote>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The result <cite>torch.Tensor</cite> of the <cite>collective_permute()</cite> operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.add_step_closure">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">add_step_closure</code><span class="sig-paren">(</span><em class="sig-param">closure</em>, <em class="sig-param">args=()</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#add_step_closure"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.add_step_closure" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a closure to the list of the ones to be run at the end of the step.</p>
<p>Many times during model training there is the need to print/report (print to
console, post to tensorboard, etc…) information which require the content of
intermediary tensors to be inspected.
Inspecting different tensors content in different points of the model code
requires many executions and typically causes performance issues.
Adding a step closure will ensure that it will be run after the barrier, when
all the live tensors will be already materialized to device data.
Live tensors which will include the ones captured by the closure arguments.
So using <cite>add_step_closure()</cite> will ensure a single execution will be
performed, even when multiple closures are queued, requiring multiple tensors
to be inspected.
Step closures will be run sequentially in the order they have been queued.
Note that even though using this API the execution will be optimized, it is
advised to throttle the printing/reporting events once every N steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>closure</strong> (<em>callable</em>) – The function to be called.</p></li>
<li><p><strong>args</strong> (<em>tuple</em>) – The arguments to be passed to the closure.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.wait_device_ops">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">wait_device_ops</code><span class="sig-paren">(</span><em class="sig-param">devices=[]</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#wait_device_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.wait_device_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for all the async operations on the given devices to complete.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>devices</strong> (<em>string...</em><em>, </em><em>optional</em>) – The devices whose async ops need to be waited
for. If empty, all the local devices will be waited for.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.optimizer_step">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">optimizer_step</code><span class="sig-paren">(</span><em class="sig-param">optimizer</em>, <em class="sig-param">barrier=False</em>, <em class="sig-param">optimizer_args={}</em>, <em class="sig-param">groups=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#optimizer_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the provided optimizer step and issue the XLA device step computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Optimizer</span></code>) – The <cite>torch.Optimizer</cite> instance whose
<cite>step()</cite> function needs to be called. The <cite>step()</cite> function will be called
with the <cite>optimizer_args</cite> named arguments.</p></li>
<li><p><strong>barrier</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether the XLA tensor barrier should be issued in
this API. If using the PyTorch XLA <cite>ParallelLoader</cite> or <cite>DataParallel</cite>
support, this is not necessary as the barrier will be issued by the XLA
data loader iterator <cite>next()</cite> call.
Default: False</p></li>
<li><p><strong>optimizer_args</strong> (<em>dict</em><em>, </em><em>optional</em>) – Named arguments dictionary for the
<cite>optimizer.step()</cite> call.</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_reduce()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The same value returned by the <cite>optimizer.step()</cite> call.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.save">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">save</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">file_or_path</em>, <em class="sig-param">master_only=True</em>, <em class="sig-param">global_master=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the input data into a file.</p>
<p>The saved data is transfered to PyTorch CPU device before being saved, so a
following <cite>torch.load()</cite> will load CPU data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – The input data to be saved. Any nested combination of Python objects
(list, tuples, sets, dicts, …).</p></li>
<li><p><strong>file_or_path</strong> – The destination for the data saving operation. Either a file
path or a Python file object. If <cite>master_only</cite> is <code class="docutils literal notranslate"><span class="pre">False</span></code> the path or
file objects must point to different destinations as otherwise all the
writes from the same host will override each other.</p></li>
<li><p><strong>master_only</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether only the master device should save the
data. If False, the <cite>file_or_path</cite> argument should be a different file or
path for each of the ordinals taking part to the replication, otherwise
all the replicas on the same host will be writing to the same location.
Default: True</p></li>
<li><p><strong>global_master</strong> (<em>bool</em><em>, </em><em>optional</em>) – When <code class="docutils literal notranslate"><span class="pre">master_only</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> this flag
controls whether every host’s master (if <code class="docutils literal notranslate"><span class="pre">global_master</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>)
saves the content, or only the global master (ordinal 0).
Default: False</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.rendezvous">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">rendezvous</code><span class="sig-paren">(</span><em class="sig-param">tag</em>, <em class="sig-param">payload=b''</em>, <em class="sig-param">replicas=[]</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#rendezvous"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.rendezvous" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for all the mesh clients to reach the named rendezvous.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> (<em>string</em>) – The name of the rendezvous to join.</p></li>
<li><p><strong>payload</strong> (<em>bytes</em><em>, </em><em>optional</em>) – The payload to be sent to the rendezvous.</p></li>
<li><p><strong>replicas</strong> (<em>list</em><em>, </em><em>python:int</em>) – The replica ordinals taking part of the rendezvous.
Empty means all replicas in the mesh.
Default: []</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The payloads exchanged by all the other cores, with the payload of core
ordinal <cite>i</cite> at position <cite>i</cite> in the returned tuple.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.do_on_ordinals">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">do_on_ordinals</code><span class="sig-paren">(</span><em class="sig-param">target</em>, <em class="sig-param">data=()</em>, <em class="sig-param">ordinals=(0</em>, <em class="sig-param">)</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#do_on_ordinals"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.do_on_ordinals" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs a function only on a given set of ordinals.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> (<em>callable</em>) – The function to be run on <cite>ordinals</cite>.</p></li>
<li><p><strong>data</strong> – Any input data for the <cite>target</cite> function which contains tensors. All
the XLA tensors used by the <cite>target</cite> function must be passed in this
argument. Every other data used by the function can be captured by the
Python interpreter as usual.
Default: ()</p></li>
<li><p><strong>ordinals</strong> (<em>list</em><em>, </em><em>python:int</em>) – The list/set of ordinals where the <cite>target</cite> function
should run.
Default: (0,)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>In the ordinals that ran the <cite>target</cite> function, the function return value,
otherwise <cite>None</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.mesh_reduce">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">mesh_reduce</code><span class="sig-paren">(</span><em class="sig-param">tag</em>, <em class="sig-param">data</em>, <em class="sig-param">reduce_fn</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#mesh_reduce"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.mesh_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an out-of-graph client mesh reduction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> (<em>string</em>) – The name of the rendezvous to join.</p></li>
<li><p><strong>data</strong> – The data to be reduced. The <cite>reduce_fn</cite> callable will receive a list
with the copies of the same data coming from all the mesh client processes
(one per core).</p></li>
<li><p><strong>reduce_fn</strong> (<em>callable</em>) – A function which receives a list of <cite>data</cite>-like
objects and returns the reduced result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced value.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.set_rng_state">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">set_rng_state</code><span class="sig-paren">(</span><em class="sig-param">seed</em>, <em class="sig-param">device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#set_rng_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.set_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the random number generator state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seed</strong> (<em>python:integer</em>) – The state to be set.</p></li>
<li><p><strong>device</strong> (<em>string</em><em>, </em><em>optional</em>) – The device where the RNG state needs to be set.
If missing the default device seed will be set.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.get_rng_state">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">get_rng_state</code><span class="sig-paren">(</span><em class="sig-param">seed</em>, <em class="sig-param">device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_rng_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the current running random number generator state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>string</em><em>, </em><em>optional</em>) – The device whose RNG state needs to be retrieved.
If missing the default device seed will be set.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The RNG state, as integer.</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-torch_xla.core.functions"></span><dl class="function">
<dt id="torch_xla.core.functions.all_reduce">
<code class="sig-prename descclassname">torch_xla.core.functions.</code><code class="sig-name descname">all_reduce</code><span class="sig-paren">(</span><em class="sig-param">reduce_type</em>, <em class="sig-param">value</em>, <em class="sig-param">scale=1.0</em>, <em class="sig-param">groups=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/functions.html#all_reduce"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.functions.all_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an inplace reduce operation on the input tensor.</p>
<p>This is the same as <cite>xm.all_reduce()</cite> but supports autograd differentiation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>reduce_type</strong> (<em>string</em>) – One of <code class="docutils literal notranslate"><span class="pre">REDUCE_SUM</span></code>, <code class="docutils literal notranslate"><span class="pre">REDUCE_MUL</span></code>, <code class="docutils literal notranslate"><span class="pre">REDUCE_AND</span></code>,
<code class="docutils literal notranslate"><span class="pre">REDUCE_OR</span></code>, <code class="docutils literal notranslate"><span class="pre">REDUCE_MIN</span></code> and <code class="docutils literal notranslate"><span class="pre">REDUCE_MIN</span></code>.</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – The to perform the all reduce op to.</p></li>
<li><p><strong>scale</strong> (<em>python:float</em>) – A default scaling value to be applied after the reduce.
Default: 1.0</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_reduce()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced value across the selected replicas.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.functions.all_gather">
<code class="sig-prename descclassname">torch_xla.core.functions.</code><code class="sig-name descname">all_gather</code><span class="sig-paren">(</span><em class="sig-param">value</em>, <em class="sig-param">dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/functions.html#all_gather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.functions.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an all-gather operation along a given dimension.</p>
<p>This is the same as <cite>xm.all_gather()</cite> but supports autograd differentiation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – The input tensor.</p></li>
<li><p><strong>dim</strong> (<em>python:int</em>) – The gather dimension.
Default: 0</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor which has, in the <code class="docutils literal notranslate"><span class="pre">dim</span></code> dimension, all the values from the
participating replicas.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.functions.nms">
<code class="sig-prename descclassname">torch_xla.core.functions.</code><code class="sig-name descname">nms</code><span class="sig-paren">(</span><em class="sig-param">boxes</em>, <em class="sig-param">scores</em>, <em class="sig-param">score_threshold</em>, <em class="sig-param">iou_threshold</em>, <em class="sig-param">output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/functions.html#nms"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.functions.nms" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a Non Maximal Suppression operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>boxes</strong> (<em>torch.Tensor</em>) – A <cite>torch.Tensor</cite> of shape <cite>[N, 4]</cite> listing the boxes
coordinates in <cite>(y0, x0, y1, x1)</cite> form.</p></li>
<li><p><strong>scores</strong> (<em>torch.Tensor</em>) – A <cite>torch.Tensor</cite> of shape <cite>[N]</cite> listing the scores
of each box.</p></li>
<li><p><strong>score_threshold</strong> (<em>torch.Tensor</em>) – The minimum score for a box to qualify as
valid.</p></li>
<li><p><strong>iou_threshold</strong> (<em>torch.Tensor</em>) – The minimum IOU (Intersection Over Union)
score to trigger overlap logic.</p></li>
<li><p><strong>output_size</strong> (<em>python:int</em>) – The maximum number of returned indices (must be lower or
equal to N).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple of <cite>torch.Tensor</cite> with the first element being the selected box
indices, and the second element being the number of valid boxes.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-torch_xla.distributed.parallel_loader">
<span id="distributed"></span><h2>distributed<a class="headerlink" href="#module-torch_xla.distributed.parallel_loader" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torch_xla.distributed.parallel_loader.ParallelLoader">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.distributed.parallel_loader.</code><code class="sig-name descname">ParallelLoader</code><span class="sig-paren">(</span><em class="sig-param">loader</em>, <em class="sig-param">devices</em>, <em class="sig-param">batchdim=0</em>, <em class="sig-param">fixed_batch_size=False</em>, <em class="sig-param">loader_prefetch_size=8</em>, <em class="sig-param">device_prefetch_size=4</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/parallel_loader.html#ParallelLoader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.parallel_loader.ParallelLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Wraps an existing PyTorch DataLoader with background data upload.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loader</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>) – The PyTorch DataLoader to be
wrapped.</p></li>
<li><p><strong>devices</strong> (<cite>torch.device</cite>…) – The list of devices where the data has to be
sent. The i-th sample returned by the <cite>loader</cite> will be sent to <cite>devices[i
% len(devices)]</cite>.</p></li>
<li><p><strong>batchdim</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The dimension which is holding the batch size.
Default: 0</p></li>
<li><p><strong>fixed_batch_size</strong> (<em>bool</em><em>, </em><em>optional</em>) – Ensures that all the batch sizes sent to
the devices are of the same size. The original <cite>loader</cite> iteration stops as
soon as a not matching batch size is found.
Default: False</p></li>
<li><p><strong>loader_prefetch_size</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The max capacity of the queue used by
the thread which is reading samples from the <cite>loader</cite>, to be processed by
the worker threads which upload data to the devices.
Default: 8</p></li>
<li><p><strong>device_prefetch_size</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The max size of the per-device queues,
where the worker threads deposit tensors which have already been sent to
devices.
Default: 4</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch_xla.distributed.parallel_loader.ParallelLoader.per_device_loader">
<code class="sig-name descname">per_device_loader</code><span class="sig-paren">(</span><em class="sig-param">device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/parallel_loader.html#ParallelLoader.per_device_loader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.parallel_loader.ParallelLoader.per_device_loader" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the loader iterator object for the given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<cite>torch.device</cite>) – The device whole loader is being requested.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The loader iterator object for the <cite>device</cite>. This is not a
<cite>torch.utils.data.DataLoader</cite> interface, but a Python iterator which
returns the same tensor data structure as returned by the wrapped
<cite>torch.utils.data.DataLoader</cite>, but residing on XLA devices.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<span class="target" id="module-torch_xla.distributed.xla_multiprocessing"></span><dl class="function">
<dt id="torch_xla.distributed.xla_multiprocessing.spawn">
<code class="sig-prename descclassname">torch_xla.distributed.xla_multiprocessing.</code><code class="sig-name descname">spawn</code><span class="sig-paren">(</span><em class="sig-param">fn</em>, <em class="sig-param">args=()</em>, <em class="sig-param">nprocs=None</em>, <em class="sig-param">join=True</em>, <em class="sig-param">daemon=False</em>, <em class="sig-param">start_method='spawn'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/xla_multiprocessing.html#spawn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.xla_multiprocessing.spawn" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables multi processing based replication.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> (<em>callable</em>) – The function to be called for each device which takes part of
the replication. The function will be called with a first argument being
the global index of the process within the replication, followed by the
arguments passed in <cite>args</cite>.</p></li>
<li><p><strong>args</strong> (<em>tuple</em>) – The arguments for <cite>fn</cite>.
Default: Empty tuple</p></li>
<li><p><strong>nprocs</strong> (<em>python:int</em>) – The number of processes/devices for the replication. At the
moment, if specified, can be either 1 or the maximum number of devices.</p></li>
<li><p><strong>join</strong> (<em>bool</em>) – Whether the call should block waiting for the completion of the
processes which have being spawned.
Default: True</p></li>
<li><p><strong>daemon</strong> (<em>bool</em>) – Whether the processes being spawned should have the <cite>daemon</cite>
flag set (see Python multi-processing API).
Default: False</p></li>
<li><p><strong>start_method</strong> (<em>string</em>) – The Python <cite>multiprocessing</cite> process creation method.
Default: <cite>spawn</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The same object returned by the <cite>torch.multiprocessing.spawn</cite> API. If
<cite>nprocs</cite> is 1 the <cite>fn</cite> function will be called directly, and the API will
not return.</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch_xla.distributed.xla_multiprocessing.MpModelWrapper">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.distributed.xla_multiprocessing.</code><code class="sig-name descname">MpModelWrapper</code><span class="sig-paren">(</span><em class="sig-param">model</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/xla_multiprocessing.html#MpModelWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.xla_multiprocessing.MpModelWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Wraps a model to minimize host memory usage when <cite>fork</cite> method is used.</p>
<p>This class should be used together with the <cite>spawn(…, start_method=’fork’)</cite>
API to minimize the use of host memory.
Instead of creating models on each multiprocessing process, hence replicating
the model’s initial host memory, the model is created once at global scope,
and then moved into each device inside the <cite>spawn()</cite> target function.
Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">WRAPPED_MODEL</span> <span class="o">=</span> <span class="n">xmp</span><span class="o">.</span><span class="n">MpModelWrapper</span><span class="p">(</span><span class="n">MyNetwork</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">_mp_fn</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">WRAPPED_MODEL</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="o">...</span>

<span class="n">xmp</span><span class="o">.</span><span class="n">spwan</span><span class="p">(</span><span class="n">_mp_fn</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">start_method</span><span class="o">=</span><span class="s1">&#39;fork&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This method has two advantages. First if uses only one copy of the memory
pages to host the original model weights, and second it serializes the move
of the wrapped model into each device, by lowering the load onto the system
memory during the process.</p>
<dl class="method">
<dt id="torch_xla.distributed.xla_multiprocessing.MpModelWrapper.to">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/xla_multiprocessing.html#MpModelWrapper.to"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.xla_multiprocessing.MpModelWrapper.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the model moved onto the specified device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>torch.device</em>) – The device where the model should be moved onto.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The model on the specified device.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch_xla.distributed.xla_multiprocessing.MpSerialExecutor">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.distributed.xla_multiprocessing.</code><code class="sig-name descname">MpSerialExecutor</code><a class="reference internal" href="_modules/torch_xla/distributed/xla_multiprocessing.html#MpSerialExecutor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.xla_multiprocessing.MpSerialExecutor" title="Permalink to this definition">¶</a></dt>
<dd><p>Utility to run a function in a serialized fashion among multi-core processes.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># At global scope.</span>
<span class="n">SERIAL_EXEC</span> <span class="o">=</span> <span class="n">xmp</span><span class="o">.</span><span class="n">MpSerialExecutor</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">maybe_download_and_load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_mp_fn</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
  <span class="c1"># Avoid all cores downloading the same data with the serial executor.</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">SERIAL_EXEC</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;/tmp/mnist-data&#39;</span><span class="p">))</span>
  <span class="o">...</span>

<span class="n">xmp</span><span class="o">.</span><span class="n">spwan</span><span class="p">(</span><span class="n">_mp_fn</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch_xla.distributed.xla_multiprocessing.MpSerialExecutor.run">
<code class="sig-name descname">run</code><span class="sig-paren">(</span><em class="sig-param">fn</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/xla_multiprocessing.html#MpSerialExecutor.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.xla_multiprocessing.MpSerialExecutor.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs the provided function serialized WRT each per-core process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<em>callable</em>) – The function to run in a serialized fashion.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The <cite>fn</cite> return value.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torch_xla.utils.tf_record_reader">
<span id="utils"></span><h2>utils<a class="headerlink" href="#module-torch_xla.utils.tf_record_reader" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torch_xla.utils.tf_record_reader.TfRecordReader">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.utils.tf_record_reader.</code><code class="sig-name descname">TfRecordReader</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">compression=''</em>, <em class="sig-param">buffer_size=16777216</em>, <em class="sig-param">transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/tf_record_reader.html#TfRecordReader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.tf_record_reader.TfRecordReader" title="Permalink to this definition">¶</a></dt>
<dd><p>Reads TfRecords or TfExamples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>string</em>) – The path to the file containing TfRecords.</p></li>
<li><p><strong>compression</strong> (<em>string</em><em>, </em><em>optional</em>) – The compression type. The empty string for
no compression, otherwise <code class="docutils literal notranslate"><span class="pre">ZLIB</span></code> or <code class="docutils literal notranslate"><span class="pre">GZIP</span></code>.
Default: No compression.</p></li>
<li><p><strong>buffer_size</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The size of the buffer to be used to read
TfRecords.
Default: 16 * 1024 * 1024</p></li>
<li><p><strong>transforms</strong> (<em>dict</em><em>, </em><em>optional</em>) – A dictionary with the key matching the
TfExample label name, and value which is either a callable which will be
called to tranform the matching tensor data, or <code class="docutils literal notranslate"><span class="pre">STR</span></code> for string
conversion.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-torch_xla.utils.utils"></span><dl class="class">
<dt id="torch_xla.utils.utils.SampleGenerator">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.utils.utils.</code><code class="sig-name descname">SampleGenerator</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">sample_count</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/utils.html#SampleGenerator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.utils.SampleGenerator" title="Permalink to this definition">¶</a></dt>
<dd><p>Iterator which returns multiple samples of a given input data.</p>
<p>Can be used in place of a PyTorch <cite>DataLoader</cite> to generate synthetic data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – The data which should be returned at each iterator step.</p></li>
<li><p><strong>sample_count</strong> – The maximum number of <cite>data</cite> samples to be returned.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch_xla.utils.utils.DataWrapper">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.utils.utils.</code><code class="sig-name descname">DataWrapper</code><a class="reference internal" href="_modules/torch_xla/utils/utils.html#DataWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.utils.DataWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Utility class to wrap data structures to be sent to device.</p>
</dd></dl>

<span class="target" id="module-torch_xla.utils.gcsfs"></span><dl class="function">
<dt id="torch_xla.utils.gcsfs.open">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">open</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">mode='r'</em>, <em class="sig-param">encoding=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#open"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.open" title="Permalink to this definition">¶</a></dt>
<dd><p>Opens a Google Cloud Storage (GCS) file for reading or writing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>string</em>) – The GCS path of the file. Must be “gs://BUCKET_NAME/PATH”
where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite>
delimited path.</p></li>
<li><p><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – The open mode, similar to the <code class="docutils literal notranslate"><span class="pre">open()</span></code> API.
Default: ‘r’</p></li>
<li><p><strong>encoding</strong> (<em>string</em><em>, </em><em>optional</em>) – The character encoding to be used to decode
bytes into strings when opening in text mode.
Default: None</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The GCS file object.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.list">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">list</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.list" title="Permalink to this definition">¶</a></dt>
<dd><p>Lists the content of a GCS bucket.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – The GCS path of the file. Must be “gs://BUCKET_NAME/PATH”
where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite>
delimited path.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of <code class="docutils literal notranslate"><span class="pre">GcsBlob</span></code> objects.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.stat">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">stat</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#stat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.stat" title="Permalink to this definition">¶</a></dt>
<dd><p>Fetches the information of a GCS file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – The GCS path of the file. Must be “gs://BUCKET_NAME/PATH”
where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite>
delimited path.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">GcsBlob</span></code> object.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.remove">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">remove</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#remove"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.remove" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes a GCS blob.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – The GCS path of the file. Must be “gs://BUCKET_NAME/PATH”
where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite>
delimited path.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.rmtree">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">rmtree</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#rmtree"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.rmtree" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes all the GCS blobs within a given path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – <p>The GCS path of the file pattern or folder. Must be
“gs://BUCKET_NAME/PATH” where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS</p>
<blockquote>
<div><p>bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite> delimited path.</p>
</div></blockquote>
</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.read">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">read</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#read"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.read" title="Permalink to this definition">¶</a></dt>
<dd><p>Reads the whole content of a GCS blob.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – The GCS path of the file. Must be “gs://BUCKET_NAME/PATH”
where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite>
delimited path.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The bytes stored within the GCS blob.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.write">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">write</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">content</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#write"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.write" title="Permalink to this definition">¶</a></dt>
<dd><p>Write a string/bytes or file into a GCS blob.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>string</em>) – The GCS path of the file. Must be “gs://BUCKET_NAME/PATH”
where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite>
delimited path.</p></li>
<li><p><strong>content</strong> (<em>string</em><em>, </em><em>bytes</em><em> or </em><em>file object</em>) – The content to be written into
<code class="docutils literal notranslate"><span class="pre">path</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.generic_open">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">generic_open</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">mode='r'</em>, <em class="sig-param">encoding=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#generic_open"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.generic_open" title="Permalink to this definition">¶</a></dt>
<dd><p>Opens a file (GCS or not) for reding or writing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>string</em>) – <p>The path of the file to be opened. If a GCS path, it must be
“gs://BUCKET_NAME/PATH” where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS</p>
<blockquote>
<div><p>bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite> delimited path.</p>
</div></blockquote>
</p></li>
<li><p><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – The open mode, similar to the <code class="docutils literal notranslate"><span class="pre">open()</span></code> API.
Default: ‘r’</p></li>
<li><p><strong>encoding</strong> (<em>string</em><em>, </em><em>optional</em>) – The character encoding to be used to decode
bytes into strings when opening in text mode.
Default: None</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The opened file object.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.generic_read">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">generic_read</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#generic_read"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.generic_read" title="Permalink to this definition">¶</a></dt>
<dd><p>Reads the whole content of the provided location.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – The GCS path or local path to be read.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The bytes stored within the GCS blob or local file.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.generic_write">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">generic_write</code><span class="sig-paren">(</span><em class="sig-param">output_string</em>, <em class="sig-param">output_path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#generic_write"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.generic_write" title="Permalink to this definition">¶</a></dt>
<dd><p>Write a string/bytes or file into a GCS blob or local disk.</p>
<p>Depending on the output_path passed in, this API can write to local or GCS
file. Checks if the <cite>output_path</cite> starts with
the ‘gs://’ prefix, and uses <cite>open</cite> otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_string</strong> (<em>string</em>) – The string to be written to the output.</p></li>
<li><p><strong>output_path</strong> (<em>string</em>) – The GCS path or local path of the output.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>


             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright .

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">PyTorch on XLA Devices</a><ul>
<li><a class="reference internal" href="#creating-an-xla-tensor">Creating an XLA Tensor</a></li>
<li><a class="reference internal" href="#xla-tensors-are-pytorch-tensors">XLA Tensors are PyTorch Tensors</a></li>
<li><a class="reference internal" href="#running-models-on-xla-devices">Running Models on XLA Devices</a><ul>
<li><a class="reference internal" href="#running-on-a-single-xla-device">Running on a Single XLA Device</a></li>
<li><a class="reference internal" href="#running-on-multiple-xla-devices-with-multiprocessing">Running on Multiple XLA Devices with MultiProcessing</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id1">XLA Tensor Deep Dive</a><ul>
<li><a class="reference internal" href="#xla-tensors-are-lazy">XLA Tensors are Lazy</a></li>
<li><a class="reference internal" href="#xla-tensors-and-bfloat16">XLA Tensors and bFloat16</a></li>
<li><a class="reference internal" href="#memory-layout">Memory Layout</a></li>
<li><a class="reference internal" href="#moving-xla-tensors-to-and-from-the-cpu">Moving XLA Tensors to and from the CPU</a></li>
<li><a class="reference internal" href="#saving-and-loading-xla-tensors">Saving and Loading XLA Tensors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#further-reading">Further Reading</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch-xla-api">PyTorch/XLA API</a><ul>
<li><a class="reference internal" href="#module-torch_xla.core.xla_model">xla_model</a></li>
<li><a class="reference internal" href="#module-torch_xla.distributed.parallel_loader">distributed</a></li>
<li><a class="reference internal" href="#module-torch_xla.utils.tf_record_reader">utils</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>