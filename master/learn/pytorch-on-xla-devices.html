


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>PyTorch on XLA Devices &mdash; PyTorch/XLA master documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PyTorch/XLA API" href="api-guide.html" />
    <link rel="prev" title="Pytorch/XLA overview" href="xla-overview.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (2.7.0+gite4a9e12 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Learn about Pytorch/XLA</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="xla-overview.html">Pytorch/XLA overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">PyTorch on XLA Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="api-guide.html">PyTorch/XLA API</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_shape.html">Dynamic shape</a></li>
<li class="toctree-l1"><a class="reference internal" href="eager.html">Eager Mode + Compile API</a></li>
<li class="toctree-l1"><a class="reference internal" href="pjrt.html">PJRT Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshoot.html">Troubleshoot</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Learn about accelerators</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/tpu.html">Learn about TPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accelerators/gpu.html">Learn about GPUs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyTorch/XLA features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../features/pallas.html">Custom Kernels via Pallas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/stablehlo.html">Torch Export to StableHLO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features/triton.html">Custom GPU Kernels via Triton</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Improve Pytorch/XLA workload performance</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../perf/amp.html">Automatic Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perf/spmd_basic.html">PyTorch/XLA SPMD User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perf/spmd_advanced.html">PyTorch/XLA SPMD advanced topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perf/spmd_distributed_checkpoint.html">Distributed Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perf/spmd_gpu.html">Running SPMD on GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perf/ddp.html">How to do DistributedDataParallel(DDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perf/dynamo.html">TorchDynamo integration in PyTorch XLA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perf/fori_loop.html">Optimize memory utilization using <code class="docutils literal notranslate"><span class="pre">while_loop</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../perf/fsdp.html">Fully Sharded Data Parallel in PyTorch XLA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perf/fsdpv2.html">Fully Sharded Data Parallel using SPMD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perf/quantized_ops.html">Quantized Operations for XLA (Experimental feature)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perf/recompilation.html">Source of recompilations in Pytorch/XLA</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contribute to Pytorch/XLA</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribute/configure-environment.html">Configure a development environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute/codegen_migration.html">Codegen migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute/op_lowering.html">OP Lowering Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute/plugins.html">Custom Hardware Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute/bazel.html">Bazel in Pytorch/XLA</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>PyTorch on XLA Devices</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/learn/pytorch-on-xla-devices.md.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="pytorch-on-xla-devices">
<h1>PyTorch on XLA Devices<a class="headerlink" href="#pytorch-on-xla-devices" title="Permalink to this heading">¶</a></h1>
<p>PyTorch runs on XLA devices, like TPUs, with the <a class="reference external" href="https://github.com/pytorch/xla/">torch_xla package</a>. This document describes how
to run your models on these devices.</p>
<div class="section" id="creating-an-xla-tensor">
<h2>Creating an XLA Tensor<a class="headerlink" href="#creating-an-xla-tensor" title="Permalink to this heading">¶</a></h2>
<p>PyTorch/XLA adds a new <code class="docutils literal notranslate"><span class="pre">xla</span></code> device type to PyTorch. This device type
works just like other PyTorch device types. For example, here’s how to
create and print an XLA tensor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<p>This code should look familiar. PyTorch/XLA uses the same interface as
regular PyTorch with a few additions. Importing <code class="docutils literal notranslate"><span class="pre">torch_xla</span></code> initializes
PyTorch/XLA, and <code class="docutils literal notranslate"><span class="pre">xm.xla_device()</span></code> returns the current XLA device. This
may be a CPU or TPU depending on your environment.</p>
</div>
<div class="section" id="xla-tensors-are-pytorch-tensors">
<h2>XLA Tensors are PyTorch Tensors<a class="headerlink" href="#xla-tensors-are-pytorch-tensors" title="Permalink to this heading">¶</a></h2>
<p>PyTorch operations can be performed on XLA tensors just like CPU or CUDA
tensors.</p>
<p>For example, XLA tensors can be added together:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t0</span> <span class="o">+</span> <span class="n">t1</span><span class="p">)</span>
</pre></div>
</div>
<p>Or matrix multiplied:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">t1</span><span class="p">))</span>
</pre></div>
</div>
<p>Or used with neural network modules:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">l_out</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">l_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">l_out</span><span class="p">)</span>
</pre></div>
</div>
<p>Like other device types, XLA tensors only work with other XLA tensors on
the same device. So code like</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">l_out</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">l_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">l_out</span><span class="p">)</span>
<span class="c1"># Input tensor is not an XLA tensor: torch.FloatTensor</span>
</pre></div>
</div>
<p>will throw an error since the <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> module is on the CPU.</p>
</div>
<div class="section" id="running-models-on-xla-devices">
<h2>Running Models on XLA Devices<a class="headerlink" href="#running-models-on-xla-devices" title="Permalink to this heading">¶</a></h2>
<p>Building a new PyTorch network or converting an existing one to run on
XLA devices requires only a few lines of XLA-specific code. The
following snippets highlight these lines when running on a single device
and multiple devices with XLA multi-processing.</p>
<div class="section" id="running-on-a-single-xla-device">
<h3>Running on a Single XLA Device<a class="headerlink" href="#running-on-a-single-xla-device" title="Permalink to this heading">¶</a></h3>
<p>The following snippet shows a network training on a single XLA device:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
  <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
  <span class="n">xm</span><span class="o">.</span><span class="n">mark_step</span><span class="p">()</span>
</pre></div>
</div>
<p>This snippet highlights how easy it is to switch your model to run on
XLA. The model definition, dataloader, optimizer and training loop can
work on any device. The only XLA-specific code is a couple lines that
acquire the XLA device and mark the step. Calling <code class="docutils literal notranslate"><span class="pre">xm.mark_step()</span></code> at
the end of each training iteration causes XLA to execute its current
graph and update the model’s parameters. See <a class="reference external" href="#xla-tensor-deep-dive">XLA Tensor Deep
Dive</a> for more on how XLA creates graphs and runs
operations.</p>
</div>
<div class="section" id="running-on-multiple-xla-devices-with-multi-processing">
<h3>Running on Multiple XLA Devices with Multi-processing<a class="headerlink" href="#running-on-multiple-xla-devices-with-multi-processing" title="Permalink to this heading">¶</a></h3>
<p>PyTorch/XLA makes it easy to accelerate training by running on multiple
XLA devices. The following snippet shows how:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.parallel_loader</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="k">def</span> <span class="nf">_mp_fn</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
  <span class="n">mp_device_loader</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">MpDeviceLoader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">mp_device_loader</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">xm</span><span class="o">.</span><span class="n">optimizer_step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="n">torch_xla</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">_mp_fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">())</span>
</pre></div>
</div>
<p>There are three differences between this multi-device snippet and the
previous single device snippet. Let’s go over then one by one.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch_xla.launch()</span></code></p>
<ul>
<li><p>Creates the processes that each run an XLA device.</p></li>
<li><p>This function is a wrapper of multithreading spawn to allow user
run the script with torchrun command line also. Each process
will only be able to access the device assigned to the current
process. For example on a TPU v4-8, there will be 4 processes
being spawn up and each process will own a TPU device.</p></li>
<li><p>Note that if you print the <code class="docutils literal notranslate"><span class="pre">xm.xla_device()</span></code> on each process you
will see <code class="docutils literal notranslate"><span class="pre">xla:0</span></code> on all devices. This is because each process
can only see one device. This does not mean multi-process is not
functioning. The only execution is with PJRT runtime on TPU v2
and TPU v3 since there will be <code class="docutils literal notranslate"><span class="pre">#devices/2</span></code> processes and each
process will have 2 threads(check this
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/docs/pjrt.md#tpus-v2v3-vs-v4">doc</a>
for more details).</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">MpDeviceLoader</span></code></p>
<ul>
<li><p>Loads the training data onto each device.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MpDeviceLoader</span></code> can wrap on a torch dataloader. It can preload
the data to the device and overlap the dataloading with device
execution to improve the performance.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MpDeviceLoader</span></code> also call <code class="docutils literal notranslate"><span class="pre">xm.mark_step</span></code> for you every
<code class="docutils literal notranslate"><span class="pre">batches_per_execution</span></code>(default to 1) batch being yield.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">xm.optimizer_step(optimizer)</span></code></p>
<ul>
<li><p>Consolidates the gradients between devices and issues the XLA
device step computation.</p></li>
<li><p>It is pretty much a <code class="docutils literal notranslate"><span class="pre">all_reduce_gradients</span></code> +
<code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> + <code class="docutils literal notranslate"><span class="pre">mark_step</span></code> and returns the loss being
reduced.</p></li>
</ul>
</li>
</ul>
<p>The model definition, optimizer definition and training loop remain the
same.</p>
<blockquote>
<div><p><strong>NOTE:</strong> It is important to note that, when using multi-processing,
the user can start retrieving and accessing XLA devices only from
within the target function of <code class="docutils literal notranslate"><span class="pre">torch_xla.launch()</span></code> (or any function
which has <code class="docutils literal notranslate"><span class="pre">torch_xla.launch()</span></code> as parent in the call stack).</p>
</div></blockquote>
<p>See the <a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_train_mp_mnist.py">full multiprocessing
example</a>
for more on training a network on multiple XLA devices with
multi-processing.</p>
</div>
<div class="section" id="running-on-tpu-pods">
<h3>Running on TPU Pods<a class="headerlink" href="#running-on-tpu-pods" title="Permalink to this heading">¶</a></h3>
<p>Multi-host setup for different accelerators can be very different. This
doc will talk about the device independent bits of multi-host training
and will use the TPU + PJRT runtime(currently available on 1.13 and 2.x
releases) as an example.</p>
<p>Before you being, please take a look at our user guide at
<a class="reference external" href="https://cloud.google.com/tpu/docs/run-calculation-pytorch">here</a> which
will explain some Google Cloud basis like how to use <code class="docutils literal notranslate"><span class="pre">gcloud</span></code> command
and how to setup your project. You can also check
<a class="reference external" href="https://cloud.google.com/tpu/docs/how-to">here</a> for all Cloud TPU
Howto. This doc will focus on the PyTorch/XLA perspective of the Setup.</p>
<p>Let’s assume you have the above mnist example from above section in a
<code class="docutils literal notranslate"><span class="pre">train_mnist_xla.py</span></code>. If it is a single host multi device training, you
would ssh to the TPUVM and run command like</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">TPU</span> <span class="n">python3</span> <span class="n">train_mnist_xla</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Now in order to run the same models on a TPU v4-16 (which has 2 host,
each with 4 TPU devices), you will need to - Make sure each host can
access the training script and training data. This is usually done by
using the <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">scp</span></code> command or <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">ssh</span></code> command to copy the
training scripts to all hosts. - Run the same training command on all
hosts at the same time.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=$ZONE --project=$PROJECT --worker=all --command=&quot;PJRT_DEVICE=TPU python3 train_mnist_xla.py&quot;</span>
</pre></div>
</div>
<p>Above <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">ssh</span></code> command will ssh to all hosts in TPUVM Pod and run
the same command at the same time..</p>
<blockquote>
<div><p><strong>NOTE:</strong> You need to run run above <code class="docutils literal notranslate"><span class="pre">gcloud</span></code> command outside of the
TPUVM vm.</p>
</div></blockquote>
<p>The model code and training script is the same for the multi-process
training and the multi-host training. PyTorch/XLA and the underlying
infrastructure will make sure each device is aware of the global
topology and each device’s local and global ordinal. Cross-device
communication will happen across all devices instead of local devices.</p>
<p>For more details regarding PJRT runtime and how to run it on pod, please
refer to this
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/docs/pjrt.md#tpu">doc</a>. For
more information about PyTorch/XLA and TPU pod and a complete guide to
run a resnet50 with fakedata on TPU pod, please refer to this
<a class="reference external" href="https://cloud.google.com/tpu/docs/pytorch-pods">guide</a>.</p>
</div>
</div>
<div class="section" id="id3">
<h2>XLA Tensor Deep Dive<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h2>
<p>Using XLA tensors and devices requires changing only a few lines of
code. But even though XLA tensors act a lot like CPU and CUDA tensors,
their internals are different. This section describes what makes XLA
tensors unique.</p>
<div class="section" id="xla-tensors-are-lazy">
<h3>XLA Tensors are Lazy<a class="headerlink" href="#xla-tensors-are-lazy" title="Permalink to this heading">¶</a></h3>
<p>CPU and CUDA tensors launch operations immediately or eagerly. XLA
tensors, on the other hand, are lazy. They record operations in a graph
until the results are needed. Deferring execution like this lets XLA
optimize it. A graph of multiple separate operations might be fused into
a single optimized operation, for example.</p>
<p>Lazy execution is generally invisible to the caller. PyTorch/XLA
automatically constructs the graphs, sends them to XLA devices, and
synchronizes when copying data between an XLA device and the CPU.
Inserting a barrier when taking an optimizer step explicitly
synchronizes the CPU and the XLA device. For more information about our
lazy tensor design, you can read <a class="reference external" href="https://arxiv.org/pdf/2102.13267.pdf">this
paper</a>.</p>
</div>
<div class="section" id="memory-layout">
<h3>Memory Layout<a class="headerlink" href="#memory-layout" title="Permalink to this heading">¶</a></h3>
<p>The internal data representation of XLA tensors is opaque to the user.
They do not expose their storage and they always appear to be
contiguous, unlike CPU and CUDA tensors. This allows XLA to adjust a
tensor’s memory layout for better performance.</p>
</div>
<div class="section" id="moving-xla-tensors-to-and-from-the-cpu">
<h3>Moving XLA Tensors to and from the CPU<a class="headerlink" href="#moving-xla-tensors-to-and-from-the-cpu" title="Permalink to this heading">¶</a></h3>
<p>XLA tensors can be moved from the CPU to an XLA device and from an XLA
device to the CPU. If a view is moved then the data its viewing is also
copied to the other device and the view relationship is not preserved.
Put another way, once data is copied to another device it has no
relationship with its previous device or any tensors on it. Again,
depending on how your code operates, appreciating and accommodating this
transition can be important.</p>
</div>
<div class="section" id="saving-and-loading-xla-tensors">
<h3>Saving and Loading XLA Tensors<a class="headerlink" href="#saving-and-loading-xla-tensors" title="Permalink to this heading">¶</a></h3>
<p>XLA tensors should be moved to the CPU before saving, as in the
following snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">tensors</span> <span class="o">=</span> <span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">t1</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>

<span class="n">tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>This lets you put the loaded tensors on any available device, not just
the one on which they were initialized.</p>
<p>Per the above note on moving XLA tensors to the CPU, care must be taken
when working with views. Instead of saving views it is recommended that
you recreate them after the tensors have been loaded and moved to their
destination device(s).</p>
<p>A utility API is provided to save data by taking care of previously
moving it to CPU:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">xm</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
<p>In case of multiple devices, the above API will only save the data for
the master device ordinal (0).</p>
<p>In case where memory is limited compared to the size of the model
parameters, an API is provided that reduces the memory footprint on the
host:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.utils.serialization</span> <span class="k">as</span> <span class="nn">xser</span>

<span class="n">xser</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
<p>This API streams XLA tensors to CPU one at a time, reducing the amount
of host memory used, but it requires a matching load API to restore:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.utils.serialization</span> <span class="k">as</span> <span class="nn">xser</span>

<span class="n">state_dict</span> <span class="o">=</span> <span class="n">xser</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</pre></div>
</div>
<p>Directly saving XLA tensors is possible but not recommended. XLA tensors
are always loaded back to the device they were saved from, and if that
device is unavailable the load will fail. PyTorch/XLA, like all of
PyTorch, is under active development and this behavior may change in the
future.</p>
</div>
</div>
<div class="section" id="compilation-caching">
<h2>Compilation Caching<a class="headerlink" href="#compilation-caching" title="Permalink to this heading">¶</a></h2>
<p>The XLA compiler converts the traced HLO into an executable which runs
on the devices. Compilation can be time consuming, and in cases where
the HLO doesn’t change across executions, the compilation result can be
persisted to disk for reuse, significantly reducing development
iteration time.</p>
<p>Note that if the HLO changes between executions, a recompilation will
still occur.</p>
<p>This is currently an experimental opt-in API, which must be activated
before any computations are executed. Initialization is done through the
<code class="docutils literal notranslate"><span class="pre">initialize_cache</span></code> API:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.runtime</span> <span class="k">as</span> <span class="nn">xr</span>
<span class="n">xr</span><span class="o">.</span><span class="n">initialize_cache</span><span class="p">(</span><span class="s1">&#39;YOUR_CACHE_PATH&#39;</span><span class="p">,</span> <span class="n">readonly</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>This will initialize a persistent compilation cache at the specified
path. The <code class="docutils literal notranslate"><span class="pre">readonly</span></code> parameter can be used to control whether the worker
will be able to write to the cache, which can be useful when a shared
cache mount is used for an SPMD workload.</p>
<p>If you want to use persistent compilation cache in the multi process
training(with <code class="docutils literal notranslate"><span class="pre">torch_xla.launch</span></code> or <code class="docutils literal notranslate"><span class="pre">xmp.spawn</span></code>), you should use the
different path for different process.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_mp_fn</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
  <span class="c1"># cache init needs to happens inside the mp_fn.</span>
  <span class="n">xr</span><span class="o">.</span><span class="n">initialize_cache</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;/tmp/xla_cache_</span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">readonly</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="o">....</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="n">torch_xla</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">_mp_fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">())</span>
</pre></div>
</div>
<p>If you don’t have the access to the <code class="docutils literal notranslate"><span class="pre">index</span></code>, you can use
<code class="docutils literal notranslate"><span class="pre">xr.global_ordinal()</span></code>. Check out the runnable example in
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/examples/data_parallel/train_resnet_xla_ddp.py">here</a>.</p>
</div>
<div class="section" id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this heading">¶</a></h2>
<p>Additional documentation is available at the <a class="reference external" href="https://github.com/pytorch/xla/">PyTorch/XLA
repo</a>. More examples of running
networks on TPUs are available
<a class="reference external" href="https://github.com/pytorch-tpu/examples">here</a>.</p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="api-guide.html" class="btn btn-neutral float-right" title="PyTorch/XLA API" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="xla-overview.html" class="btn btn-neutral" title="Pytorch/XLA overview" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright .

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">PyTorch on XLA Devices</a><ul>
<li><a class="reference internal" href="#creating-an-xla-tensor">Creating an XLA Tensor</a></li>
<li><a class="reference internal" href="#xla-tensors-are-pytorch-tensors">XLA Tensors are PyTorch Tensors</a></li>
<li><a class="reference internal" href="#running-models-on-xla-devices">Running Models on XLA Devices</a><ul>
<li><a class="reference internal" href="#running-on-a-single-xla-device">Running on a Single XLA Device</a></li>
<li><a class="reference internal" href="#running-on-multiple-xla-devices-with-multi-processing">Running on Multiple XLA Devices with Multi-processing</a></li>
<li><a class="reference internal" href="#running-on-tpu-pods">Running on TPU Pods</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id3">XLA Tensor Deep Dive</a><ul>
<li><a class="reference internal" href="#xla-tensors-are-lazy">XLA Tensors are Lazy</a></li>
<li><a class="reference internal" href="#memory-layout">Memory Layout</a></li>
<li><a class="reference internal" href="#moving-xla-tensors-to-and-from-the-cpu">Moving XLA Tensors to and from the CPU</a></li>
<li><a class="reference internal" href="#saving-and-loading-xla-tensors">Saving and Loading XLA Tensors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#compilation-caching">Compilation Caching</a></li>
<li><a class="reference internal" href="#further-reading">Further Reading</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/sphinx_highlight.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>