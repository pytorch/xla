# Copyright 2018 The Google Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "torch_xla"
dynamic = ["version"]
description = "Differentiate, compile, and transform Numpy code."
readme = "README.md"
license = {file = "LICENSE"}
requires-python = ">=3.10"
authors = [{ name = "torch_xla team", email = "torch_xla-dev@google.com" }]
classifiers = [
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
]
dependencies = [
    "torch >=2.6.0, <=2.6.1",
    "ml_dtypes>=0.4.0",
    "numpy>=1.25",
    "numpy>=1.26.0; python_version>='3.12'",
    "opt_einsum",
    "scipy>=1.11.1",
]

[project.optional-dependencies]
# Used only for CI builds that install torch_xla from github HEAD.
ci = ["torch==0.5.0"]
# A CPU-only torch_xla doesn't require any extras, but we keep this extra around for compatibility.
cpu = []
cuda = ["torch==2.6.0", "torch_xla-cuda12-plugin[with_cuda]>=0.5.0,<=0.5.1"]
cuda12 = ["torch==2.6.0", "torch_xla-cuda12-plugin[with_cuda]>=0.5.0,<=0.5.1"]
# Target that does not depend on the CUDA pip wheels, for those who want to use a preinstalled CUDA.
cuda12_local = ["torch==2.6.0", "torch_xla-cuda12-plugin==0.5.0"]
# Deprecated alias for cuda12, kept to avoid breaking users who wrote cuda12_pip in their CI.
cuda12_pip = ["torch==2.6.0", "torch_xla-cuda12-plugin[with_cuda]>=0.5.0,<=0.5.1"]
dev = [
    "mypy>=1.15",
    "pre-commit>=4.1",
    "pytest>=8.3",
    "ruff>=0.9.7",
    "setuptools>=75.8",
    "tomlkit>=0.13.2",
]
# For automatic bootstrapping distributed jobs in Kubernetes
k8s = ["kubernetes"]
# Minimum torch version; used in testing.
minimum-torch = ["torch==2.6.0"]
# ROCm support for ROCm 6.0 and above.
rocm = ["torch==2.6.0", "torch_xla-rocm60-plugin>=0.5.0,<=0.5.1"]
# Cloud TPU VM torch can be installed via:
# $ pip install "torch_xla[tpu]" -f https://storage.googleapis.com/torch_xla-releases/libtpu_releases.html
tpu = ["torch>=2.6.0,<=2.6.1", "libtpu==0.0.8", "requests"]

[project.urls]
homepage = "https://github.com/pytorch/xla"
repository = "https://github.com/pytorch/xla"

[tool.setuptools.dynamic]
version = {attr = "torch_xla.version.__version__"}

[tool.setuptools.packages.find]
exclude = ["examples", "torch_xla/src/internal_test_util"]

[tool.setuptools.package-data]
torch_xla = ['py.typed', "*.pyi", "**/*.pyi"]

[tool.mypy]
show_error_codes = true
disable_error_code = "attr-defined, name-defined, annotation-unchecked"
