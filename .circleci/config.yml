version: 2.1

update_submodule: &update_submodule
  name: Update Submodule
  command: |
    git submodule sync
    git submodule update --recursive --init

setup_base_docker: &setup_base_docker
  name: Setup Base Docker Image
  command: |
    .circleci/setup_ci_environment.sh
    .circleci/download_llvm_raw.sh

launch_docker_and_build: &launch_docker_and_build
  name: Launch Docker Container and Build
  no_output_timeout: "1h"
  command: |
    set -ex
    eval $(aws ecr get-login --region us-east-1 --no-include-email)
    docker pull ${GCR_DOCKER_IMAGE} >/dev/null
    # This is to stage PyTorch/XLA base image for use in the upstream.
    # To allow the upstream workflow to access PyTorch/XLA build images, we
    # need to have them in the ECR. This is not expensive, and only pushes it
    # if image layers are not present in the repo.
    # Note: disable the following 2 lines while testing a new image, so we do not
    # push to the upstream.
    docker tag ${GCR_DOCKER_IMAGE} ${ECR_DOCKER_IMAGE_BASE}:v1.0 >/dev/null
    docker push ${ECR_DOCKER_IMAGE_BASE}:v1.0 >/dev/null
    sudo pkill -SIGHUP dockerd
    # To enable remote caching, use SCCACHE_BUCKET=${SCCACHE_BUCKET}
    # To disable remote caching, use SCCACHE_BUCKET=${LOCAL_BUCKET}
    echo "declare -x SCCACHE_BUCKET=${SCCACHE_BUCKET}" >> /home/circleci/project/env
    echo "declare -x SCCACHE_CACHE_SIZE=${SCCACHE_CACHE_SIZE}" >> /home/circleci/project/env
    echo "declare -x AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_SCCACHE_AND_XLA_BAZEL_S3_BUCKET_V1}" >> /home/circleci/project/env
    echo "declare -x AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_AND_XLA_BAZEL_S3_BUCKET_V1}" >> /home/circleci/project/env
    echo "declare -x XLA_CLANG_CACHE_S3_BUCKET_NAME=${XLA_CLANG_CACHE_S3_BUCKET_NAME}" >> /home/circleci/project/env
    echo "declare -x CIRCLE_JOB=${CIRCLE_JOB}" >> /home/circleci/project/env
    echo "declare -x MAX_JOBS=8" >> /home/circleci/project/env
    echo "declare -x CC=clang-8 CXX=clang++-8" >> /home/circleci/project/xla_env
    echo "declare -x XLA_USE_XRT=1" >> /home/circleci/project/xla_env
    echo "declare -x XLA_CUDA=1" >> /home/circleci/project/xla_env
    echo "declare -x USE_CUDA=0" >> /home/circleci/project/env
    echo "declare -x GITHUB_TORCH_XLA_BOT_TOKEN=${GITHUB_TORCH_XLA_BOT_2_TOKEN}" >> /home/circleci/project/xla_env
    echo "declare -x CIRCLE_PULL_REQUEST=${CIRCLE_PULL_REQUEST}" >> /home/circleci/project/env
    echo "declare -x CIRCLE_PROJECT_USERNAME=${CIRCLE_PROJECT_USERNAME}" >> /home/circleci/project/env
    echo "declare -x CIRCLE_PROJECT_REPONAME=${CIRCLE_PROJECT_REPONAME}" >> /home/circleci/project/env

    pid=$(docker run -t -d -w $WORKDIR ${GCR_DOCKER_IMAGE})
    docker cp /home/circleci/project/. "$pid:$WORKDIR"
    docker exec -u jenkins ${pid} sudo chown -R jenkins ${WORKDIR}
    echo ${pid} > .docker_pid

    # Build
    docker exec -u jenkins $(cat .docker_pid) bash -c '. ~/.bashrc && .circleci/build.sh'

    # Push built docker image
    output_image=${GCR_DOCKER_IMAGE}-${CIRCLE_SHA1}
    export COMMIT_DOCKER_IMAGE=$output_image
    docker commit "$pid" ${COMMIT_DOCKER_IMAGE}
    time docker push ${COMMIT_DOCKER_IMAGE}

download_docker: &download_docker
  name: Download Docker Container
  no_output_timeout: "1h"
  command: |
    set -e
    output_image=${GCR_DOCKER_IMAGE}-${CIRCLE_SHA1}
    export COMMIT_DOCKER_IMAGE=$output_image
    echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}

    sudo pip install awscli==1.16.35 -qqq
    time docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null
    if [ -n "${USE_CUDA_DOCKER_RUNTIME:-}" ]; then
      pid=$(docker run --runtime=nvidia -t -d -w $WORKDIR ${COMMIT_DOCKER_IMAGE})
    else
      pid=$(docker run -t -d -w $WORKDIR ${COMMIT_DOCKER_IMAGE})
    fi
    echo ${pid} > .docker_pid

calculate_docker_image: &calculate_docker_image
  name: Calculate docker image
  command: |
    # For staging our build image in ECR for upstream testing
    echo "declare -x ECR_DOCKER_IMAGE_BASE=308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/xla_base" >> "${BASH_ENV}"
    # For PyTorch/XLA CI workflow, we only pull & push to gcr.io
    echo "declare -x GCR_DOCKER_IMAGE=gcr.io/tpu-pytorch/xla_base:latest" >> "${BASH_ENV}"

run_build: &run_build
  resource_class: 2xlarge
  machine:
    image: ubuntu-2004:202111-02
  steps:
  - checkout
  - run:
      name: "Setup custom environment variables"
      command: |
        pyenv install 3.7.0
        pyenv global 3.7.0
        echo "export CLOUDSDK_PYTHON=$(which python3)" >> $BASH_ENV
  - run:
      <<: *update_submodule
  - run:
      <<: *setup_base_docker
  - run:
      <<: *calculate_docker_image
  - run:
      <<: *launch_docker_and_build

run_test: &run_test
  machine:
    image: ubuntu-2004:202111-02
  steps:
  - checkout
  - run:
      name: "Setup custom environment variables"
      command: |
        pyenv install 3.7.0
        pyenv global 3.7.0
        echo "export CLOUDSDK_PYTHON=$(which python3)" >> $BASH_ENV
  - run:
      <<: *update_submodule
  - run:
      <<: *setup_base_docker
  - run:
      <<: *calculate_docker_image
  - run:
      <<: *download_docker
  - run:
      name: Test
      no_output_timeout: "1h"
      command: |
        docker exec -u jenkins $(cat .docker_pid) bash -c '. ~/.bashrc && .circleci/test.sh'

run_test_and_push_doc: &run_test_and_push_doc
  machine:
    image: ubuntu-2004:202111-02
  steps:
  - checkout
  - run:
      name: "Setup custom environment variables"
      command: |
        pyenv install 3.7.0
        pyenv global 3.7.0
        echo "export CLOUDSDK_PYTHON=$(which python3)" >> $BASH_ENV
  - run:
      <<: *update_submodule
  - run:
      <<: *setup_base_docker
  - run:
      <<: *calculate_docker_image
  - run:
      <<: *download_docker
  - run:
      name: Test
      no_output_timeout: "1h"
      command: |
        docker exec -u jenkins $(cat .docker_pid) bash -c '. ~/.bashrc && .circleci/test.sh'
  - run:
      name: Push doc to public
      no_output_timeout: "1h"
      command: |
        docker exec -u jenkins $(cat .docker_pid) bash -c '. ~/.bashrc && .circleci/doc_push.sh'


run_clang_format: &run_clang_format
  name: Run clang-format
  command: |
    sudo apt-get update
    sudo apt install -y clang-format-7
    git_status=$(git status --porcelain)
    if [[ $git_status ]]; then
      echo "Checkout code is not clean"
      echo "${git_status}"
      exit 1
    fi

    find -name '*.cpp' -o -name '*.h' | xargs clang-format-7 -i -style=file
    git_status=$(git status --porcelain)
    if [[ $git_status ]]; then
      git diff
      echo "clang-format-7 recommends the changes above, please manually apply them OR automatically apply the changes "
      echo "by running \"clang-format-7 -i -style /PATH/TO/foo.cpp\" to the following files"
      echo "${git_status}"
      exit 1
    else
      echo "PASSED C++ format"
    fi

run_yapf: &run_yapf
  name: Run yapf
  command: |
    pyenv install 3.7.0
    pyenv global 3.7.0
    pip install --upgrade pip
    pip install yapf==0.30.0

    git_status=$(git status --porcelain)
    if [[ $git_status ]]; then
      echo "Checkout code is not clean"
      echo "${git_status}"
      exit 1
    fi

    yapf -i -r *.py test/ scripts/ torch_xla/
    git_status=$(git status --porcelain)
    if [[ $git_status ]]; then
      git diff
      echo "yapf recommends the changes above, please manually apply them OR automatically apply the changes "
      echo "by running `yapf -i /PATH/TO/foo.py` to the following files"
      echo "${git_status}"
      exit 1
    else
      echo "PASSED Python format"
    fi

assert_no_torch_pin: &assert_no_torch_pin
  name: Make sure torch_patches/.torch_pin is removed before merging
  command: |
      TORCH_PIN=./torch_patches/.torch_pin
      if [[ -f "${TORCH_PIN}" ]]; then
        echo "Please remove ${TORCH_PIN} before landing."
        exit 1
      else
        echo "No ${TORCH_PIN} found, safe to land..."
      fi

assert_continue_on_error_is_false: &assert_continue_on_error_is_false
  name: Make sure CONTINUE_ON_ERROR flags are set to false before merging
  command: |
      CONTINUE_ON_ERROR="CONTINUE_ON_ERROR=true"
      if grep -Fxq "$CONTINUE_ON_ERROR" ./test/run_tests.sh
      then
        echo "Please set CONTINUE_ON_ERROR at test/run_tests.sh to false before landing."
        exit 1
      fi
      if grep -Fxq "$CONTINUE_ON_ERROR" ./.circleci/common.sh
      then
        echo "Please set CONTINUE_ON_ERROR at .circleci/common.sh to false before landing."
        exit 1
      fi
      echo "CONTINUE_ON_ERROR flags are set to false, safe to land..."

ci_params: &ci_params
  parameters:
    resource_class:
      type: string
      default: "large"
    use_cuda_docker_runtime:
      type: string
      default: ""
  environment:
    USE_CUDA_DOCKER_RUNTIME: << parameters.use_cuda_docker_runtime >>
  resource_class: << parameters.resource_class >>

jobs:
  linter_check:
    machine:
      image: ubuntu-2004:202111-02
    steps:
    - checkout
    - run:
        <<: *run_clang_format
    - run:
        <<: *run_yapf

  linter_check_no_torch_pin:
    machine:
      image: ubuntu-2004:202111-02
    steps:
    - checkout
    - run:
        <<: *assert_no_torch_pin
    - run:
        <<: *run_clang_format
    - run:
        <<: *run_yapf

  continue_on_error_is_false:
    machine:
      image: ubuntu-2004:202111-02
    steps:
    - checkout
    - run:
        <<: *assert_continue_on_error_is_false

  pytorch_xla_run_build:
    <<: *run_build

  pytorch_xla_run_test:
    <<: *ci_params
    <<: *run_test

  pytorch_xla_run_test_and_push_doc:
    <<: *ci_params
    <<: *run_test_and_push_doc

workflows:
  version: 2
  build:
    jobs:
      - linter_check:
          filters:
            branches:
              ignore:
                - master
                - ^r\d*\.?\d*$
      - linter_check_no_torch_pin:
          filters:
            branches:
              only:
                - master
                - ^r\d*\.?\d*$
      - pytorch_xla_run_build
      - pytorch_xla_run_test:
          name: pytorch_xla_run_CPU_test
          resource_class: 2xlarge
          requires:
            - pytorch_xla_run_build
          filters:
            branches:
              ignore:
                - master
                - ^r\d*\.?\d*$
      - pytorch_xla_run_test:
          name: pytorch_xla_run_GPU_test
          resource_class: gpu.nvidia.small.multi
          use_cuda_docker_runtime: "1"
          requires:
            - pytorch_xla_run_build
      - pytorch_xla_run_test_and_push_doc:
          name: pytorch_xla_run_CPU_test_and_push_doc
          requires:
            - pytorch_xla_run_build
          filters:
            branches:
              only:
                - master
                - ^r\d*\.?\d*$
