


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>PyTorch on XLA Devices &mdash; PyTorch/XLA master documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (2.0 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">PyTorch on XLA Devices</a><ul>
<li><a class="reference internal" href="#creating-an-xla-tensor">Creating an XLA Tensor</a></li>
<li><a class="reference internal" href="#xla-tensors-are-pytorch-tensors">XLA Tensors are PyTorch Tensors</a></li>
<li><a class="reference internal" href="#running-models-on-xla-devices">Running Models on XLA Devices</a><ul>
<li><a class="reference internal" href="#running-on-a-single-xla-device">Running on a Single XLA Device</a></li>
<li><a class="reference internal" href="#running-on-multiple-xla-devices-with-multi-processing">Running on Multiple XLA Devices with Multi-processing</a></li>
<li><a class="reference internal" href="#running-on-tpu-pods">Running on TPU Pods</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id3">XLA Tensor Deep Dive</a><ul>
<li><a class="reference internal" href="#xla-tensors-are-lazy">XLA Tensors are Lazy</a></li>
<li><a class="reference internal" href="#xla-tensors-and-bfloat16">XLA Tensors and bFloat16</a></li>
<li><a class="reference internal" href="#memory-layout">Memory Layout</a></li>
<li><a class="reference internal" href="#moving-xla-tensors-to-and-from-the-cpu">Moving XLA Tensors to and from the CPU</a></li>
<li><a class="reference internal" href="#saving-and-loading-xla-tensors">Saving and Loading XLA Tensors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#further-reading">Further Reading</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch-xla-api">PyTorch/XLA API</a><ul>
<li><a class="reference internal" href="#module-torch_xla.core.xla_model">xla_model</a></li>
<li><a class="reference internal" href="#module-torch_xla.distributed.parallel_loader">distributed</a></li>
<li><a class="reference internal" href="#module-torch_xla.utils.tf_record_reader">utils</a></li>
<li><a class="reference internal" href="#test">test</a></li>
</ul>
</li>
<li><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li><a class="reference internal" href="#perform-a-auto-metrics-analysis">Perform A Auto-Metrics Analysis</a></li>
<li><a class="reference internal" href="#get-a-metrics-report">Get A Metrics Report</a></li>
<li><a class="reference internal" href="#understand-the-metrics-report">Understand The Metrics Report</a></li>
<li><a class="reference internal" href="#clar-the-metrics-report">Clar The Metrics Report</a></li>
<li><a class="reference internal" href="#performance-profiling">Performance Profiling</a></li>
<li><a class="reference internal" href="#known-performance-caveats">Known Performance Caveats</a></li>
<li><a class="reference internal" href="#xla-tensor-quirks">XLA Tensor Quirks</a></li>
<li><a class="reference internal" href="#more-debugging-tools">More Debugging Tools</a><ul>
<li><a class="reference internal" href="#environment-variables">Environment Variables</a></li>
<li><a class="reference internal" href="#retrieving-stack-traces">Retrieving Stack Traces</a></li>
</ul>
</li>
<li><a class="reference internal" href="#using-debug-run-py-to-collect-debug-information">Using debug_run.py To Collect Debug Information</a></li>
<li><a class="reference internal" href="#common-issues">Common Issues</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pjrt-runtime-beta">PJRT Runtime (Beta)</a><ul>
<li><a class="reference internal" href="#tl-dr">TL;DR</a></li>
<li><a class="reference internal" href="#benefits">Benefits</a></li>
<li><a class="reference internal" href="#quickstart">Quickstart</a><ul>
<li><a class="reference internal" href="#cpu">CPU</a></li>
<li><a class="reference internal" href="#tpu">TPU</a><ul>
<li><a class="reference internal" href="#pods">Pods</a></li>
<li><a class="reference internal" href="#docker">Docker</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gpu">GPU</a></li>
</ul>
</li>
<li><a class="reference internal" href="#differences-from-xrt">Differences from XRT</a><ul>
<li><a class="reference internal" href="#id6">Multithreading on TPU v2/v3</a></li>
<li><a class="reference internal" href="#changes-to-xm-rendezvous">Changes to xm.rendezvous</a></li>
<li><a class="reference internal" href="#pjrt-and-torch-distributed">PJRT and torch.distributed</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance">Performance</a><ul>
<li><a class="reference internal" href="#new-tpu-runtime">New TPU runtime</a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchdynamo-torch-compile-integration-in-pytorch-xla">TorchDynamo(torch.compile) integration in PyTorch XLA</a><ul>
<li><a class="reference internal" href="#inference">Inference</a></li>
<li><a class="reference internal" href="#training">Training</a></li>
<li><a class="reference internal" href="#take-away">Take away</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fully-sharded-data-parallel-fsdp-in-pytorch-xla">Fully Sharded Data Parallel (FSDP) in PyTorch XLA</a><ul>
<li><a class="reference internal" href="#example-training-scripts-on-mnist-and-imagenet">Example training scripts on MNIST and ImageNet</a><ul>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#clone-pytorch-xla-repo">Clone PyTorch/XLA repo</a></li>
<li><a class="reference internal" href="#train-mnist-on-v3-8-tpu">Train MNIST on v3-8 TPU</a></li>
<li><a class="reference internal" href="#train-imagenet-with-resnet-50-on-v3-8-tpu">Train ImageNet with ResNet-50 on v3-8 TPU</a></li>
</ul>
</li>
<li><a class="reference internal" href="#example-training-scripts-on-tpu-pod-with-10-billion-parameters">Example training scripts on TPU pod (with 10 billion parameters)</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-do-distributeddataparallel">How to do <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code></a><ul>
<li><a class="reference internal" href="#background-motivation">Background / Motivation</a></li>
<li><a class="reference internal" href="#how-to-use-distributeddataparallel">How to use DistributedDataParallel</a></li>
<li><a class="reference internal" href="#benchmarking">Benchmarking</a><ul>
<li><a class="reference internal" href="#resnet50-with-fake-data">Resnet50 with fake data</a></li>
<li><a class="reference internal" href="#mnist-with-fake-data">MNIST with fake data</a></li>
<li><a class="reference internal" href="#mnist-with-real-data">MNIST with real data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#disclaimer">Disclaimer</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-run-with-pytorch-xla-gpu">How to run with PyTorch/XLA:GPU</a><ul>
<li><a class="reference internal" href="#create-a-gpu-instance">Create a GPU instance</a></li>
<li><a class="reference internal" href="#environment-setup">Environment Setup</a><ul>
<li><a class="reference internal" href="#id8">Docker</a></li>
<li><a class="reference internal" href="#wheel">Wheel</a></li>
</ul>
</li>
<li><a class="reference internal" href="#run-a-simple-model">Run a simple model</a></li>
<li><a class="reference internal" href="#amp-automatic-mixed-precision">AMP (AUTOMATIC MIXED PRECISION)</a></li>
</ul>
</li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="#">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>PyTorch on XLA Devices</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="pytorch-on-xla-devices">
<h1>PyTorch on XLA Devices<a class="headerlink" href="#pytorch-on-xla-devices" title="Permalink to this headline">¶</a></h1>
<p>PyTorch runs on XLA devices, like TPUs, with the
<a class="reference external" href="https://github.com/pytorch/xla/">torch_xla package</a>. This document describes
how to run your models on these devices.</p>
<div class="section" id="creating-an-xla-tensor">
<h2>Creating an XLA Tensor<a class="headerlink" href="#creating-an-xla-tensor" title="Permalink to this headline">¶</a></h2>
<p>PyTorch/XLA adds a new <code class="docutils literal notranslate"><span class="pre">xla</span></code> device type to PyTorch. This device type works just
like other PyTorch device types. For example, here’s how to create and
print an XLA tensor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<p>This code should look familiar. PyTorch/XLA uses the same interface as regular
PyTorch with a few additions. Importing <code class="docutils literal notranslate"><span class="pre">torch_xla</span></code> initializes PyTorch/XLA, and
<code class="docutils literal notranslate"><span class="pre">xm.xla_device()</span></code> returns the current XLA device. This may be a CPU or TPU
depending on your environment.</p>
</div>
<div class="section" id="xla-tensors-are-pytorch-tensors">
<h2>XLA Tensors are PyTorch Tensors<a class="headerlink" href="#xla-tensors-are-pytorch-tensors" title="Permalink to this headline">¶</a></h2>
<p>PyTorch operations can be performed on XLA tensors just like CPU or CUDA tensors.</p>
<p>For example, XLA tensors can be added together:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t0</span> <span class="o">+</span> <span class="n">t1</span><span class="p">)</span>
</pre></div>
</div>
<p>Or matrix multiplied:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">t1</span><span class="p">))</span>
</pre></div>
</div>
<p>Or used with neural network modules:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">l_out</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">l_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">l_out</span><span class="p">)</span>
</pre></div>
</div>
<p>Like other device types, XLA tensors only work with other XLA tensors on the
same device. So code like</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
<span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">l_out</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">l_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">l_out</span><span class="p">)</span>
<span class="c1"># Input tensor is not an XLA tensor: torch.FloatTensor</span>
</pre></div>
</div>
<p>will throw an error since the <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> module is on the CPU.</p>
</div>
<div class="section" id="running-models-on-xla-devices">
<h2>Running Models on XLA Devices<a class="headerlink" href="#running-models-on-xla-devices" title="Permalink to this headline">¶</a></h2>
<p>Building a new PyTorch network or converting an existing one to run on XLA
devices requires only a few lines of XLA-specific code. The following snippets
highlight these lines when running on a single device and multiple devices with XLA
multi-processing.</p>
<div class="section" id="running-on-a-single-xla-device">
<h3>Running on a Single XLA Device<a class="headerlink" href="#running-on-a-single-xla-device" title="Permalink to this headline">¶</a></h3>
<p>The following snippet shows a network training on a single XLA device:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
  <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

  <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
  <span class="n">xm</span><span class="o">.</span><span class="n">mark_step</span><span class="p">()</span>
</pre></div>
</div>
<p>This snippet highlights how easy it is to switch your model to run on XLA. The
model definition, dataloader, optimizer and training loop can work on any device.
The only XLA-specific code is a couple lines that acquire the XLA device and
mark the step. Calling
<code class="docutils literal notranslate"><span class="pre">xm.mark_step()</span></code> at the end of each training
iteration causes XLA to execute its current graph and update the model’s
parameters. See <a class="reference external" href="#xla-tensor-deep-dive">XLA Tensor Deep Dive</a> for more on
how XLA creates graphs and runs operations.</p>
</div>
<div class="section" id="running-on-multiple-xla-devices-with-multi-processing">
<h3>Running on Multiple XLA Devices with Multi-processing<a class="headerlink" href="#running-on-multiple-xla-devices-with-multi-processing" title="Permalink to this headline">¶</a></h3>
<p>PyTorch/XLA makes it easy to accelerate training by running on multiple XLA
devices. The following snippet shows how:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.parallel_loader</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_multiprocessing</span> <span class="k">as</span> <span class="nn">xmp</span>

<span class="k">def</span> <span class="nf">_mp_fn</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
  <span class="n">mp_device_loader</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">MpDeviceLoader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">mp_device_loader</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">xm</span><span class="o">.</span><span class="n">optimizer_step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="n">xmp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">_mp_fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">())</span>
</pre></div>
</div>
<p>There are three differences between this multi-device snippet and the previous
single device snippet. Let’s go over then one by one.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">xmp.spawn()</span></code></p>
<ul>
<li><p>Creates the processes that each run an XLA device.</p></li>
<li><p>Each process will only be able to access the device assigned to the current process. For example on a TPU v4-8, there will be 4 processes being spawn up and each process will own a TPU device.</p></li>
<li><p>Note that if you print the <code class="docutils literal notranslate"><span class="pre">xm.xla_device()</span></code> on each process you will see <code class="docutils literal notranslate"><span class="pre">xla:0</span></code> on all devices. This is because each process can only see one device. This does not mean multi-process is not functioning. The only execution is with PJRT runtime on TPU v2 and TPU v3 since there will be <code class="docutils literal notranslate"><span class="pre">#devices/2</span></code> processes and each process will have 2 threads(check this <a class="reference external" href="https://github.com/pytorch/xla/blob/master/docs/pjrt.md#tpus-v2v3-vs-v4">doc</a> for more details).</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">MpDeviceLoader</span></code></p>
<ul>
<li><p>Loads the training data onto each device.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MpDeviceLoader</span></code> can wrap on a torch dataloader. It can preload the data to the device and overlap the dataloading with device execution to improve the performance.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MpDeviceLoader</span></code> also call <code class="docutils literal notranslate"><span class="pre">xm.mark_step</span></code> for you every <code class="docutils literal notranslate"><span class="pre">batches_per_execution</span></code>(default to 1) batch being yield.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">xm.optimizer_step(optimizer)</span></code></p>
<ul>
<li><p>Consolidates the gradients between devices and issues the XLA device step computation.</p></li>
<li><p>It is pretty much a <code class="docutils literal notranslate"><span class="pre">all_reduce_gradients</span></code> + <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> + <code class="docutils literal notranslate"><span class="pre">mark_step</span></code> and returns the loss being reduced.</p></li>
</ul>
</li>
</ul>
<p>The model definition, optimizer definition and training loop remain the same.</p>
<blockquote>
<div><p><strong>NOTE:</strong> It is important to note that, when using multi-processing, the user can start
retrieving and accessing XLA devices only from within the target function of
<code class="docutils literal notranslate"><span class="pre">xmp.spawn()</span></code> (or any function which has <code class="docutils literal notranslate"><span class="pre">xmp.spawn()</span></code> as parent in the call
stack).</p>
</div></blockquote>
<p>See the
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_train_mp_mnist.py">full multiprocessing example</a>
for more on training a network on multiple XLA devices with multi-processing.</p>
</div>
<div class="section" id="running-on-tpu-pods">
<h3>Running on TPU Pods<a class="headerlink" href="#running-on-tpu-pods" title="Permalink to this headline">¶</a></h3>
<p>Multi-host setup for different accelerators can be very different. This doc will talk about the device independent bits of multi-host training and will use the TPU + PJRT runtime(currently available on 1.13 and 2.x releases) as an example.</p>
<p>Before you being, please take a look at our user guide at <a class="reference external" href="https://cloud.google.com/tpu/docs/run-calculation-pytorch">here</a> which will explain some Google Cloud basis like how to use <code class="docutils literal notranslate"><span class="pre">gcloud</span></code> command and how to setup your project. You can also check <a class="reference external" href="https://cloud.google.com/tpu/docs/how-to">here</a> for all Cloud TPU Howto. This doc will focus on the PyTorch/XLA perspective of the Setup.</p>
<p>Let’s assume you have the above mnist example from above section in a <code class="docutils literal notranslate"><span class="pre">train_mnist_xla.py</span></code>. If it is a single host multi device training, you would ssh to the TPUVM and run command like</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">TPU</span> <span class="n">python3</span> <span class="n">train_mnist_xla</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Now in order to run the same models on a TPU v4-16 (which has 2 host, each with 4 TPU devices), you will need to</p>
<ul class="simple">
<li><p>Make sure each host can access the training script and training data. This is usually done by using the <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">scp</span></code> command or <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">ssh</span></code> command to copy the training scripts to all hosts.</p></li>
<li><p>Run the same training command on all hosts at the same time.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=$ZONE --project=$PROJECT --worker=all --command=&quot;PJRT_DEVICE=TPU python3 train_mnist_xla.py&quot;
</pre></div>
</div>
<p>Above <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">ssh</span></code> command will ssh to all hosts in TPUVM Pod and run the same command at the same time..</p>
<blockquote>
<div><p><strong>NOTE:</strong> You need to run run above <code class="docutils literal notranslate"><span class="pre">gcloud</span></code> command outside of the TPUVM vm.</p>
</div></blockquote>
<p>The model code and training scirpt is the same for the multi-process training and the multi-host training. PyTorch/XLA and the underlying infrastructure will make sure each device is aware of the global topology and each device’s local and global ordinal. Cross-device communication will happen across all devices instead of local devices.</p>
<p>For more details regarding PJRT runtime and how to run it on pod, please refer to this <a class="reference external" href="https://github.com/pytorch/xla/blob/master/docs/pjrt.md#tpu">doc</a>. For more information about PyTorch/XLA and TPU pod and a complete guide to run a resnet50 with fakedata on TPU pod, please refer to this <a class="reference external" href="https://cloud.google.com/tpu/docs/pytorch-pods">guide</a>.</p>
</div>
</div>
<div class="section" id="id3">
<h2>XLA Tensor Deep Dive<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>Using XLA tensors and devices requires changing only a few lines of code. But
even though XLA tensors act a lot like CPU and CUDA tensors, their internals are
different. This section describes what makes XLA tensors unique.</p>
<div class="section" id="xla-tensors-are-lazy">
<h3>XLA Tensors are Lazy<a class="headerlink" href="#xla-tensors-are-lazy" title="Permalink to this headline">¶</a></h3>
<p>CPU and CUDA tensors launch operations immediately or <span class="raw-html-m2r"><b>eagerly</b></span>. XLA tensors,
on the other hand, are <span class="raw-html-m2r"><b>lazy</b></span>. They record operations in a graph until the
results are needed. Deferring execution like this lets XLA optimize it. A graph
of multiple separate operations might be fused into a single optimized
operation, for example.</p>
<p>Lazy execution is generally invisible to the caller. PyTorch/XLA automatically
constructs the graphs, sends them to XLA devices, and synchronizes when
copying data between an XLA device and the CPU. Inserting a barrier when
taking an optimizer step explicitly synchronizes the CPU and the XLA device. For
more information about our lazy tensor design, you can read <a class="reference external" href="https://arxiv.org/pdf/2102.13267.pdf">this paper</a>.</p>
</div>
<div class="section" id="xla-tensors-and-bfloat16">
<h3>XLA Tensors and bFloat16<a class="headerlink" href="#xla-tensors-and-bfloat16" title="Permalink to this headline">¶</a></h3>
<p>PyTorch/XLA can use the
<a class="reference external" href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16</a>
datatype when running on TPUs. In fact, PyTorch/XLA handles float types
(<code class="docutils literal notranslate"><span class="pre">torch.float</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.double</span></code>) differently on TPUs. This behavior is
controlled by the <code class="docutils literal notranslate"><span class="pre">XLA_USE_BF16</span></code> and <code class="docutils literal notranslate"><span class="pre">XLA_DOWNCAST_BF16</span></code> environment variable:</p>
<ul class="simple">
<li><p>By default both <code class="docutils literal notranslate"><span class="pre">torch.float</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.double</span></code> are
<code class="docutils literal notranslate"><span class="pre">torch.float</span></code> on TPUs.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">XLA_USE_BF16</span></code> is set, then <code class="docutils literal notranslate"><span class="pre">torch.float</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.double</span></code> are both
<code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> on TPUs.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">XLA_DOWNCAST_BF16</span></code> is set, then <code class="docutils literal notranslate"><span class="pre">torch.float</span></code> is <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> on TPUs and <code class="docutils literal notranslate"><span class="pre">torch.double</span></code> is <code class="docutils literal notranslate"><span class="pre">float32</span></code> on TPUs.</p></li>
<li><p>If a PyTorch tensor has <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code> data type, this will be directly
mapped to the TPU <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> (XLA <code class="docutils literal notranslate"><span class="pre">BF16</span></code> primitive type).</p></li>
</ul>
<p>Developers should note that <em>XLA tensors on TPUs will always report their PyTorch datatype</em> regardless of
the actual datatype they’re using. This conversion is automatic and opaque.
If an XLA tensor on a TPU is moved back to the CPU it will be converted
from its actual datatype to its PyTorch datatype. Depending on how your code operates, this conversion triggered by
the type of processing unit can be important.</p>
</div>
<div class="section" id="memory-layout">
<h3>Memory Layout<a class="headerlink" href="#memory-layout" title="Permalink to this headline">¶</a></h3>
<p>The internal data representation of XLA tensors is opaque to the user. They
do not expose their storage and they always appear to be contiguous, unlike
CPU and CUDA tensors. This allows XLA to adjust a tensor’s memory layout for
better performance.</p>
</div>
<div class="section" id="moving-xla-tensors-to-and-from-the-cpu">
<h3>Moving XLA Tensors to and from the CPU<a class="headerlink" href="#moving-xla-tensors-to-and-from-the-cpu" title="Permalink to this headline">¶</a></h3>
<p>XLA tensors can be moved from the CPU to an XLA device and from an XLA device
to the CPU. If a view is moved then the data its viewing is also copied to the
other device and the view relationship is not preserved. Put another way,
once data is copied to another device it has no relationship with its
previous device or any tensors on it. Again, depending on how your code operates,
appreciating and accommodating this transition can be important.</p>
</div>
<div class="section" id="saving-and-loading-xla-tensors">
<h3>Saving and Loading XLA Tensors<a class="headerlink" href="#saving-and-loading-xla-tensors" title="Permalink to this headline">¶</a></h3>
<p>XLA tensors should be moved to the CPU before saving, as in the following
snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">tensors</span> <span class="o">=</span> <span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">t1</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>

<span class="n">tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>This lets you put the loaded tensors on any available device, not just the one on which they were initialized.</p>
<p>Per the above note on moving XLA tensors to the CPU, care must be taken when
working with views. Instead of saving views it is recommended that you recreate
them after the tensors have been loaded and moved to their destination device(s).</p>
<p>A utility API is provided to save data by taking care of previously moving it
to CPU:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="n">xm</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
<p>In case of multiple devices, the above API will only save the data for the master
device ordinal (0).</p>
<p>In case where memory is limited compared to the size of the model parameters, an
API is provided that reduces the memory footprint on the host:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.utils.serialization</span> <span class="k">as</span> <span class="nn">xser</span>

<span class="n">xser</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
<p>This API streams XLA tensors to CPU one at a time, reducing the amount of host
memory used, but it requires a matching load API to restore:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.utils.serialization</span> <span class="k">as</span> <span class="nn">xser</span>

<span class="n">state_dict</span> <span class="o">=</span> <span class="n">xser</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</pre></div>
</div>
<p>Directly saving XLA tensors is possible but not recommended. XLA
tensors are always loaded back to the device they were saved from, and if
that device is unavailable the load will fail. PyTorch/XLA, like all of PyTorch,
is under active development and this behavior may change in the future.</p>
</div>
</div>
<div class="section" id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h2>
<p>Additional documentation is available at the
<a class="reference external" href="https://github.com/pytorch/xla/">PyTorch/XLA repo</a>. More examples of running
networks on TPUs are available
<a class="reference external" href="https://github.com/pytorch-tpu/examples">here</a>.</p>
</div>
</div>
<div class="section" id="pytorch-xla-api">
<h1>PyTorch/XLA API<a class="headerlink" href="#pytorch-xla-api" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-torch_xla.core.xla_model">
<span id="xla-model"></span><h2>xla_model<a class="headerlink" href="#module-torch_xla.core.xla_model" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch_xla.core.xla_model.xla_device">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">xla_device</code><span class="sig-paren">(</span><em class="sig-param">n=None</em>, <em class="sig-param">devkind=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#xla_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.xla_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a given instance of an XLA device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The specific instance (ordinal) to be returned. If
specified, the specific XLA device instance will be returned. Otherwise
the first device of <cite>devkind</cite> will be returned.</p></li>
<li><p><strong>devkind</strong> (<em>string...</em><em>, </em><em>optional</em>) – If specified, one of <cite>TPU</cite>, <cite>GPU</cite> or <cite>CPU</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>torch.device</cite> with the requested instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.get_xla_supported_devices">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">get_xla_supported_devices</code><span class="sig-paren">(</span><em class="sig-param">devkind=None</em>, <em class="sig-param">max_devices=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_xla_supported_devices"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_xla_supported_devices" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of supported devices of a given kind.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>devkind</strong> (<em>string...</em><em>, </em><em>optional</em>) – If specified, one of <cite>TPU</cite>, <cite>GPU</cite> or <cite>CPU</cite>
(the ‘GPU’ XLA device is currently not implemented).</p></li>
<li><p><strong>max_devices</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The maximum number of devices to be returned of
that kind.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The list of device strings.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.xla_device_hw">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">xla_device_hw</code><span class="sig-paren">(</span><em class="sig-param">device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#xla_device_hw"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.xla_device_hw" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the hardware type of the given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>string</em><em> or </em><em>torch.device</em>) – The xla device that will be mapped to the
real device.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A string representation of the hardware type (<cite>CPU</cite>, <cite>TPU</cite>, <cite>GPU</cite>) of the
given device.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.get_ordinal">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">get_ordinal</code><span class="sig-paren">(</span><em class="sig-param">defval=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_ordinal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_ordinal" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the replication ordinal of the current thread.</p>
<p>The ordinals range from 0 to <cite>xrt_world_size()</cite> minus 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>defval</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The default value to be returned in case there is no
replication information available. Ignored for PjRt.
Default: 0</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The replication ordinal of the current thread.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.get_local_ordinal">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">get_local_ordinal</code><span class="sig-paren">(</span><em class="sig-param">defval=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_local_ordinal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_local_ordinal" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the replication local ordinal of the current thread.</p>
<p>The local ordinals range from 0 to the number of local devices minus 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>defval</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The default value to be returned in case there is no
replication information available. Ignored for PjRt.
Default: 0</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The replication local ordinal of the current thread.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.is_master_ordinal">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">is_master_ordinal</code><span class="sig-paren">(</span><em class="sig-param">local=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#is_master_ordinal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.is_master_ordinal" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks whether the current process is the master ordinal (0).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>local</strong> (<em>bool</em>) – Whether the local or global master ordinal should be checked.
In case of multi-host replication, there is only one global master ordinal
(host 0, device 0), while there are NUM_HOSTS local master ordinals.
Default: True</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A boolean indicating whether the current process is the master ordinal.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.xrt_world_size">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">xrt_world_size</code><span class="sig-paren">(</span><em class="sig-param">defval=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#xrt_world_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.xrt_world_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the number of devices which is taking part of the replication.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>defval</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The default value to be returned in case there is no
replication information available.
Default: 1</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The number of devices which is taking part of the replication.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.all_reduce">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">all_reduce</code><span class="sig-paren">(</span><em class="sig-param">reduce_type</em>, <em class="sig-param">inputs</em>, <em class="sig-param">scale=1.0</em>, <em class="sig-param">groups=None</em>, <em class="sig-param">cctx=None</em>, <em class="sig-param">pin_layout=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#all_reduce"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.all_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an inplace reduce operation on the input tensor(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>reduce_type</strong> (<em>string</em>) – One of <code class="docutils literal notranslate"><span class="pre">xm.REDUCE_SUM</span></code>, <code class="docutils literal notranslate"><span class="pre">xm.REDUCE_MUL</span></code>,
<code class="docutils literal notranslate"><span class="pre">xm.REDUCE_AND</span></code>, <code class="docutils literal notranslate"><span class="pre">xm.REDUCE_OR</span></code>, <code class="docutils literal notranslate"><span class="pre">xm.REDUCE_MIN</span></code> and
<code class="docutils literal notranslate"><span class="pre">xm.REDUCE_MAX</span></code>.</p></li>
<li><p><strong>inputs</strong> – Either a single <cite>torch.Tensor</cite> or a list of <cite>torch.Tensor</cite> to
perform the all reduce op to.</p></li>
<li><p><strong>scale</strong> (<em>python:float</em>) – A default scaling value to be applied after the reduce.
Default: 1.0</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_reduce()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
<li><p><strong>pin_layout</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to pin the layout for this communication op.
Layout pining can prevent potential data corruption when each process that
participate in the communication has slightly different program, but it might
cause some xla compiation to fail. Unpin the layout when you see error message
like “HloModule has a mix of layout constrained”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>If a single <cite>torch.Tensor</cite> is passed, the return value is a <cite>torch.Tensor</cite>
holding the reduced value (across the replicas). If a list/tuple is passed,
this function performs an inplace all-reduce op on the input tensors, and
returns the list/tuple itself.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.all_gather">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">all_gather</code><span class="sig-paren">(</span><em class="sig-param">value</em>, <em class="sig-param">dim=0</em>, <em class="sig-param">groups=None</em>, <em class="sig-param">output=None</em>, <em class="sig-param">pin_layout=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#all_gather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an all-gather operation along a given dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – The input tensor.</p></li>
<li><p><strong>dim</strong> (<em>python:int</em>) – The gather dimension.
Default: 0</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_gather()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
<li><p><strong>output</strong> (<em>torch.Tensor</em>) – Optional output tensor.</p></li>
<li><p><strong>pin_layout</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to pin the layout for this communication op.
Layout pining can prevent potential data corruption when each process that
participate in the communication has slightly different program, but it might
cause some xla compiation to fail. Unpin the layout when you see error message
like “HloModule has a mix of layout constrained”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor which has, in the <code class="docutils literal notranslate"><span class="pre">dim</span></code> dimension, all the values from the
participating replicas.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.all_to_all">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">all_to_all</code><span class="sig-paren">(</span><em class="sig-param">value</em>, <em class="sig-param">split_dimension</em>, <em class="sig-param">concat_dimension</em>, <em class="sig-param">split_count</em>, <em class="sig-param">groups=None</em>, <em class="sig-param">pin_layout=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#all_to_all"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.all_to_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an XLA <cite>AllToAll()</cite> operation on the input tensor.</p>
<p>See: <a class="reference external" href="https://www.tensorflow.org/xla/operation_semantics#alltoall">https://www.tensorflow.org/xla/operation_semantics#alltoall</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – The input tensor.</p></li>
<li><p><strong>split_dimension</strong> (<em>python:int</em>) – The dimension upon which the split should happen.</p></li>
<li><p><strong>concat_dimension</strong> (<em>python:int</em>) – The dimension upon which the concat should happen.</p></li>
<li><p><strong>split_count</strong> (<em>python:int</em>) – The split count.</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_reduce()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
<li><p><strong>pin_layout</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to pin the layout for this communication op.
Layout pining can prevent potential data corruption when each process that
participate in the communication has slightly different program, but it might
cause some xla compiation to fail. Unpin the layout when you see error message
like “HloModule has a mix of layout constrained”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The result <cite>torch.Tensor</cite> of the <cite>all_to_all()</cite> operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.add_step_closure">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">add_step_closure</code><span class="sig-paren">(</span><em class="sig-param">closure</em>, <em class="sig-param">args=()</em>, <em class="sig-param">run_async=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#add_step_closure"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.add_step_closure" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a closure to the list of the ones to be run at the end of the step.</p>
<p>Many times during model training there is the need to print/report (print to
console, post to tensorboard, etc…) information which require the content of
intermediary tensors to be inspected.
Inspecting different tensors content in different points of the model code
requires many executions and typically causes performance issues.
Adding a step closure will ensure that it will be run after the barrier, when
all the live tensors will be already materialized to device data.
Live tensors which will include the ones captured by the closure arguments.
So using <cite>add_step_closure()</cite> will ensure a single execution will be
performed, even when multiple closures are queued, requiring multiple tensors
to be inspected.
Step closures will be run sequentially in the order they have been queued.
Note that even though using this API the execution will be optimized, it is
advised to throttle the printing/reporting events once every N steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>closure</strong> (<em>callable</em>) – The function to be called.</p></li>
<li><p><strong>args</strong> (<em>tuple</em>) – The arguments to be passed to the closure.</p></li>
<li><p><strong>run_async</strong> – If True, run the closure asynchronously.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.wait_device_ops">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">wait_device_ops</code><span class="sig-paren">(</span><em class="sig-param">devices=[]</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#wait_device_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.wait_device_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for all the async operations on the given devices to complete.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>devices</strong> (<em>string...</em><em>, </em><em>optional</em>) – The devices whose async ops need to be waited
for. If empty, all the local devices will be waited for.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.optimizer_step">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">optimizer_step</code><span class="sig-paren">(</span><em class="sig-param">optimizer</em>, <em class="sig-param">barrier=False</em>, <em class="sig-param">optimizer_args={}</em>, <em class="sig-param">groups=None</em>, <em class="sig-param">pin_layout=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#optimizer_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the provided optimizer step and issue the XLA device step computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Optimizer</span></code>) – The <cite>torch.Optimizer</cite> instance whose
<cite>step()</cite> function needs to be called. The <cite>step()</cite> function will be called
with the <cite>optimizer_args</cite> named arguments.</p></li>
<li><p><strong>barrier</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether the XLA tensor barrier should be issued in
this API. If using the PyTorch XLA <cite>ParallelLoader</cite> or <cite>DataParallel</cite>
support, this is not necessary as the barrier will be issued by the XLA
data loader iterator <cite>next()</cite> call.
Default: False</p></li>
<li><p><strong>optimizer_args</strong> (<em>dict</em><em>, </em><em>optional</em>) – Named arguments dictionary for the
<cite>optimizer.step()</cite> call.</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_reduce()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
<li><p><strong>pin_layout</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to pin the layout when reducing gradients.
See <cite>xm.all_reduce</cite> for details.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The same value returned by the <cite>optimizer.step()</cite> call.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.save">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">save</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">file_or_path</em>, <em class="sig-param">master_only=True</em>, <em class="sig-param">global_master=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the input data into a file.</p>
<p>The saved data is transferred to PyTorch CPU device before being saved, so a
following <cite>torch.load()</cite> will load CPU data.
Care must be taken when working with views. Instead of saving views it’s
recommended that you recreate them after the tensors have been loaded and
moved to their destination device(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – The input data to be saved. Any nested combination of Python objects
(list, tuples, sets, dicts, …).</p></li>
<li><p><strong>file_or_path</strong> – The destination for the data saving operation. Either a file
path or a Python file object. If <cite>master_only</cite> is <code class="docutils literal notranslate"><span class="pre">False</span></code> the path or
file objects must point to different destinations as otherwise all the
writes from the same host will override each other.</p></li>
<li><p><strong>master_only</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether only the master device should save the
data. If False, the <cite>file_or_path</cite> argument should be a different file or
path for each of the ordinals taking part to the replication, otherwise
all the replicas on the same host will be writing to the same location.
Default: True</p></li>
<li><p><strong>global_master</strong> (<em>bool</em><em>, </em><em>optional</em>) – When <code class="docutils literal notranslate"><span class="pre">master_only</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> this flag
controls whether every host’s master (if <code class="docutils literal notranslate"><span class="pre">global_master</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>)
saves the content, or only the global master (ordinal 0).
Default: False</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.rendezvous">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">rendezvous</code><span class="sig-paren">(</span><em class="sig-param">tag</em>, <em class="sig-param">payload=b''</em>, <em class="sig-param">replicas=[]</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#rendezvous"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.rendezvous" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for all the mesh clients to reach the named rendezvous.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> (<em>string</em>) – The name of the rendezvous to join.</p></li>
<li><p><strong>payload</strong> (<em>bytes</em><em>, </em><em>optional</em>) – The payload to be sent to the rendezvous.</p></li>
<li><p><strong>replicas</strong> (<em>list</em><em>, </em><em>python:int</em>) – The replica ordinals taking part of the rendezvous.
Empty means all replicas in the mesh.
Default: []</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The payloads exchanged by all the other cores, with the payload of core
ordinal <cite>i</cite> at position <cite>i</cite> in the returned tuple.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.do_on_ordinals">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">do_on_ordinals</code><span class="sig-paren">(</span><em class="sig-param">target</em>, <em class="sig-param">data=()</em>, <em class="sig-param">ordinals=(0</em>, <em class="sig-param">)</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#do_on_ordinals"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.do_on_ordinals" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs a function only on a given set of ordinals.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> (<em>callable</em>) – The function to be run on <cite>ordinals</cite>.</p></li>
<li><p><strong>data</strong> – Any input data for the <cite>target</cite> function which contains tensors. All
the XLA tensors used by the <cite>target</cite> function must be passed in this
argument. Every other data used by the function can be captured by the
Python interpreter as usual.
Default: ()</p></li>
<li><p><strong>ordinals</strong> (<em>list</em><em>, </em><em>python:int</em>) – The list/set of ordinals where the <cite>target</cite> function
should run.
Default: (0,)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>In the ordinals that ran the <cite>target</cite> function, the function return value,
otherwise <cite>None</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.mesh_reduce">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">mesh_reduce</code><span class="sig-paren">(</span><em class="sig-param">tag</em>, <em class="sig-param">data</em>, <em class="sig-param">reduce_fn</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#mesh_reduce"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.mesh_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an out-of-graph client mesh reduction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> (<em>string</em>) – The name of the rendezvous to join.</p></li>
<li><p><strong>data</strong> – The data to be reduced. The <cite>reduce_fn</cite> callable will receive a list
with the copies of the same data coming from all the mesh client processes
(one per core).</p></li>
<li><p><strong>reduce_fn</strong> (<em>callable</em>) – A function which receives a list of <cite>data</cite>-like
objects and returns the reduced result.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced value.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.set_rng_state">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">set_rng_state</code><span class="sig-paren">(</span><em class="sig-param">seed</em>, <em class="sig-param">device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#set_rng_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.set_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the random number generator state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seed</strong> (<em>python:integer</em>) – The state to be set.</p></li>
<li><p><strong>device</strong> (<em>string</em><em>, </em><em>optional</em>) – The device where the RNG state needs to be set.
If missing the default device seed will be set.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.get_rng_state">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">get_rng_state</code><span class="sig-paren">(</span><em class="sig-param">device=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_rng_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the current running random number generator state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>string</em><em>, </em><em>optional</em>) – The device whose RNG state needs to be retrieved.
If missing the default device seed will be set.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The RNG state, as integer.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.xla_model.get_memory_info">
<code class="sig-prename descclassname">torch_xla.core.xla_model.</code><code class="sig-name descname">get_memory_info</code><span class="sig-paren">(</span><em class="sig-param">device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/xla_model.html#get_memory_info"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.xla_model.get_memory_info" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the device memory information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>string</em>) – The device whose memory information are requested.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dictionary with <cite>kb_free</cite> (free memory in KB) and <cite>kb_total</cite> (total
memory in KB) keys.</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-torch_xla.core.functions"></span><dl class="function">
<dt id="torch_xla.core.functions.all_reduce">
<code class="sig-prename descclassname">torch_xla.core.functions.</code><code class="sig-name descname">all_reduce</code><span class="sig-paren">(</span><em class="sig-param">reduce_type</em>, <em class="sig-param">value</em>, <em class="sig-param">scale=1.0</em>, <em class="sig-param">groups=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/functions.html#all_reduce"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.functions.all_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an inplace reduce operation on the input tensor.</p>
<p>This is the same as <cite>xm.all_reduce()</cite> but supports autograd differentiation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>reduce_type</strong> (<em>string</em>) – One of <code class="docutils literal notranslate"><span class="pre">REDUCE_SUM</span></code>, <code class="docutils literal notranslate"><span class="pre">REDUCE_MUL</span></code>, <code class="docutils literal notranslate"><span class="pre">REDUCE_AND</span></code>,
<code class="docutils literal notranslate"><span class="pre">REDUCE_OR</span></code>, <code class="docutils literal notranslate"><span class="pre">REDUCE_MIN</span></code> and <code class="docutils literal notranslate"><span class="pre">REDUCE_MAX</span></code>.</p></li>
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – The to perform the all reduce op to.</p></li>
<li><p><strong>scale</strong> (<em>python:float</em>) – A default scaling value to be applied after the reduce.
Default: 1.0</p></li>
<li><p><strong>groups</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>A list of list, representing the replica groups for
the <cite>all_reduce()</cite> operation. Example: <cite>[[0, 1, 2, 3], [4, 5, 6, 7]]</cite></p>
<blockquote>
<div><p>defines two groups, one with the <cite>[0, 1, 2, 3]</cite> replicas and one with
the <cite>[4, 5, 6, 7]</cite> replicas. If <cite>None</cite> there will be only one group with
all the replicas in it.</p>
</div></blockquote>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The reduced value across the selected replicas.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.functions.all_gather">
<code class="sig-prename descclassname">torch_xla.core.functions.</code><code class="sig-name descname">all_gather</code><span class="sig-paren">(</span><em class="sig-param">value</em>, <em class="sig-param">dim=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/functions.html#all_gather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.functions.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs an all-gather operation along a given dimension.</p>
<p>This is the same as <cite>xm.all_gather()</cite> but supports autograd differentiation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> (<em>torch.Tensor</em>) – The input tensor.</p></li>
<li><p><strong>dim</strong> (<em>python:int</em>) – The gather dimension.
Default: 0</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor which has, in the <code class="docutils literal notranslate"><span class="pre">dim</span></code> dimension, all the values from the
participating replicas.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.core.functions.nms">
<code class="sig-prename descclassname">torch_xla.core.functions.</code><code class="sig-name descname">nms</code><span class="sig-paren">(</span><em class="sig-param">boxes</em>, <em class="sig-param">scores</em>, <em class="sig-param">score_threshold</em>, <em class="sig-param">iou_threshold</em>, <em class="sig-param">output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/core/functions.html#nms"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.core.functions.nms" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a Non Maximal Suppression operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>boxes</strong> (<em>torch.Tensor</em>) – A <cite>torch.Tensor</cite> of shape <cite>[N, 4]</cite> listing the boxes
coordinates in <cite>(y0, x0, y1, x1)</cite> form.</p></li>
<li><p><strong>scores</strong> (<em>torch.Tensor</em>) – A <cite>torch.Tensor</cite> of shape <cite>[N]</cite> listing the scores
of each box.</p></li>
<li><p><strong>score_threshold</strong> (<em>torch.Tensor</em>) – The minimum score for a box to qualify as
valid.</p></li>
<li><p><strong>iou_threshold</strong> (<em>torch.Tensor</em>) – The minimum IOU (Intersection Over Union)
score to trigger overlap logic.</p></li>
<li><p><strong>output_size</strong> (<em>python:int</em>) – The maximum number of returned indices (must be lower or
equal to N).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple of <cite>torch.Tensor</cite> with the first element being the selected box
indices, and the second element being the number of valid boxes.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-torch_xla.distributed.parallel_loader">
<span id="distributed"></span><h2>distributed<a class="headerlink" href="#module-torch_xla.distributed.parallel_loader" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torch_xla.distributed.parallel_loader.ParallelLoader">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.distributed.parallel_loader.</code><code class="sig-name descname">ParallelLoader</code><span class="sig-paren">(</span><em class="sig-param">loader</em>, <em class="sig-param">devices</em>, <em class="sig-param">batchdim=0</em>, <em class="sig-param">batches_per_execution=1</em>, <em class="sig-param">loader_prefetch_size=8</em>, <em class="sig-param">device_prefetch_size=4</em>, <em class="sig-param">host_to_device_transfer_threads=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/parallel_loader.html#ParallelLoader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.parallel_loader.ParallelLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Wraps an existing PyTorch DataLoader with background data upload.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loader</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>) – The PyTorch DataLoader to be
wrapped.</p></li>
<li><p><strong>devices</strong> (<cite>torch.device</cite>…) – The list of devices where the data has to be
sent. The i-th sample returned by the <cite>loader</cite> will be sent to <cite>devices[i
% len(devices)]</cite>.</p></li>
<li><p><strong>batchdim</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The dimension which is holding the batch size.
Default: 0</p></li>
<li><p><strong>loader_prefetch_size</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The max capacity of the queue used by
the thread which is reading samples from the <cite>loader</cite>, to be processed by
the worker threads which upload data to the devices.
Default: 8</p></li>
<li><p><strong>device_prefetch_size</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The max size of the per-device queues,
where the worker threads deposit tensors which have already been sent to
devices.
Default: 4</p></li>
<li><p><strong>host_to_device_transfer_threads</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The number of threads that
work in parallel to transfer data from loader queue to device queue.
Default: 1</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch_xla.distributed.parallel_loader.ParallelLoader.per_device_loader">
<code class="sig-name descname">per_device_loader</code><span class="sig-paren">(</span><em class="sig-param">device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/parallel_loader.html#ParallelLoader.per_device_loader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.parallel_loader.ParallelLoader.per_device_loader" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the loader iterator object for the given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<cite>torch.device</cite>) – The device whole loader is being requested.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The loader iterator object for the <cite>device</cite>. This is not a
<cite>torch.utils.data.DataLoader</cite> interface, but a Python iterator which
returns the same tensor data structure as returned by the wrapped
<cite>torch.utils.data.DataLoader</cite>, but residing on XLA devices.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<span class="target" id="module-torch_xla.distributed.xla_multiprocessing"></span><dl class="function">
<dt id="torch_xla.distributed.xla_multiprocessing.spawn">
<code class="sig-prename descclassname">torch_xla.distributed.xla_multiprocessing.</code><code class="sig-name descname">spawn</code><span class="sig-paren">(</span><em class="sig-param">fn</em>, <em class="sig-param">args=()</em>, <em class="sig-param">nprocs=None</em>, <em class="sig-param">join=True</em>, <em class="sig-param">daemon=False</em>, <em class="sig-param">start_method='spawn'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/xla_multiprocessing.html#spawn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.xla_multiprocessing.spawn" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables multi processing based replication.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> (<em>callable</em>) – The function to be called for each device which takes part of
the replication. The function will be called with a first argument being
the global index of the process within the replication, followed by the
arguments passed in <cite>args</cite>.</p></li>
<li><p><strong>args</strong> (<em>tuple</em>) – The arguments for <cite>fn</cite>.
Default: Empty tuple</p></li>
<li><p><strong>nprocs</strong> (<em>python:int</em>) – The number of processes/devices for the replication. At the
moment, if specified, can be either 1 or the maximum number of devices.</p></li>
<li><p><strong>join</strong> (<em>bool</em>) – Whether the call should block waiting for the completion of the
processes which have being spawned.
Default: True</p></li>
<li><p><strong>daemon</strong> (<em>bool</em>) – Whether the processes being spawned should have the <cite>daemon</cite>
flag set (see Python multi-processing API).
Default: False</p></li>
<li><p><strong>start_method</strong> (<em>string</em>) – The Python <cite>multiprocessing</cite> process creation method.
Default: <cite>spawn</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The same object returned by the <cite>torch.multiprocessing.spawn</cite> API. If
<cite>nprocs</cite> is 1 the <cite>fn</cite> function will be called directly, and the API will
return None.</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch_xla.distributed.xla_multiprocessing.MpModelWrapper">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.distributed.xla_multiprocessing.</code><code class="sig-name descname">MpModelWrapper</code><span class="sig-paren">(</span><em class="sig-param">model</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/xla_multiprocessing.html#MpModelWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.xla_multiprocessing.MpModelWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Wraps a model to minimize host memory usage when <cite>fork</cite> method is used.</p>
<p>This class should be used together with the <cite>spawn(…, start_method=’fork’)</cite>
API to minimize the use of host memory.
Instead of creating models on each multiprocessing process, hence replicating
the model’s initial host memory, the model is created once at global scope,
and then moved into each device inside the <cite>spawn()</cite> target function.
Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">WRAPPED_MODEL</span> <span class="o">=</span> <span class="n">xmp</span><span class="o">.</span><span class="n">MpModelWrapper</span><span class="p">(</span><span class="n">MyNetwork</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">_mp_fn</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">WRAPPED_MODEL</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="o">...</span>

<span class="n">xmp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">_mp_fn</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">start_method</span><span class="o">=</span><span class="s1">&#39;fork&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This method has two advantages. First it uses only one copy of the memory
pages to host the original model weights, and second it serializes the move
of the wrapped model into each device, by lowering the load onto the system
memory during the process.</p>
<dl class="method">
<dt id="torch_xla.distributed.xla_multiprocessing.MpModelWrapper.to">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/xla_multiprocessing.html#MpModelWrapper.to"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.xla_multiprocessing.MpModelWrapper.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves the model moved onto the specified device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>torch.device</em>) – The device where the model should be moved onto.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The model on the specified device.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch_xla.distributed.xla_multiprocessing.MpSerialExecutor">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.distributed.xla_multiprocessing.</code><code class="sig-name descname">MpSerialExecutor</code><a class="reference internal" href="_modules/torch_xla/distributed/xla_multiprocessing.html#MpSerialExecutor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.xla_multiprocessing.MpSerialExecutor" title="Permalink to this definition">¶</a></dt>
<dd><p>Utility to run a function in a serialized fashion among multi-core processes.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># At global scope.</span>
<span class="n">SERIAL_EXEC</span> <span class="o">=</span> <span class="n">xmp</span><span class="o">.</span><span class="n">MpSerialExecutor</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">maybe_download_and_load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_mp_fn</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
  <span class="c1"># Avoid all cores downloading the same data with the serial executor.</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">SERIAL_EXEC</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;/tmp/mnist-data&#39;</span><span class="p">))</span>
  <span class="o">...</span>

<span class="n">xmp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">_mp_fn</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch_xla.distributed.xla_multiprocessing.MpSerialExecutor.run">
<code class="sig-name descname">run</code><span class="sig-paren">(</span><em class="sig-param">fn</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/distributed/xla_multiprocessing.html#MpSerialExecutor.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.distributed.xla_multiprocessing.MpSerialExecutor.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs the provided function serialized WRT each per-core process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<em>callable</em>) – The function to run in a serialized fashion.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The <cite>fn</cite> return value.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torch_xla.utils.tf_record_reader">
<span id="utils"></span><h2>utils<a class="headerlink" href="#module-torch_xla.utils.tf_record_reader" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torch_xla.utils.tf_record_reader.TfRecordReader">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.utils.tf_record_reader.</code><code class="sig-name descname">TfRecordReader</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">compression=''</em>, <em class="sig-param">buffer_size=16777216</em>, <em class="sig-param">transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/tf_record_reader.html#TfRecordReader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.tf_record_reader.TfRecordReader" title="Permalink to this definition">¶</a></dt>
<dd><p>Reads TfRecords or TfExamples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>string</em>) – The path to the file containing TfRecords.</p></li>
<li><p><strong>compression</strong> (<em>string</em><em>, </em><em>optional</em>) – The compression type. The empty string for
no compression, otherwise <code class="docutils literal notranslate"><span class="pre">ZLIB</span></code> or <code class="docutils literal notranslate"><span class="pre">GZIP</span></code>.
Default: No compression.</p></li>
<li><p><strong>buffer_size</strong> (<em>python:int</em><em>, </em><em>optional</em>) – The size of the buffer to be used to read
TfRecords.
Default: 16 * 1024 * 1024</p></li>
<li><p><strong>transforms</strong> (<em>dict</em><em>, </em><em>optional</em>) – A dictionary with the key matching the
TfExample label name, and value which is either a callable which will be
called to tranform the matching tensor data, or <code class="docutils literal notranslate"><span class="pre">STR</span></code> for string
conversion.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-torch_xla.utils.utils"></span><dl class="class">
<dt id="torch_xla.utils.utils.SampleGenerator">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.utils.utils.</code><code class="sig-name descname">SampleGenerator</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">sample_count</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/utils.html#SampleGenerator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.utils.SampleGenerator" title="Permalink to this definition">¶</a></dt>
<dd><p>Iterator which returns multiple samples of a given input data.</p>
<p>Can be used in place of a PyTorch <cite>DataLoader</cite> to generate synthetic data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – The data which should be returned at each iterator step.</p></li>
<li><p><strong>sample_count</strong> – The maximum number of <cite>data</cite> samples to be returned.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch_xla.utils.utils.DataWrapper">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.utils.utils.</code><code class="sig-name descname">DataWrapper</code><a class="reference internal" href="_modules/torch_xla/utils/utils.html#DataWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.utils.DataWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Utility class to wrap data structures to be sent to device.</p>
</dd></dl>

<span class="target" id="module-torch_xla.utils.serialization"></span><dl class="function">
<dt id="torch_xla.utils.serialization.save">
<code class="sig-prename descclassname">torch_xla.utils.serialization.</code><code class="sig-name descname">save</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">path</em>, <em class="sig-param">master_only=True</em>, <em class="sig-param">global_master=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/serialization.html#save"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.serialization.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the input data into a file.</p>
<p>The saved data is transferred to PyTorch CPU device before being saved, so a
following <cite>torch.load()</cite> will load CPU data.
Care must be taken when working with views. Instead of saving views it’s
recommended that you recreate them after the tensors have been loaded and
moved to their destination device(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – The input data to be saved. Any nested combination of Python objects
(list, tuples, sets, dicts, …).</p></li>
<li><p><strong>path</strong> – The destination file for the data saving operation. If <cite>master_only</cite>
is <code class="docutils literal notranslate"><span class="pre">False</span></code> the path must point to different destinations as otherwise
all the writes from the same host will override each other.</p></li>
<li><p><strong>master_only</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether only the master device should save the
data. If False, the <cite>path</cite> argument should be a different path for each of
the ordinals taking part to the replication, otherwise all the replicas on
the same host will be writing to the same location.
Default: True</p></li>
<li><p><strong>global_master</strong> (<em>bool</em><em>, </em><em>optional</em>) – When <code class="docutils literal notranslate"><span class="pre">master_only</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> this flag
controls whether every host’s master (if <code class="docutils literal notranslate"><span class="pre">global_master</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>)
saves the content, or only the global master (ordinal 0).
Default: False</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.serialization.load">
<code class="sig-prename descclassname">torch_xla.utils.serialization.</code><code class="sig-name descname">load</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/serialization.html#load"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.serialization.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads data previously saved with the <cite>save()</cite> API.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>str</em>) – The path passed to the <cite>save()</cite> API.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The loaded data.</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-torch_xla.utils.gcsfs"></span><dl class="function">
<dt id="torch_xla.utils.gcsfs.open">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">open</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">mode='r'</em>, <em class="sig-param">encoding=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#open"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.open" title="Permalink to this definition">¶</a></dt>
<dd><p>Opens a Google Cloud Storage (GCS) file for reading or writing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>string</em>) – The GCS path of the file. Must be “gs://BUCKET_NAME/PATH”
where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite>
delimited path.</p></li>
<li><p><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – The open mode, similar to the <code class="docutils literal notranslate"><span class="pre">open()</span></code> API.
Default: ‘r’</p></li>
<li><p><strong>encoding</strong> (<em>string</em><em>, </em><em>optional</em>) – The character encoding to be used to decode
bytes into strings when opening in text mode.
Default: None</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The GCS file object.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.list">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">list</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.list" title="Permalink to this definition">¶</a></dt>
<dd><p>Lists the content of a GCS bucket.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – The GCS path of the file. Must be “gs://BUCKET_NAME/PATH”
where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite>
delimited path.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of <code class="docutils literal notranslate"><span class="pre">GcsBlob</span></code> objects.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.stat">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">stat</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#stat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.stat" title="Permalink to this definition">¶</a></dt>
<dd><p>Fetches the information of a GCS file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – The GCS path of the file. Must be “gs://BUCKET_NAME/PATH”
where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite>
delimited path.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">GcsBlob</span></code> object.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.remove">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">remove</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#remove"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.remove" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes a GCS blob.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – The GCS path of the file. Must be “gs://BUCKET_NAME/PATH”
where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite>
delimited path.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.rmtree">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">rmtree</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#rmtree"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.rmtree" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes all the GCS blobs within a given path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – <p>The GCS path of the file pattern or folder. Must be
“gs://BUCKET_NAME/PATH” where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS</p>
<blockquote>
<div><p>bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite> delimited path.</p>
</div></blockquote>
</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.read">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">read</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#read"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.read" title="Permalink to this definition">¶</a></dt>
<dd><p>Reads the whole content of a GCS blob.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – The GCS path of the file. Must be “gs://BUCKET_NAME/PATH”
where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite>
delimited path.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The bytes stored within the GCS blob.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.write">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">write</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">content</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#write"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.write" title="Permalink to this definition">¶</a></dt>
<dd><p>Write a string/bytes or file into a GCS blob.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>string</em>) – The GCS path of the file. Must be “gs://BUCKET_NAME/PATH”
where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite>
delimited path.</p></li>
<li><p><strong>content</strong> (<em>string</em><em>, </em><em>bytes</em><em> or </em><em>file object</em>) – The content to be written into
<code class="docutils literal notranslate"><span class="pre">path</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.generic_open">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">generic_open</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">mode='r'</em>, <em class="sig-param">encoding=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#generic_open"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.generic_open" title="Permalink to this definition">¶</a></dt>
<dd><p>Opens a file (GCS or not) for reding or writing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>string</em>) – <p>The path of the file to be opened. If a GCS path, it must be
“gs://BUCKET_NAME/PATH” where <code class="docutils literal notranslate"><span class="pre">BUCKET_NAME</span></code> is the name of the GCS</p>
<blockquote>
<div><p>bucket, and <code class="docutils literal notranslate"><span class="pre">PATH</span></code> is a <cite>/</cite> delimited path.</p>
</div></blockquote>
</p></li>
<li><p><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – The open mode, similar to the <code class="docutils literal notranslate"><span class="pre">open()</span></code> API.
Default: ‘r’</p></li>
<li><p><strong>encoding</strong> (<em>string</em><em>, </em><em>optional</em>) – The character encoding to be used to decode
bytes into strings when opening in text mode.
Default: None</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The opened file object.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.generic_read">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">generic_read</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#generic_read"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.generic_read" title="Permalink to this definition">¶</a></dt>
<dd><p>Reads the whole content of the provided location.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – The GCS path or local path to be read.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The bytes stored within the GCS blob or local file.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.generic_write">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">generic_write</code><span class="sig-paren">(</span><em class="sig-param">output_string</em>, <em class="sig-param">path</em>, <em class="sig-param">makedirs=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#generic_write"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.generic_write" title="Permalink to this definition">¶</a></dt>
<dd><p>Write a string/bytes or file into a GCS blob or local disk.</p>
<p>Depending on the path passed in, this API can write to local or GCS file.
Checks if the <cite>path</cite> starts with the ‘gs://’ prefix, and uses <cite>open</cite> otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_string</strong> (<em>string</em>) – The string to be written to the output.</p></li>
<li><p><strong>path</strong> (<em>string</em>) – The GCS path or local path of the output.</p></li>
<li><p><strong>makedirs</strong> (<em>bool</em>) – Whether the <cite>path</cite> parent folders should be created if
missing.
Default: False</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch_xla.utils.gcsfs.is_gcs_path">
<code class="sig-prename descclassname">torch_xla.utils.gcsfs.</code><code class="sig-name descname">is_gcs_path</code><span class="sig-paren">(</span><em class="sig-param">path</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/gcsfs.html#is_gcs_path"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.gcsfs.is_gcs_path" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks whether a path is a GCS path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<em>string</em>) – The path to be checked.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Whether <cite>path</cite> is a GCS path.</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-torch_xla.utils.cached_dataset"></span><dl class="class">
<dt id="torch_xla.utils.cached_dataset.CachedDataset">
<em class="property">class </em><code class="sig-prename descclassname">torch_xla.utils.cached_dataset.</code><code class="sig-name descname">CachedDataset</code><span class="sig-paren">(</span><em class="sig-param">data_set</em>, <em class="sig-param">path</em>, <em class="sig-param">max_files_per_folder=1000</em>, <em class="sig-param">compress=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch_xla/utils/cached_dataset.html#CachedDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch_xla.utils.cached_dataset.CachedDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Wraps an existing <cite>torch.utils.data.Dataset</cite> by providing file caching.</p>
<p>The <cite>CachedDataset</cite> can be used to trade the CPU/RAM resources required to
process a raw dataset, with storage/network resources.
Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span>
    <span class="n">FLAGS</span><span class="o">.</span><span class="n">datadir</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
        <span class="p">[</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
         <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))]))</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">CachedDataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">dscache_dir</span><span class="p">)</span>
</pre></div>
</div>
<p>The <cite>CachedDataset</cite> will transparently cache the original <cite>Dataset</cite> samples,
so that every run after the first, will not trigger any more CPU/RAM usage
related to the raw samples processing.
Once a <cite>CachedDataset</cite> is fully cached, it can be exported (ie, tar.gz) and
used in different machines.
Just unpack the tar.gz and pass <cite>None</cite> as original <cite>Dataset</cite>:
Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">CachedDataset</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">dscache_dir</span><span class="p">)</span>
</pre></div>
</div>
<p>To fully cache <cite>CachedDataset</cite> just run the <cite>warmup()</cite> API.
A <cite>CachedDataset</cite> saved on GCS has the advantage to be able to be used from
different machines without explicit exporting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_set</strong> (<em>torch.utils.data.Dataset</em>) – The raw <cite>torch.utils.data.Dataset</cite> to be
cached. It can be set to <cite>None</cite> in case all the input samples are stored
within the <cite>path</cite> folder.</p></li>
<li><p><strong>path</strong> (<em>string</em>) – The path where the dataset samples should be stored/loaded.
The <cite>path</cite> needs to be writeable, unless all the samples are already stored.
The <cite>path</cite> can be a GCS path (prefixed with <cite>gs://</cite>).</p></li>
<li><p><strong>max_files_per_folder</strong> (<em>python:int</em>) – The maximum amount of files to be stored within a
single folder. If <cite>data_set</cite> is <cite>None</cite> this value is ignored and taken from
the cached metadata.
Default: 1000</p></li>
<li><p><strong>compress</strong> (<em>bool</em>) – Whether the saved samples should be compressed. Compression
saves space at the expense of CPU required to compress/decompress.
If <cite>data_set</cite> is <cite>None</cite> this value is ignored and taken from the cached
metadata.
Default: True</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="test">
<h2>test<a class="headerlink" href="#test" title="Permalink to this headline">¶</a></h2>
</div>
</div>
<div class="section" id="troubleshooting">
<h1>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline">¶</a></h1>
<p>Note that the information in this section is subject to be removed in future releases of the <em>PyTorch/XLA</em> software,
since many of them are peculiar to a given internal implementation which might change.</p>
<p>To diagnose issues, we can use the execution metrics and counters provided by <em>PyTorch/XLA</em>
The <strong>first thing</strong> to check when model is slow is to generate a metrics report.</p>
<p>Metrics report is extremely helpful in diagnosing issues. Please try to include it in your bug
report sent to us if you have it.</p>
<div class="section" id="perform-a-auto-metrics-analysis">
<h2>Perform A Auto-Metrics Analysis<a class="headerlink" href="#perform-a-auto-metrics-analysis" title="Permalink to this headline">¶</a></h2>
<p>We provide ways to automatically analyze the metrics report and provide a summary. Simply run your workload with <code class="docutils literal notranslate"><span class="pre">PT_XLA_DEBUG=1</span></code>. Some example output would be</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pt</span><span class="o">-</span><span class="n">xla</span><span class="o">-</span><span class="n">profiler</span><span class="p">:</span> <span class="n">CompileTime</span> <span class="n">too</span> <span class="n">frequent</span><span class="p">:</span> <span class="mi">21</span> <span class="n">counts</span> <span class="n">during</span> <span class="mi">11</span> <span class="n">steps</span>
<span class="n">pt</span><span class="o">-</span><span class="n">xla</span><span class="o">-</span><span class="n">profiler</span><span class="p">:</span> <span class="n">TransferFromServerTime</span> <span class="n">too</span> <span class="n">frequent</span><span class="p">:</span> <span class="mi">11</span> <span class="n">counts</span> <span class="n">during</span> <span class="mi">11</span> <span class="n">steps</span>
<span class="n">pt</span><span class="o">-</span><span class="n">xla</span><span class="o">-</span><span class="n">profiler</span><span class="p">:</span> <span class="n">Op</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="ow">not</span> <span class="n">lowered</span><span class="p">:</span> <span class="n">aten</span><span class="p">::</span><span class="n">_ctc_loss</span><span class="p">,</span> <span class="n">aten</span><span class="p">::</span><span class="n">_ctc_loss_backward</span><span class="p">,</span>  <span class="n">Please</span> <span class="nb">open</span> <span class="n">a</span> <span class="n">GitHub</span> <span class="n">issue</span> <span class="k">with</span> <span class="n">the</span> <span class="n">above</span> <span class="n">op</span> <span class="n">lowering</span> <span class="n">requests</span><span class="o">.</span>
<span class="n">pt</span><span class="o">-</span><span class="n">xla</span><span class="o">-</span><span class="n">profiler</span><span class="p">:</span> <span class="n">CompileTime</span> <span class="n">too</span> <span class="n">frequent</span><span class="p">:</span> <span class="mi">23</span> <span class="n">counts</span> <span class="n">during</span> <span class="mi">12</span> <span class="n">steps</span>
<span class="n">pt</span><span class="o">-</span><span class="n">xla</span><span class="o">-</span><span class="n">profiler</span><span class="p">:</span> <span class="n">TransferFromServerTime</span> <span class="n">too</span> <span class="n">frequent</span><span class="p">:</span> <span class="mi">12</span> <span class="n">counts</span> <span class="n">during</span> <span class="mi">12</span> <span class="n">steps</span>
</pre></div>
</div>
<p>Following section will explain how to get and understand a more detial metrics report.</p>
</div>
<div class="section" id="get-a-metrics-report">
<h2>Get A Metrics Report<a class="headerlink" href="#get-a-metrics-report" title="Permalink to this headline">¶</a></h2>
<p>Put the following line in your program to generate a report:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.debug.metrics</span> <span class="k">as</span> <span class="nn">met</span>

<span class="c1"># For short report that only contains a few key metrics.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">met</span><span class="o">.</span><span class="n">short_metrics_report</span><span class="p">())</span>
<span class="c1"># For full report that includes all metrics.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">met</span><span class="o">.</span><span class="n">short_metrics_report</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="understand-the-metrics-report">
<h2>Understand The Metrics Report<a class="headerlink" href="#understand-the-metrics-report" title="Permalink to this headline">¶</a></h2>
<p>The report includes things like:</p>
<ul class="simple">
<li><p>how many time we issue <em>XLA</em> compilations and time spent on issuing.</p></li>
<li><p>how many times we execute and time spent on execution</p></li>
<li><p>how many device data handles we create/destroy etc.</p></li>
</ul>
<p>This information is reported in terms of percentiles of the samples. An example is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Metric</span><span class="p">:</span> <span class="n">CompileTime</span>
  <span class="n">TotalSamples</span><span class="p">:</span> <span class="mi">202</span>
  <span class="n">Counter</span><span class="p">:</span> <span class="mi">06</span><span class="n">m09s401ms746</span><span class="o">.</span><span class="mi">001</span><span class="n">us</span>
  <span class="n">ValueRate</span><span class="p">:</span> <span class="mi">778</span><span class="n">ms572</span><span class="o">.</span><span class="mi">062</span><span class="n">us</span> <span class="o">/</span> <span class="n">second</span>
  <span class="n">Rate</span><span class="p">:</span> <span class="mf">0.425201</span> <span class="o">/</span> <span class="n">second</span>
  <span class="n">Percentiles</span><span class="p">:</span> <span class="mi">1</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms32</span><span class="o">.</span><span class="mi">778</span><span class="n">us</span><span class="p">;</span> <span class="mi">5</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms61</span><span class="o">.</span><span class="mi">283</span><span class="n">us</span><span class="p">;</span> <span class="mi">10</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms79</span><span class="o">.</span><span class="mi">236</span><span class="n">us</span><span class="p">;</span> <span class="mi">20</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms110</span><span class="o">.</span><span class="mi">973</span><span class="n">us</span><span class="p">;</span> <span class="mi">50</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms228</span><span class="o">.</span><span class="mi">773</span><span class="n">us</span><span class="p">;</span> <span class="mi">80</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms339</span><span class="o">.</span><span class="mi">183</span><span class="n">us</span><span class="p">;</span> <span class="mi">90</span><span class="o">%=</span><span class="mi">001</span><span class="n">ms434</span><span class="o">.</span><span class="mi">305</span><span class="n">us</span><span class="p">;</span> <span class="mi">95</span><span class="o">%=</span><span class="mi">002</span><span class="n">ms921</span><span class="o">.</span><span class="mi">063</span><span class="n">us</span><span class="p">;</span> <span class="mi">99</span><span class="o">%=</span><span class="mi">21</span><span class="n">s102ms853</span><span class="o">.</span><span class="mi">173</span><span class="n">us</span>
</pre></div>
</div>
<p>We also provide counters, which are named integer variables which track internal software status. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Counter</span><span class="p">:</span> <span class="n">CachedSyncTensors</span>
  <span class="n">Value</span><span class="p">:</span> <span class="mi">395</span>
</pre></div>
</div>
<p>In this report, any counter that starts with <code class="docutils literal notranslate"><span class="pre">aten::</span></code>
indicates a context switch between the XLA device and CPU, which can be a
potential performance optimization area in the model code.</p>
<p>Counters are useful to understand which operations are routed back to the CPU engine of <em>PyTorch</em>.
They are fully qualified with their C++ namespace:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Counter</span><span class="p">:</span> <span class="n">aten</span><span class="p">::</span><span class="n">nonzero</span>
  <span class="n">Value</span><span class="p">:</span> <span class="mi">33</span>
</pre></div>
</div>
<p>If you see <code class="docutils literal notranslate"><span class="pre">aten::</span></code> ops other than <code class="docutils literal notranslate"><span class="pre">nonzero</span></code> and <code class="docutils literal notranslate"><span class="pre">_local_scalar_dense</span></code>, that usually means a missing
lowering in PyTorch/XLA. Feel free to open a feature request for it on <a class="reference external" href="https://github.com/pytorch/xla/issues">GitHub issues</a>.</p>
</div>
<div class="section" id="clar-the-metrics-report">
<h2>Clar The Metrics Report<a class="headerlink" href="#clar-the-metrics-report" title="Permalink to this headline">¶</a></h2>
<p>If you want to clear the metrics between steps/epoches, you can use</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.debug.metrics</span> <span class="k">as</span> <span class="nn">met</span>

<span class="n">met</span><span class="o">.</span><span class="n">clear_all</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="performance-profiling">
<h2>Performance Profiling<a class="headerlink" href="#performance-profiling" title="Permalink to this headline">¶</a></h2>
<p>To profile your workload in depth to undertand bottlenecks please check the following resources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm">Official tutorial</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/pytorch-xla-profiling-colab.ipynb">Colab notebook</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_profile_mp_mnist.py">Sample MNIST training script with profiling</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/xla/blob/master/scripts/capture_profile.py">Utility script for capturing performance profiles</a></p></li>
</ul>
</div>
<div class="section" id="known-performance-caveats">
<h2>Known Performance Caveats<a class="headerlink" href="#known-performance-caveats" title="Permalink to this headline">¶</a></h2>
<p>PyTorch/XLA behaves semantically like regular PyTorch and XLA tensors share the full tensor interface with CPU &amp; GPU tensors.
However, constraints in XLA/hardware and the lazy evaluation model suggest certain patterns might result in bad performance.</p>
<p>If your model shows bad performance, keep in mind the following caveats:</p>
<ol class="arabic">
<li><p><strong>XLA/TPU yield degraded performance with too many recompilations.</strong></p>
<p>XLA compilation is expensive. PyTorch/XLA automatically recompiles the graph every time new shapes are encountered.
Usually models should stabilize within a few steps and you can see huge speedup for the rest of training.</p>
<p>In order to avoid recompilations, not only must shapes be constant, but computations across XLA devices in all hosts should also be constant.</p>
<p><em>Possible sources</em>:</p>
<ul class="simple">
<li><p>Direct or indirect uses of <code class="docutils literal notranslate"><span class="pre">nonzero</span></code> introduce dynamic shapes; for example, masked indexing <code class="docutils literal notranslate"><span class="pre">base[index]</span></code> where <code class="docutils literal notranslate"><span class="pre">index</span></code> is a mask tensor.</p></li>
<li><p>Loops with a different number of iterations between steps can result in different execution graphs, thus require recompilations.</p></li>
</ul>
<p><em>Solution</em>:</p>
<ul class="simple">
<li><p>Tensor shapes should be the same between iterations, or a low number of shape variations should be used.</p></li>
<li><p>Pad tensors to fixed sizes when possible.</p></li>
</ul>
</li>
<li><p><strong>Certain operations don’t have native translations to XLA.</strong></p>
<p>For these operations PyTorch/XLA automatically transfers to the CPU memory, evaluates on CPU, and transfers the result back to the XLA device.
Doing too many such operations during the training step can lead to significant slowdowns.</p>
<p><em>Possible sources</em>:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">item()</span></code> operation explicitly asks to evaluate the result. Don’t use it unless it’s necessary.</p></li>
</ul>
<p><em>Solution</em>:</p>
<ul>
<li><p>For most ops we can lower them to XLA to fix it. Checkout <a class="reference external" href="#metrics-report">metrics report section</a> to find out the missing ops and open a feature request on <a class="reference external" href="https://github.com/pytorch/xla/issues">GitHub</a>.</p></li>
<li><p>Even when a PyTorch tensor is known as a scalar, avoid using <code class="docutils literal notranslate"><span class="pre">tensor.item()</span></code>. Keep it as a tensor and use tensor operations on it.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">torch.where</span></code> to substitute control flow when applicable.
E.g. The control flow with <code class="docutils literal notranslate"><span class="pre">item()</span></code> used in <a class="reference external" href="https://github.com/pytorch/pytorch/blob/de19eeee99a2a282fc441f637b23d8e50c75ecd1/torch/nn/utils/clip_grad.py#L33">clip_grad*norm*</a> is problematic and impacts performance, so we have <a class="reference external" href="https://github.com/pytorch/xla/blob/master/torch_patches/X10-clip_grad.diff">patched</a> <code class="docutils literal notranslate"><span class="pre">clip_grad_norm_</span></code> by calling <code class="docutils literal notranslate"><span class="pre">torch.where</span></code> instead, which gives us a dramatic performance improvement.
.. code-block:: python</p>
<blockquote>
<div><p>…
else:</p>
<blockquote>
<div><p>device = parameters[0].device
total_norm = torch.zeros([], device=device if parameters else None)
for p in parameters:</p>
<blockquote>
<div><p>param_norm = p.grad.data.norm(norm_type) ** norm_type
total_norm.add_(param_norm)</p>
</div></blockquote>
<p>total_norm = (total_norm ** (1. / norm_type))</p>
</div></blockquote>
<p>clip_coef = torch.tensor(max_norm, device=device) / (total_norm + 1e-6)
for p in parameters:</p>
<blockquote>
<div><p>p.grad.data.mul_(torch.where(clip_coef &lt; 1, clip_coef, torch.tensor(1., device=device)))</p>
</div></blockquote>
</div></blockquote>
</li>
</ul>
</li>
<li><p><strong>Iterators in ``torch_xla.distributed.data_parallel`` may drop the last few batches in the input iterator.</strong></p>
<p>This is to make sure we do the same amount of work on all XLA devices.</p>
<p><em>Solution</em>:</p>
<ul class="simple">
<li><p>When dataset is small, and there are too few steps, this may result in a no-op epoch. Therefore, it is better to use
small batch sizes in those cases.</p></li>
</ul>
</li>
</ol>
</div>
<div class="section" id="xla-tensor-quirks">
<h2>XLA Tensor Quirks<a class="headerlink" href="#xla-tensor-quirks" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p><strong>XLA tensor internals are opaque.</strong> XLA tensors always appear to be
contiguous and without storage. Networks should not try to check the strides
of XLA tensors.</p></li>
<li><p><strong>XLA tensors should be moved to the CPU before saving them.</strong> Saving
XLA tensors directly causes them to be loaded back on the device(s) they were
saved from. If a device is unavailable at load time then the load will fail.
Moving XLA tensors to the CPU before saving them lets you decide which
device(s) to put the loaded tensors on. This is necessary if you want to
load the tensors on a machine without XLA devices. Care should be taken
moving the XLA tensors to the CPU before saving them, however, as moving
tensors across device types does not preserve view relationships. Instead,
views should be reconstructed as necessary after the tensors are loaded.</p></li>
<li><p><strong>Copying an XLA Tensor with Python’s copy.copy returns a deep copy, not a
shallow copy.</strong> Use a view of an XLA tensor to get a shallow copy of it.</p></li>
<li><p><strong>Handling shared weights.</strong> Modules can share weights by setting the
Parameters of one module to another. This “tying” of module weights should
be done <strong>AFTER</strong> the modules are moved to an XLA device. Otherwise two
independent copies of the shared tensor will be made on the XLA device.</p></li>
</ol>
</div>
<div class="section" id="more-debugging-tools">
<h2>More Debugging Tools<a class="headerlink" href="#more-debugging-tools" title="Permalink to this headline">¶</a></h2>
<p>We don’t expect users to use tools in this section to debug their models. But we might ask for
them when you submit a bug report since they provide additional information that metrics report
doesn’t have.</p>
<div class="section" id="environment-variables">
<h3>Environment Variables<a class="headerlink" href="#environment-variables" title="Permalink to this headline">¶</a></h3>
<p>There are also a number of environment variables which control the behavior of the <em>PyTorch/XLA</em>
software stack.</p>
<p>Setting such variables will cause different degrees of performance degradation, so they should
only be enabled for debugging.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_IR_DEBUG</span></code>: Enables the <em>Python</em> stack trace to be captured where creating IR nodes,
hence allowing to understand which <em>PyTorch</em> operation was responsible for generating the IR.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_HLO_DEBUG</span></code>: Enables the <em>Python</em> stack frame captured when _XLA_IR<em>DEBUG</em> is active,
to be propagated to the <em>XLA</em> <em>HLO</em> metadata.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_SAVE_TENSORS_FILE</span></code>: The path to a file which will be used to dump the IR graphs during
execution. Note that the file can become really big if the option is left enabled and the
<em>PyTorch</em> program let run for long time. The graphs are appended to the file, so to have a clean
sheet from run to run, the file should be explicitly removed.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_SAVE_TENSORS_FMT</span></code>: The format of the graphs stored within the _XLA_SAVE_TENSORS<em>FILE</em>
file. Can be <code class="docutils literal notranslate"><span class="pre">text</span></code> (the default), <code class="docutils literal notranslate"><span class="pre">dot</span></code> (the <em>Graphviz</em> format) or <code class="docutils literal notranslate"><span class="pre">hlo</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_METRICS_FILE</span></code>: If set, the path to a local file where the internal metrics will be
saved at every step. Metrics will be appended to the file, if already existing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_SAVE_HLO_FILE</span></code>: If set, the path to a local file where, in case of compilation/execution
error, the offending HLO graph will be saved.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_GET_TENSORS_OPBYOP</span></code>: Enables pure <em>OpByOp</em> dispatch. The <em>PyTorch/XLA</em> software tries to
fuse together many <em>PyTorch</em> operations into a single computation graph, but sometimes, either
for debugging, or in case the <em>PyTorch</em> code have a very dynamic nature (in shapes or graph
terms), it is better to force the execution in <em>OpByOp</em> mode (every IR node is lowered into
a separate <em>XLA</em> computation, and chain-executed). This environment variable, if set to 1,
enables <em>OpByOp</em> during the “get tensors” operation (the operation used by <em>PyTorch/XLA</em> to
fetch intermediate values back from the <em>TPU</em> device into <em>PyTorch</em> CPU tensors).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_SYNC_TENSORS_OPBYOP</span></code>: The same as _XLA_GET_TENSORS<em>OPBYOP</em> but for “sync tensors”
operation (the operation used at the end of a step, to flush pending IR computations and
materialize them into <em>TPU</em> device data).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_SYNC_WAIT</span></code>: Forces the XLA tensor sync operation to wait for its completion, before
moving to the next step.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_USE_BF16</span></code>: If set to 1, tranforms all the <em>PyTorch</em> <em>Float</em> values into <em>BiFloat16</em>
when sending to the <em>TPU</em> device. Note that when using <code class="docutils literal notranslate"><span class="pre">XLA_USE_BF16=1</span></code> tensor arithmetic will
be done in reduced precision and so tensors will not be accurate if accumulated over time.
For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># In reduced bfloat16 precision</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">4096.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="c1"># Whereas in full float32 precision</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">4096</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">4097</span><span class="p">)</span>
</pre></div>
</div>
<p>So to get accurate metrics such as average loss value over many steps, use manual mixed
precision where metrics stay in FP32.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_USE_F16</span></code>: If set to 1, tranforms all the <em>PyTorch</em> <em>Float</em> values into <em>Float16</em>
(<em>PyTorch</em> <em>Half</em> type) when sending to devices which supports them.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_USE_32BIT_LONG</span></code>: If set to 1, maps <em>PyTorch</em> <em>Long</em> types to <em>XLA</em> 32bit type.
On the versions of the TPU HW at the time of writing, 64bit integer computations are
expensive, so setting this flag might help. It should be verified by the user that truncating
to 32bit values is a valid operation according to the use of <em>PyTorch</em> <em>Long</em> values in it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TF_CPP_LOG_THREAD_ID</span></code>: If set to 1, the TF logs will show the thread ID
helping with debugging multithreaded processes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TF_CPP_VMODULE</span></code>: Environment variable used for TF VLOGs and takes the
form of <code class="docutils literal notranslate"><span class="pre">TF_CPP_VMODULE=name=value,...</span></code>. Note that for VLOGs you must set
<code class="docutils literal notranslate"><span class="pre">TF_CPP_MIN_LOG_LEVEL=0</span></code>. For PyTorch/XLA using a configuration like
<code class="docutils literal notranslate"><span class="pre">TF_CPP_VMODULE=tensor=5</span></code> would enable logging such as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>2019-10-03 17:23:56.419040: I   27891 torch_xla/csrc/tensor.cpp:1104]
Executing IR graph hash 4211381954965020633 on device TPU:3 done!
2019-10-03 17:23:56.419448: I   27890 torch_xla/csrc/tensor.cpp:1104]
Executing IR graph hash 15483856951158150605 on device TPU:5 done!
2019-10-03 17:23:56.419539: I   27896 torch_xla/csrc/tensor.cpp:1104]
Executing IR graph hash 4211381954965020633 on device TPU:4 done!
...
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">TF_CPP_MIN_LOG_LEVEL</span></code>: Level to print messages for. <code class="docutils literal notranslate"><span class="pre">TF_CPP_MIN_LOG_LEVEL=0</span></code> will turn
on INFO logging, <code class="docutils literal notranslate"><span class="pre">TF_CPP_MIN_LOG_LEVEL=1</span></code> WARNING and so on. Our PyTorch/XLA <code class="docutils literal notranslate"><span class="pre">TF_VLOG</span></code> uses
<code class="docutils literal notranslate"><span class="pre">tensorflow::INFO</span></code> level by default so to see VLOGs set <code class="docutils literal notranslate"><span class="pre">TF_CPP_MIN_LOG_LEVEL=0</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XLA_DUMP_HLO_GRAPH</span></code>: If set to <code class="docutils literal notranslate"><span class="pre">=1</span></code> in case of a compilation or execution error the
offending HLO graph will be dumped as part of the runtime error raised by <code class="docutils literal notranslate"><span class="pre">xla_util.cc</span></code>.</p></li>
</ul>
</div>
<div class="section" id="retrieving-stack-traces">
<h3>Retrieving Stack Traces<a class="headerlink" href="#retrieving-stack-traces" title="Permalink to this headline">¶</a></h3>
<p>In the event that the <em>PyTorch</em> process is hanging, it might be useful to include the stack
traces together with the GitHub issue.</p>
<p>First thing is to find out which PID the <em>PyTorch</em> process is associated with. Using the <code class="docutils literal notranslate"><span class="pre">ps</span></code>
command it is possible to find that information. It will be a <em>python</em> process running your
main <em>python</em> file.</p>
<p>In order to allow <em>GDB</em> to attach a user process the following command should be run as root:</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span><span class="nb">echo</span> <span class="m">0</span> &gt; /proc/sys/kernel/yama/ptrace_scope
</pre></div>
</div>
<p>The above command remains active until the machine is rebooted.</p>
<p>The, given the PID, it is possible to grab the stack traces with the following command:</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>./scripts/dump_stacks.py PID &gt; /tmp/stack-traces.log
</pre></div>
</div>
</div>
</div>
<div class="section" id="using-debug-run-py-to-collect-debug-information">
<h2>Using debug_run.py To Collect Debug Information<a class="headerlink" href="#using-debug-run-py-to-collect-debug-information" title="Permalink to this headline">¶</a></h2>
<p>A utility is provided in <code class="docutils literal notranslate"><span class="pre">scripts/debug_run.py</span></code> which can be used to create a <code class="docutils literal notranslate"><span class="pre">tar.gz</span></code>
archive with the information required to debug <em>PyTorch/XLA</em> executions.</p>
<p>Example:</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>./scripts/debug_run.py --outfile /tmp/debug_run.tar.gz -- python -u SCRIPT <span class="o">[</span>ARGS...<span class="o">]</span>
</pre></div>
</div>
<p>The <em>python</em> <code class="docutils literal notranslate"><span class="pre">-u</span></code> flag is suggested to disable buffering so that captured logs are correctly
interleaved (otherwise STDOUT will be rendered after all STDERR).</p>
<p>The above command line example will leave the temporary folder containing the archived
information on the filesystem. Use the <code class="docutils literal notranslate"><span class="pre">--tidy</span></code> flag to have that removed on exit:</p>
<div class="highlight-Shell notranslate"><div class="highlight"><pre><span></span>./scripts/debug_run.py --tidy --outfile /tmp/debug_run.tar.gz -- python -u SCRIPT <span class="o">[</span>ARGS...<span class="o">]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">debug_run.tar.gz</span></code> file should then be attached to bug reports when necessary.</p>
<p>Since the script will collect a lot of data, it should usually be let run for no more
than hundred steps or so.</p>
<p>If the SCRIPT has arguments to control the number of steps, those should be used,
otherwise hitting <code class="docutils literal notranslate"><span class="pre">CTRL^C</span></code> will interrupt the run.</p>
<p>It is also sugested to run in single-core mode, to minimize the amount of data.
Running in single-core mode is also strongly suggested when debugging execution issues.</p>
</div>
<div class="section" id="common-issues">
<h2>Common Issues<a class="headerlink" href="#common-issues" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Missing</span> <span class="pre">XLA</span> <span class="pre">configuration</span></code> error message: You need to set <code class="docutils literal notranslate"><span class="pre">XRT_TPU_CONFIG</span></code> if using TPUs. If using GPUs set <code class="docutils literal notranslate"><span class="pre">GPU_NUM_DEVICES=N</span></code> for <code class="docutils literal notranslate"><span class="pre">N</span></code> number of GPUs. If using CPUs set <code class="docutils literal notranslate"><span class="pre">XRT_DEVICE_MAP=&quot;CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">XRT_WORKERS=&quot;localservice:0;grpc://localhost:9002&quot;</span></code></p></li>
</ul>
</div>
</div>
<div class="section" id="pjrt-runtime-beta">
<h1>PJRT Runtime (Beta)<a class="headerlink" href="#pjrt-runtime-beta" title="Permalink to this headline">¶</a></h1>
<p><em>This document reflects the current state of PJRT support in current nightly
builds</em>. See the <a class="reference external" href="https://github.com/pytorch/xla/blob/r2.0/docs/pjrt.md">same document on the r2.0 branch</a>
for the status in the latest stable release.</p>
<p>The PyTorch/XLA team is currently migrating from the currently-supported XRT
runtime to the <a class="reference external" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler/xla/pjrt">PJRT
runtime</a>
used by <a class="reference external" href="https://github.com/google/jax">JAX</a>.</p>
<p>PJRT is available for preview in PyTorch/XLA 2.0. <strong>We are planning to make
PJRT our officially supported runtime</strong>, so we encourage all users to experiment
with it. We aim to make PJRT stable in release 2.1, so if you encounter a bug
with PJRT, please file an issue on GitHub with the <code class="docutils literal notranslate"><span class="pre">runtime</span></code> tag.</p>
<p><em>New features in PyTorch/XLA r2.0</em>:</p>
<ul class="simple">
<li><p>PJRT will be configured by default if you don’t pass in any other runtime
configuration. If you continue to set XRT configuration (<code class="docutils literal notranslate"><span class="pre">XRT_TPU_CONFIG</span></code>),
this change has no impact</p></li>
<li><p>New TPU runtime implementation in <code class="docutils literal notranslate"><span class="pre">libtpu</span></code> improves performance by up to 30%.</p></li>
<li><p>New <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> implementation that scales to thousands of TPU cores</p></li>
<li><p>[experimental] <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> support for TPU v2 and v3, including
<code class="docutils literal notranslate"><span class="pre">pjrt://</span></code> <code class="docutils literal notranslate"><span class="pre">init_method</span></code></p></li>
<li><p>[experimental] Single-host GPU support in PJRT. Multi-host support coming
soon!</p></li>
</ul>
<div class="section" id="tl-dr">
<h2>TL;DR<a class="headerlink" href="#tl-dr" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>To use the PJRT preview runtime, set the <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE</span></code> environment variable to
<code class="docutils literal notranslate"><span class="pre">CPU</span></code>, <code class="docutils literal notranslate"><span class="pre">TPU,</span> <span class="pre">or</span></code>GPU`</p></li>
<li><p>In XRT, all distributed workloads are multiprocess, with one process per
device. On TPU v2 and v3 in PJRT, workloads are multiprocess and multithreaded
(4 processes with 2 threads each), so your workload should be thread-safe. See
<a class="reference external" href="#multithreading-on-tpu-v2v3">Multithreading on TPU v2/v3</a> and the
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/API_GUIDE.md#running-on-multiple-xla-devices-with-multi-processing">Multiprocessing section of the API
guide</a>
for more information. Key differences to keep in mind:</p>
<ul>
<li><p>To initialize a model in a thread-safe way, either broadcast the parameters
across replicas after initialization
(<code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.pjrt.broadcast_master_param</span></code>) or load each
replica’s parameters from a common checkpoint.</p></li>
<li><p>For other random number generation, use <code class="docutils literal notranslate"><span class="pre">torch.Generator</span></code> where possible.
The global <code class="docutils literal notranslate"><span class="pre">torch</span></code> RNG is <em>not</em> thread-safe, even if you set the same
<code class="docutils literal notranslate"><span class="pre">torch.manual_seed</span></code> across replicas.</p></li>
<li><p>To use <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code>, import <code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.pjrt_backend</span></code> and
use the <code class="docutils literal notranslate"><span class="pre">pjrt://</span></code> <code class="docutils literal notranslate"><span class="pre">init_method</span></code>.</p></li>
<li><p>These steps are optional for GPU and TPU v4.</p></li>
</ul>
</li>
</ul>
<p>Sample diff from XRT to PJRT:</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span> import os

 import torch
 import torch.nn as nn
 from torch.nn.parallel import DistributedDataParallel as DDP
 import torch.optim as optim
 import torch.distributed as dist
 import torch_xla.core.xla_model as xm
 import torch_xla.distributed.parallel_loader as pl
 import torch_xla.distributed.xla_backend
 import torch_xla.distributed.xla_multiprocessing as xmp
<span class="gi">+import torch_xla.experimental.pjrt_backend</span>
<span class="gi">+import torch_xla.experimental.pjrt as pjrt</span>


 def _mp_fn(index):
   device = xm.xla_device()
<span class="gd">-  dist.init_process_group(&#39;xla&#39;, rank=xm.get_ordinal(), world_size=xm.xrt_world_size())</span>
<span class="gi">+  dist.init_process_group(&#39;xla&#39;, init_method=&#39;pjrt://&#39;)</span>

   torch.manual_seed(42)
   model = nn.Linear(128, 10).to(device)

<span class="gi">+  # Optional for TPU v4 and GPU</span>
<span class="gi">+  pjrt.broadcast_master_param(model)</span>
   model = DDP(model, gradient_as_bucket_view=True)

   loss_fn = nn.MSELoss()
   optimizer = optim.SGD(model.parameters(), lr=.001)

   for i in range(10):
     data, target = torch.randn((128, 128), device=device), torch.randn((128, 10), device=device)

     optimizer.zero_grad()
     output = model(data)
     loss = loss_fn(output, target)
     loss.backward()

     optimizer.step()
     xm.mark_step()

   # Print mean parameters so we can confirm they&#39;re the same across replicas
   print([p.mean() for p in model.parameters()])

 if __name__ == &#39;__main__&#39;:
<span class="gd">-  os.environ[&#39;XRT_TPU_CONFIG&#39;] = &#39;localservice;0;localhost:51011&#39;</span>
<span class="gd">-  os.environ[&#39;MASTER_ADDR&#39;] = &#39;localhost&#39;</span>
<span class="gd">-  os.environ[&#39;MASTER_PORT&#39;] = &#39;12355&#39;</span>

<span class="gi">+  # Recommended: set PJRT_DEVICE to your local device type</span>
<span class="gi">+  os.environ[&#39;PJRT_DEVICE&#39;] = &#39;TPU&#39;</span>

   xmp.spawn(_mp_fn)
</pre></div>
</div>
</div>
<div class="section" id="benefits">
<h2>Benefits<a class="headerlink" href="#benefits" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Simple runtime configuration: just set <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE</span></code> to <code class="docutils literal notranslate"><span class="pre">TPU</span></code>, <code class="docutils literal notranslate"><span class="pre">CPU</span></code>, or <code class="docutils literal notranslate"><span class="pre">GPU</span></code>
and start using XLA! Or, let PJRT select a device automatically based on your
environment.</p></li>
<li><p>Improved performance: reduced overhead from gRPC means faster end-to-end
execution. On TorchBench 2.0, we observed a &gt;35% improvement in training time
on TPU v4.</p></li>
<li><p>Easy pod execution: just copy your code to each TPU worker, and execute them
all at the same time with <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">compute</span> <span class="pre">tpus</span> <span class="pre">tpuvm</span> <span class="pre">ssh</span> <span class="pre">--worker=all</span></code>.</p></li>
<li><p>Better scaling: removes <a class="reference external" href="https://github.com/pytorch/xla/pull/3920">XRT’s limitation on parameter
sizes</a> and supports up to 2048 TPU
chips.</p></li>
</ul>
</div>
<div class="section" id="quickstart">
<h2>Quickstart<a class="headerlink" href="#quickstart" title="Permalink to this headline">¶</a></h2>
<p>To start using PJRT with PyTorch/XLA, all you need to do is set the
<code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE</span></code> environment variable. If you’re working on a TPU v2 or v3, keep
reading to learn about the differences between TPU v2 and v3 and v4.</p>
<div class="section" id="cpu">
<h3>CPU<a class="headerlink" href="#cpu" title="Permalink to this headline">¶</a></h3>
<p>On any machine with PyTorch/XLA installed, you can run our MNIST example on CPU
like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">CPU</span> <span class="n">python3</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_mnist</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">fake_data</span>
</pre></div>
</div>
</div>
<div class="section" id="tpu">
<h3>TPU<a class="headerlink" href="#tpu" title="Permalink to this headline">¶</a></h3>
<p>To create a new TPU with PyTorch/XLA r1.13 installed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gcloud alpha compute tpus tpu-vm create $USER-pjrt --accelerator-type=v4-8 --version=tpu-vm-v4-pt-1.13 --zone=us-central2-b --project=$PROJECT
</pre></div>
</div>
<p>On a v4-8, you can run our ResNet50 example like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="o">--</span><span class="n">depth</span><span class="o">=</span><span class="mi">1</span> <span class="o">--</span><span class="n">branch</span> <span class="n">r1</span><span class="o">.</span><span class="mi">13</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">xla</span><span class="o">.</span><span class="n">git</span>
<span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">TPU</span> <span class="n">python3</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>By default, PJRT will use all TPU chips. To use only one TPU chip, configure
<code class="docutils literal notranslate"><span class="pre">TPU_PROCESS_BOUNDS</span></code> and <code class="docutils literal notranslate"><span class="pre">TPU_VISIBLE_CHIPS</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TPU_PROCESS_BOUNDS</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span> <span class="n">TPU_VISIBLE_CHIPS</span><span class="o">=</span><span class="mi">0</span> <span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">TPU</span> <span class="n">python3</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<div class="section" id="pods">
<h4>Pods<a class="headerlink" href="#pods" title="Permalink to this headline">¶</a></h4>
<p>On TPU Pods, use <code class="docutils literal notranslate"><span class="pre">gcloud</span></code> to run your command on each TPU in parallel:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command=&quot;git clone --depth=1 --branch r1.13 https://github.com/pytorch/xla.git&quot;
gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command=&quot;PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=256 --num_epochs=1&quot;
</pre></div>
</div>
</div>
<div class="section" id="docker">
<h4>Docker<a class="headerlink" href="#docker" title="Permalink to this headline">¶</a></h4>
<p>You can also use Docker to run your workload in a container with PyTorch/XLA
preinstalled:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export DOCKER_IMAGE=gcr.io/...

# Optional: authenticate docker if your image is in a private GCP repository
gcloud compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command &quot;sudo gcloud auth configure-docker&quot;

# Run your workload
gcloud compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command &quot;sudo docker run --rm --privileged --net=host -e PJRT_DEVICE=TPU $DOCKER_IMAGE python pytorch/xla/test/test_train_mp_imagenet.py --fake_data&quot;
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> requires privileged access to the host (<code class="docutils literal notranslate"><span class="pre">--privileged</span></code>)
to expose the TPU device to the container. Docker on TPU pods is only supported
with host networking <code class="docutils literal notranslate"><span class="pre">--net=host</span></code> at this time. See the <a class="reference external" href="https://cloud.google.com/tpu/docs/run-in-container">Cloud TPU documentation</a>
for more information.</p>
</div>
</div>
<div class="section" id="gpu">
<h3>GPU<a class="headerlink" href="#gpu" title="Permalink to this headline">¶</a></h3>
<p><em>Warning: GPU support is still highly experimental!</em></p>
<p>To use GPUs with PJRT, simply set <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE=GPU</span></code> and configure
<code class="docutils literal notranslate"><span class="pre">GPU_NUM_DEVICES</span></code> to the number of devices on the host. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">GPU</span> <span class="n">GPU_NUM_DEVICES</span><span class="o">=</span><span class="mi">4</span> <span class="n">python3</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_imagenet</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span> <span class="o">--</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
<p>Currently, only a single host is supported, and multi-host GPU cluster support
will be added in an future release.</p>
</div>
</div>
<div class="section" id="differences-from-xrt">
<h2>Differences from XRT<a class="headerlink" href="#differences-from-xrt" title="Permalink to this headline">¶</a></h2>
<p>Although in most cases we expect PJRT and XRT to work mostly interchangeably
from the end-user’s perspective (especially on TPU v4), there are some subtle
differences that are important to keep in mind. Importantly, XRT was designed
around the TPU Node architecture, so it will always spawn a client and a server
process, even on TPU VMs. Thus, every batch of inputs has additional latency
from serializing and deserializing data to send it over the network.</p>
<p>PJRT uses the local device directly with no intermediate server process. In the
default configuration, PJRT will create one process per TPU chip, or 4 processes
per TPU host. See the <a class="reference external" href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">Cloud TPU
documentation</a> for
more information about TPU architecture.</p>
<ul class="simple">
<li><p>Performance gains are possible for workloads constrained overhead from .</p></li>
<li><p>Under XRT, the server process is the only process that interacts with the TPU
devices, and client processes don’t have direct access to the TPU devices.
When profiling a single-host TPU (e.g. v3-8 or v4-8), you would normally see 8
device traces (one for each TPU core). With PJRT, each process has one chip,
and a profile from that process will show only 2 TPU cores.</p>
<ul>
<li><p>For the same reason, profiling does not work on TPU Pods with XRT, because
the server process runs independently from the user’s model code. PJRT does
not have that constraint, so it is possible to profile 2 TPU cores per
process in a TPU Pod.</p></li>
</ul>
</li>
<li><p>PJRT only supports the TPU VM architecture and we have no plans to support the
TPU Node architecture with PJRT.</p></li>
<li><p>Runtime configuration is significantly simpler with PJRT. <code class="docutils literal notranslate"><span class="pre">xla_dist</span></code> is not
required to run TPU Pod workloads. Instead, copy your code to each TPU host
(<code class="docutils literal notranslate"><span class="pre">[gcloud</span> <span class="pre">compute</span> <span class="pre">tpus</span> <span class="pre">tpu-vm</span>
<span class="pre">scp](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/scp)</span></code>)
and run the code on each host in parallel (e.g. <code class="docutils literal notranslate"><span class="pre">[gcloud</span> <span class="pre">compute</span> <span class="pre">tpus</span> <span class="pre">tpu-vm</span>
<span class="pre">ssh</span> <span class="pre">--workers=all</span> <span class="pre">--command=&quot;PJRT_DEVICE=TPU</span> <span class="pre">python</span>
<span class="pre">run.py&quot;](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/ssh)</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> has been reimplemented using XLA-native collective
communication to enhance stability on large TPU pods. See below for more
details.</p></li>
</ul>
<div class="section" id="id6">
<h3>Multithreading on TPU v2/v3<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>On TPU v2 and v3, <strong>distributed workloads always run multithreaded</strong>, since each
TPU core exposes two TPU cores as devices and only one process may open a TPU
chip at a time. In its default configuration, <code class="docutils literal notranslate"><span class="pre">xmp.spawn</span></code> automatically spawns
as many processes as possible (4 per TPU host) and creates two threads per
process (one per TPU core).</p>
<p>Note: on TPU v4, each TPU chip is represented as one PyTorch device, so
distributed workloads will run across 4 processes, each with only one thread.
This is identical to XRT’s behavior.</p>
<p>In most cases, this will not require substantial changes to your existing code.
The main change you will have to make in most cases is to model initialization.
Because <code class="docutils literal notranslate"><span class="pre">torch</span></code>’s global RNG is shared between threads, results will vary
between threads and runs even if you set <code class="docutils literal notranslate"><span class="pre">torch.manual_seed</span></code> to the same value
in every replica. To get consistent parameters between replicas, either use
<code class="docutils literal notranslate"><span class="pre">torch_xla.experimental.pjrt.broadcast_master_param</span></code> to broadcast one replica’s
parameters to all other replicas, or load each replica’s parameters from a
common checkpoint.</p>
</div>
<div class="section" id="changes-to-xm-rendezvous">
<h3>Changes to xm.rendezvous<a class="headerlink" href="#changes-to-xm-rendezvous" title="Permalink to this headline">¶</a></h3>
<p><em>New in PyTorch/XLA r2.0</em></p>
<p>With XRT, worker 0 runs a mesh master service, and all processes on all workers
connect to that service over gRPC. In practice, we found that running a single
mesh master process was unreliable on TPU pods with thousands of chips due to
the number of inbound connections to worker 0. A single client process timing
out could cause a failure and force the entire workload to restart.</p>
<p>Thus, we have reimplemented <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> with native XLA collective
communication, which is much more stable and well-tested on large TPU pods. This
imposes two new constraints compared to the XRT implementation:</p>
<ul class="simple">
<li><p>Because the payload has to become part of the XLA graph, <code class="docutils literal notranslate"><span class="pre">xm.mark_step</span></code> is
called both before and after the data is transferred. Calling <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code>
in the middle of model code may force an unwanted compilation.</p></li>
<li><p>Because XLA does not permit collective operations to run on a subset of
workers, all workers must participate in the <code class="docutils literal notranslate"><span class="pre">rendezvous</span></code>.</p></li>
</ul>
<p>If you require the old behavior of <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> (i.e. communicating data
without altering the XLA graph and/or synchronizing a subset of workers),
consider using
<cite>``torch.distributed.barrier`</cite> &lt;<a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.barrier">https://pytorch.org/docs/stable/distributed.html#torch.distributed.barrier</a>&gt;`_
or
<code class="docutils literal notranslate"><span class="pre">[torch.distributed.all_gather_object](https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather_object)</span></code>
with a <code class="docutils literal notranslate"><span class="pre">gloo</span></code> process group. If you are also using the <code class="docutils literal notranslate"><span class="pre">xla</span></code> <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code>
backend, you can use <code class="docutils literal notranslate"><span class="pre">torch.new_group</span></code> to create a <code class="docutils literal notranslate"><span class="pre">gloo</span></code> subgroup. See <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#monitored-barrier">this
example</a>
from the PyTorch documentation. Keep in mind these constraints:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> is not fully supported on TPU v2/v3. Only a subset of
operations with the <code class="docutils literal notranslate"><span class="pre">xla</span></code> backend are implemented, and <code class="docutils literal notranslate"><span class="pre">gloo</span></code> will likely not
work as expected in a multiprocessing context.</p></li>
<li><p>In our experiments, <code class="docutils literal notranslate"><span class="pre">gloo</span></code> does not scale well to thousands of TPU chips, so
expect this alternative to be less reliable than using <code class="docutils literal notranslate"><span class="pre">xm.rendezvous</span></code> with
PJRT at large scales.</p></li>
</ul>
</div>
<div class="section" id="pjrt-and-torch-distributed">
<h3>PJRT and torch.distributed<a class="headerlink" href="#pjrt-and-torch-distributed" title="Permalink to this headline">¶</a></h3>
<p><em>New in PyTorch/XLA r2.0</em></p>
<p>When using PJRT with <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> and
<code class="docutils literal notranslate"><span class="pre">[torch.nn.parallel.DistributedDataParallel](https://github.com/pytorch/xla/blob/master/docs/ddp.md)</span></code>
we strongly recommend using the new <code class="docutils literal notranslate"><span class="pre">pjrt://</span></code> <code class="docutils literal notranslate"><span class="pre">init_method</span></code>, which automatically
finds the replica IDs, world size, and master IP by querying the runtime. For
example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_multiprocessing</span> <span class="k">as</span> <span class="nn">xmp</span>
<span class="kn">from</span> <span class="nn">torch_xla.experimental</span> <span class="kn">import</span> <span class="n">pjrt</span>

<span class="c1"># Required for `pjrt://` init_method</span>
<span class="kn">import</span> <span class="nn">torch_xla.experimental.pjrt_backend</span>

<span class="k">def</span> <span class="nf">_all_gather</span><span class="p">(</span><span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
  <span class="c1"># No need to pass in `rank` or `world_size`</span>
  <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s1">&#39;xla&#39;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;pjrt://&#39;</span><span class="p">)</span>

  <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">index</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
  <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
  <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

  <span class="n">xm</span><span class="o">.</span><span class="n">mark_step</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="n">xmp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">_all_gather</span><span class="p">)</span>
</pre></div>
</div>
<p>Note: Although the <code class="docutils literal notranslate"><span class="pre">pjrt://</span></code> init_method is not required on TPU v4, it is still
recommended. If you use <code class="docutils literal notranslate"><span class="pre">env://</span></code>, <code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code> must be set to IP host that has
device 0, which is <em>not</em> always worker 0. The <code class="docutils literal notranslate"><span class="pre">pjrt://</span></code> init_method finds this
IP automatically and supports TPU v2/v3.</p>
<p>For more information about using <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> on PyTorch/XLA, see
<cite>``ddp.md`</cite> &lt;./ddp.md&gt;`_ on TPU V4. For an example that uses DDP and PJRT together,
run the following <a class="reference external" href="../test/test_train_mp_imagenet.py">example script</a> on a TPU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PJRT_DEVICE</span><span class="o">=</span><span class="n">TPU</span> <span class="n">python</span> <span class="n">xla</span><span class="o">/</span><span class="n">test</span><span class="o">/</span><span class="n">test_train_mp_mnist</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">ddp</span> <span class="o">--</span><span class="n">pjrt_distributed</span> <span class="o">--</span><span class="n">fake_data</span> <span class="o">--</span><span class="n">num_epochs</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="performance">
<h2>Performance<a class="headerlink" href="#performance" title="Permalink to this headline">¶</a></h2>
<p>TorchBench shows improvements in average training time across tasks with PJRT
compared to XRT, with an average improvement of over 35% on TPU v4-8. The
benefits vary significantly by task and model type, ranging from 0% to 175%.
The following chart shows the breakdown by task:</p>
<a class="reference external image-reference" href="assets/torchbench_pjrt_vs_xrt.svg"><img alt="PJRT vs XRT" src="assets/torchbench_pjrt_vs_xrt.svg" /></a>
<div class="section" id="new-tpu-runtime">
<h3>New TPU runtime<a class="headerlink" href="#new-tpu-runtime" title="Permalink to this headline">¶</a></h3>
<p><em>New in PyTorch/XLA r2.0</em></p>
<p>The PyTorch/XLA r2.0 release introduces support for the <a class="reference external" href="https://github.com/openxla/community/blob/main/rfcs/20230123-pjrt-plugin.md#rfc-openxla-pjrt-plugin">PJRT Plugin
API</a>,
used to access the new TFRT-based TPU runtime in <code class="docutils literal notranslate"><span class="pre">libtpu</span></code>. This is now the
default runtime when <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE=TPU</span></code> is set. The legacy StreamExecutor-based
TPU runtime used in 1.13 will still be available with <code class="docutils literal notranslate"><span class="pre">PJRT_DEVICE=TPU_LEGACY</span></code>
in the 2.0 release, but it will be removed in a future version. If you encounter
an issue that only happens on <code class="docutils literal notranslate"><span class="pre">TPU</span></code> and not <code class="docutils literal notranslate"><span class="pre">TPU_LEGACY</span></code>, please file an issue
on GitHub.</p>
<p>In most cases, we expect performance to be similar between the two runtimes, but
in some cases, the new runtime may be up to 30% faster. The following chart
shows the breakdown by task:</p>
<a class="reference external image-reference" href="assets/torchbench_tfrt_vs_se.svg"><img alt="TFRT vs StreamExecutor" src="assets/torchbench_tfrt_vs_se.svg" /></a>
<p>Note: the improvements shown in this chart are also included in the PJRT vs XRT
comparison.</p>
</div>
</div>
<div class="section" id="torchdynamo-torch-compile-integration-in-pytorch-xla">
<h2>TorchDynamo(torch.compile) integration in PyTorch XLA<a class="headerlink" href="#torchdynamo-torch-compile-integration-in-pytorch-xla" title="Permalink to this headline">¶</a></h2>
<p>Torchdynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster. It provides a clean API for compiler backends to hook in and its biggest feature is to dynamically modify Python bytecode right before it is executed. In the pytorch/xla 2.0 release, PyTorch/XLA provided an experimental backend for the TorchDynamo for both inference and training.</p>
<p>The way that XLA bridge works is that Dynamo will provide a TorchFX graph when it recognizes a model pattern and PyTorch/XLA will use existing Lazy Tensor technology to compile the FX graph and return the compiled function.</p>
<div class="section" id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h3>
<p>Here is a small code example of running resnet18 with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">imprt</span> <span class="n">torchvision</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="k">def</span> <span class="nf">eval_model</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
  <span class="n">xla_resnet18</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">xla_resnet18</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
  <span class="n">dynamo_resnet18</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
      <span class="n">xla_resnet18</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;torchxla_trace_once&#39;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">dynamo_resnet18</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p><strong>NOTE:</strong> inference backend name <code class="docutils literal notranslate"><span class="pre">torchxla_trace_once</span></code> is subject to change.</p>
</div></blockquote>
<p>With the <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> you will see that PyTorch/XLA only traces the resent18 model once during the init time and executes the compiled binary everytime <code class="docutils literal notranslate"><span class="pre">dynamo_resnet18</span></code> is invoked, instead of tracing the model every time. Note that currently Dynamo does not support fallback so if there is an op that can not be traced by XLA, it will error out. We will fix this issue in the upcoming 2.1 release. Here is a inference speed analysis to compare Dynamo and Lazy using torch bench on Cloud TPU v4-8</p>
<p>resnet18 | 1.768
resnet50 | 1.61
resnext50_32x4d | 1.328
alexnet | 1.261
mobilenet_v2 | 2.017
mnasnet1_0 | 1.686
vgg16 | 1.155
BERT_pytorch | 3.502
squeezenet1_1 | 1.674
timm_vision_transformer | 3.138
average | 1.9139</p>
</div>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<p>PyTorch/XLA also supports Dynamo for training, but it is very experimental and we are working with the PyTorch Compiler team to iterate on the implementation. On the 2.0 release it only supports forward and backward pass but not the optimizer. Here is an example of training a resnet18 with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">imprt</span> <span class="n">torchvision</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>

<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
  <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
  <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
  <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">pred</span>

<span class="k">def</span> <span class="nf">train_model_main</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
  <span class="n">xla_resnet18</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">xla_resnet18</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
  <span class="n">dynamo_train_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">train_model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;aot_torchxla_trace_once&#39;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">dynamo_train_model</span><span class="p">(</span><span class="n">xla_resnet18</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p><strong>NOTE:</strong> Backend we used here is <code class="docutils literal notranslate"><span class="pre">aot_torchxla_trace_once</span></code>(subject to change) instead of <code class="docutils literal notranslate"><span class="pre">torchxla_trace_once</span></code></p>
</div></blockquote>
<p>We expect to extract and execute 3 graphs per training step instead of one training step if you use the Lazy tensor. Here is a training speed analysis to compare Dynamo and Lazy using a torch bench on Cloud TPU v4-8.</p>
<p>resnet50 | 0.937
resnet18 | 1.003
BERT_pytorch | 1.869
resnext50_32x4d | 1.139
alexnet | 0.802
mobilenet_v2 | 0.672
mnasnet1_0 | 0.967
vgg16 | 0.742
timm_vision_transformer | 1.69
squeezenet1_1 | 0.958
average | 1.0779</p>
<blockquote>
<div><p><strong>NOTE:</strong> We run each model’s fwd and bwd for a single step and then collect the e2e time. In the real world we will run multiple steps at each training job which can easily hide the tracing cost from execution(since it is async). Lazy Tensor will have much better performance in that scenario.</p>
</div></blockquote>
<p>We are currently working on the optimizer support and that will be availiable on nightly soon but won’t be in the 2.0 release.</p>
</div>
<div class="section" id="take-away">
<h3>Take away<a class="headerlink" href="#take-away" title="Permalink to this headline">¶</a></h3>
<p>TorchDynamo provides a really promising way for the compiler backend to hide the complexity from the user and easily retrieve the modeling code in a graph format. Compared with PyTorch/XLA’s traditional Lazy Tensor way of extracting the graph, TorchDynamo can skip the graph tracing for every iteration hence provide a much better inference response time. However TorchDynamo does not trace the communication ops(like <code class="docutils literal notranslate"><span class="pre">all_reduce</span></code> and <code class="docutils literal notranslate"><span class="pre">all_gather</span></code>) yet and it provides separate graphs for the forward and the backward which hurts xla performance. These feature gaps compared to Lazy Tensor makes it less efficient in real world training use cases, especially the tracing cost can be overlapped with the execution in training. The PyTorch/XLA team will keep investing in TorchDynamo and work with upstream to mature the training story.</p>
</div>
</div>
<div class="section" id="fully-sharded-data-parallel-fsdp-in-pytorch-xla">
<h2>Fully Sharded Data Parallel (FSDP) in PyTorch XLA<a class="headerlink" href="#fully-sharded-data-parallel-fsdp-in-pytorch-xla" title="Permalink to this headline">¶</a></h2>
<p>Fully Sharded Data Parallel (FSDP) in PyTorch XLA is a utility for sharding Module parameters across data-parallel workers.</p>
<p>Example usage:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">from</span> <span class="nn">torch_xla.distributed.fsdp</span> <span class="kn">import</span> <span class="n">XlaFullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">FSDP</span><span class="p">(</span><span class="n">my_module</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>It is also possible to shard individual layers separately and have an outer wrapper handle any leftover parameters.</p>
<p>Notes:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">XlaFullyShardedDataParallel</span></code> class supports both the ZeRO-2 optimizer (sharding gradients and optimizer states) and the ZeRO-3 optimizer (sharding parameters, gradients, and optimizer states) in <a class="reference external" href="https://arxiv.org/abs/1910.02054">https://arxiv.org/abs/1910.02054</a>.</p>
<ul>
<li><p>The ZeRO-3 optimizer should be implemented via nested FSDP with <code class="docutils literal notranslate"><span class="pre">reshard_after_forward=True</span></code>. See <code class="docutils literal notranslate"><span class="pre">test/test_train_mp_mnist_fsdp_with_ckpt.py</span></code> and <code class="docutils literal notranslate"><span class="pre">test/test_train_mp_imagenet_fsdp.py</span></code> for an example.</p></li>
<li><p>For large models that cannot fit into a single TPU memory or the host CPU memory, one should interleave submodule construction with inner FSDP wrapping. See <cite>``FSDPViTModel`</cite> &lt;<a class="reference external" href="https://github.com/ronghanghu/vit_10b_fsdp_example/blob/master/run_vit_training.py">https://github.com/ronghanghu/vit_10b_fsdp_example/blob/master/run_vit_training.py</a>&gt;`_ for an example.</p></li>
</ul>
</li>
<li><p>a simple wrapper <code class="docutils literal notranslate"><span class="pre">checkpoint_module</span></code> is provided (based on <code class="docutils literal notranslate"><span class="pre">torch_xla.utils.checkpoint.checkpoint</span></code> from <a class="reference external" href="https://github.com/pytorch/xla/pull/3524">https://github.com/pytorch/xla/pull/3524</a>) to perform <a class="reference external" href="https://spell.ml/blog/gradient-checkpointing-pytorch-YGypLBAAACEAefHs">gradient checkpointing</a> over a given <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> instance. See <code class="docutils literal notranslate"><span class="pre">test/test_train_mp_mnist_fsdp_with_ckpt.py</span></code> and <code class="docutils literal notranslate"><span class="pre">test/test_train_mp_imagenet_fsdp.py</span></code> for an example.</p></li>
<li><p>Auto-wrapping submodules: instead of manually nested FSDP wrapping, one can also specify an <code class="docutils literal notranslate"><span class="pre">auto_wrap_policy</span></code> argument to automatically wrap the submodules with inner FSDP. <code class="docutils literal notranslate"><span class="pre">size_based_auto_wrap_policy</span></code> in <code class="docutils literal notranslate"><span class="pre">torch_xla.distributed.fsdp.wrap</span></code> is an example of <code class="docutils literal notranslate"><span class="pre">auto_wrap_policy</span></code> callable, this policy wraps layers with the number of parameters larger than 100M. <code class="docutils literal notranslate"><span class="pre">transformer_auto_wrap_policy</span></code> in <code class="docutils literal notranslate"><span class="pre">torch_xla.distributed.fsdp.wrap</span></code> is an example of <code class="docutils literal notranslate"><span class="pre">auto_wrap_policy</span></code> callable for transformer-like model architectures.</p></li>
</ul>
<p>For example, to automatically wrap all <code class="docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code> submodules with inner FSDP, one can use:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_xla.distributed.fsdp.wrap</span> <span class="kn">import</span> <span class="n">transformer_auto_wrap_policy</span>
<span class="n">auto_wrap_policy</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">transformer_auto_wrap_policy</span><span class="p">,</span> <span class="n">transformer_layer_cls</span><span class="o">=</span><span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">})</span>
</pre></div>
</div>
<p>Additionally, one can also specify an <code class="docutils literal notranslate"><span class="pre">auto_wrapper_callable</span></code> argument to use a custom callable wrapper for the submodules (the default wrapper is just the <code class="docutils literal notranslate"><span class="pre">XlaFullyShardedDataParallel</span></code> class itself). For example, one can use the following to apply gradient checkpointing (i.e. activation checkpointing/rematerialization) to each auto-wrapped submodule.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch_xla.distributed.fsdp</span> <span class="kn">import</span> <span class="n">checkpoint_module</span>
<span class="n">auto_wrapper_callable</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">m</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">XlaFullyShardedDataParallel</span><span class="p">(</span>
    <span class="n">checkpoint_module</span><span class="p">(</span><span class="n">m</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<ul>
<li><p>When stepping the optimizer, directly call <code class="docutils literal notranslate"><span class="pre">optimizer.step</span></code> and do not call <code class="docutils literal notranslate"><span class="pre">xm.optimizer_step</span></code>. The latter reduces the gradient across ranks, which is not needed for FSDP (where the parameters are already sharded).</p></li>
<li><p>When saving model and optimizer checkpoints during training, each training process needs to save its own checkpoint of the (sharded) model and optimizer state dicts (use <code class="docutils literal notranslate"><span class="pre">master_only=False</span></code> and set different paths for each rank in <code class="docutils literal notranslate"><span class="pre">xm.save</span></code>). When resuming, it needs to load the checkpoint for the corresponding rank.</p></li>
<li><p>Please also save <code class="docutils literal notranslate"><span class="pre">model.get_shard_metadata()</span></code> along with <code class="docutils literal notranslate"><span class="pre">model.state_dict()</span></code> as follows and use <code class="docutils literal notranslate"><span class="pre">consolidate_sharded_model_checkpoints</span></code> to stitch the sharded model checkpoints together into a full model state dict. See <code class="docutils literal notranslate"><span class="pre">test/test_train_mp_mnist_fsdp_with_ckpt.py</span></code> for an example.
.. code-block:: python3</p>
<blockquote>
<div><dl class="simple">
<dt>ckpt = {</dt><dd><p>‘model’: model.state_dict(),
‘shard_metadata’: model.get_shard_metadata(),
‘optimizer’: optimizer.state_dict(),</p>
</dd>
</dl>
<p>}
ckpt_path = f’/tmp/rank-{xm.get_ordinal()}-of-{xm.xrt_world_size()}.pth’
xm.save(ckpt, ckpt_path, master_only=False)</p>
</div></blockquote>
</li>
<li><p>The checkpoint consolidation script can also be launched from the command line as follows.
.. code-block:: bash</p>
<blockquote>
<div><p># consolidate the saved checkpoints via command line tool
python3 -m torch_xla.distributed.fsdp.consolidate_sharded_ckpts –ckpt_prefix /path/to/your_sharded_checkpoint_files –ckpt_suffix “_rank-<em>-of-</em>.pth”</p>
</div></blockquote>
</li>
</ul>
<p>The implementation of this class is largely inspired by and mostly follows the structure of <code class="docutils literal notranslate"><span class="pre">fairscale.nn.FullyShardedDataParallel</span></code> in <a class="reference external" href="https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html">https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html</a>. One of the biggest differences from <code class="docutils literal notranslate"><span class="pre">fairscale.nn.FullyShardedDataParallel</span></code> is that in XLA we don’t have explicit parameter storage, so here we resort to a different approach to free full parameters for ZeRO-3.</p>
<hr class="docutils" />
<div class="section" id="example-training-scripts-on-mnist-and-imagenet">
<h3>Example training scripts on MNIST and ImageNet<a class="headerlink" href="#example-training-scripts-on-mnist-and-imagenet" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>MNIST: <cite>``test/test_train_mp_mnist_fsdp_with_ckpt.py`</cite> &lt;<a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_train_mp_mnist_fsdp_with_ckpt.py">https://github.com/pytorch/xla/blob/master/test/test_train_mp_mnist_fsdp_with_ckpt.py</a>&gt;`_ (it also tests checkpoint consolidation)</p></li>
<li><p>ImageNet: <cite>``test/test_train_mp_imagenet_fsdp.py`</cite> &lt;<a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_train_mp_imagenet_fsdp.py">https://github.com/pytorch/xla/blob/master/test/test_train_mp_imagenet_fsdp.py</a>&gt;`_</p></li>
</ul>
<div class="section" id="installation">
<h4>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h4>
<p>FSDP is available on PyTorch/XLA 1.12 release and newer nightly. Please refer to <a class="reference external" href="https://github.com/pytorch/xla#-available-images-and-wheels">https://github.com/pytorch/xla#-available-images-and-wheels</a> for installation guide.</p>
</div>
<div class="section" id="clone-pytorch-xla-repo">
<h4>Clone PyTorch/XLA repo<a class="headerlink" href="#clone-pytorch-xla-repo" title="Permalink to this headline">¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone --recursive https://github.com/pytorch/pytorch
<span class="nb">cd</span> pytorch/
git clone --recursive https://github.com/pytorch/xla.git
<span class="nb">cd</span> ~/
</pre></div>
</div>
</div>
<div class="section" id="train-mnist-on-v3-8-tpu">
<h4>Train MNIST on v3-8 TPU<a class="headerlink" href="#train-mnist-on-v3-8-tpu" title="Permalink to this headline">¶</a></h4>
<p>It gets around 98.9 accuracy for 2 epochs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 ~/pytorch/xla/test/test_train_mp_mnist_fsdp_with_ckpt.py <span class="se">\</span>
  --batch_size <span class="m">16</span> --drop_last --num_epochs <span class="m">2</span> <span class="se">\</span>
  --use_nested_fsdp --use_gradient_checkpointing
</pre></div>
</div>
<p>This script automatically tests checkpoint consolidation at the end. You can also manually consolidate the sharded checkpoints via</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># consolidate the saved checkpoints via command line tool</span>
python3 -m torch_xla.distributed.fsdp.consolidate_sharded_ckpts <span class="se">\</span>
  --ckpt_prefix /tmp/mnist-fsdp/final_ckpt <span class="se">\</span>
  --ckpt_suffix <span class="s2">&quot;_rank-*-of-*.pth&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="train-imagenet-with-resnet-50-on-v3-8-tpu">
<h4>Train ImageNet with ResNet-50 on v3-8 TPU<a class="headerlink" href="#train-imagenet-with-resnet-50-on-v3-8-tpu" title="Permalink to this headline">¶</a></h4>
<p>It gets around 75.9 accuracy for 100 epochs; download <a class="reference external" href="https://github.com/pytorch/examples/tree/master/imagenet#requirements">ImageNet-1k</a> to <code class="docutils literal notranslate"><span class="pre">/datasets/imagenet-1k</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 ~/pytorch/xla/test/test_train_mp_imagenet_fsdp.py <span class="se">\</span>
  --datadir /datasets/imagenet-1k --drop_last <span class="se">\</span>
  --model resnet50 --test_set_batch_size <span class="m">64</span> --eval_interval <span class="m">10</span> <span class="se">\</span>
  --lr <span class="m">0</span>.4 --batch_size <span class="m">128</span> --num_warmup_epochs <span class="m">5</span> --lr_scheduler_divide_every_n_epochs <span class="m">30</span> --lr_scheduler_divisor <span class="m">10</span> --num_epochs <span class="m">100</span> <span class="se">\</span>
  --use_nested_fsdp
</pre></div>
</div>
<p>You can also add <code class="docutils literal notranslate"><span class="pre">--use_gradient_checkpointing</span></code> (which needs to be used along with <code class="docutils literal notranslate"><span class="pre">--use_nested_fsdp</span></code> or <code class="docutils literal notranslate"><span class="pre">--auto_wrap_policy</span></code>) to apply gradient checkpointing on the residual blocks.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="example-training-scripts-on-tpu-pod-with-10-billion-parameters">
<h3>Example training scripts on TPU pod (with 10 billion parameters)<a class="headerlink" href="#example-training-scripts-on-tpu-pod-with-10-billion-parameters" title="Permalink to this headline">¶</a></h3>
<p>To train large models that cannot fit into a single TPU, one should apply auto-wrap or manually wrap the submodules with inner FSDP when building the entire model to implement the ZeRO-3 algorithm.</p>
<p>Please see <a class="reference external" href="https://github.com/ronghanghu/vit_10b_fsdp_example">https://github.com/ronghanghu/vit_10b_fsdp_example</a> for an example of sharded training of a Vision Transformer (ViT) model using this XLA FSDP PR.</p>
</div>
</div>
</div>
<div class="section" id="how-to-do-distributeddataparallel">
<h1>How to do <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code><a class="headerlink" href="#how-to-do-distributeddataparallel" title="Permalink to this headline">¶</a></h1>
<p>This document shows how to use torch.nn.parallel.DistributedDataParallel in xla,
and further describes its difference against the native xla data parallel
approach.</p>
<div class="section" id="background-motivation">
<h2>Background / Motivation<a class="headerlink" href="#background-motivation" title="Permalink to this headline">¶</a></h2>
<p>Customers have long requested the ability to use PyTorch’s
DistributedDataParallel API with xla. And here we enable it as an experimental
feature.</p>
</div>
<div class="section" id="how-to-use-distributeddataparallel">
<h2>How to use DistributedDataParallel<a class="headerlink" href="#how-to-use-distributeddataparallel" title="Permalink to this headline">¶</a></h2>
<p>For those who switched from the PyTorch eager mode to XLA, here are all the
changes you need to do to convert your eager DDP model into XLA model. We assume
that you already know how to use XLA <a class="reference external" href="../API_GUIDE.md#running-on-a-single-xla-device">on a single
device</a>.</p>
<ol class="arabic simple">
<li><p>Import xla specific distributed packages:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_backend</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Init xla process group similar to other process groups such as nccl and gloo.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">&quot;xla&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Use xla specific APIs to get rank and world_size if you need to.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">new_rank</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">get_ordinal</span><span class="p">()</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xrt_world_size</span><span class="p">()</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Pass <code class="docutils literal notranslate"><span class="pre">gradient_as_bucket_view=True</span></code> to the DDP wrapper.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ddp_model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">gradient_as_bucket_view</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Finally launch your model with xla specific launcher.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xmp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">demo_fn</span><span class="p">)</span>
</pre></div>
</div>
<p>Here we have put everything together (the example is actually taken from the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">DDP tutorial</a>).
The way you code it is pretty similar to the eager experience. Just with xla
specific touches on a single device plus the above five changes to your script.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>

<span class="c1"># additional imports for xla</span>
<span class="kn">import</span> <span class="nn">torch_xla.core.xla_model</span> <span class="k">as</span> <span class="nn">xm</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_backend</span>
<span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_multiprocessing</span> <span class="k">as</span> <span class="nn">xmp</span>

<span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;localhost&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;12355&#39;</span>

    <span class="c1"># initialize the xla process group</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">&quot;xla&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cleanup</span><span class="p">():</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">ToyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ToyModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1000000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000000</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">demo_basic</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
    <span class="c1"># xla specific APIs to get rank, world_size.</span>
    <span class="n">new_rank</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">get_ordinal</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">new_rank</span> <span class="o">==</span> <span class="n">rank</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xrt_world_size</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running basic DDP example on rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">setup</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>

    <span class="c1"># create model and move it to XLA device</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ToyModel</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># currently, graident_as_bucket_view is needed to make DDP work for xla</span>
    <span class="n">ddp_model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">gradient_as_bucket_view</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">ddp_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">ddp_model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># xla specific API to execute the graph</span>
    <span class="n">xm</span><span class="o">.</span><span class="n">mark_step</span><span class="p">()</span>

    <span class="n">cleanup</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">run_demo</span><span class="p">(</span><span class="n">demo_fn</span><span class="p">):</span>
    <span class="c1"># xla specific launcher</span>
    <span class="n">xmp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">demo_fn</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">run_demo</span><span class="p">(</span><span class="n">demo_basic</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="benchmarking">
<h2>Benchmarking<a class="headerlink" href="#benchmarking" title="Permalink to this headline">¶</a></h2>
<div class="section" id="resnet50-with-fake-data">
<h3>Resnet50 with fake data<a class="headerlink" href="#resnet50-with-fake-data" title="Permalink to this headline">¶</a></h3>
<p>The following results are collected with the command: <code class="docutils literal notranslate"><span class="pre">python</span>
<span class="pre">test/test_train_mp_imagenet.py</span> <span class="pre">--fake_data</span> <span class="pre">--model=resnet50</span> <span class="pre">--num_epochs=1</span></code> on a
TPU VM V3-8 environment with ToT PyTorch and PyTorch/XLA. And the statistical
metrics are produced by using the script in this <a class="reference external" href="https://github.com/pytorch/xla/pull/4107">pull
request</a>. The unit for the rate is
images per second.</p>
<table>
  <tr>
   <td>Type
   </td>
   <td>Mean
   </td>
   <td>Median
   </td>
   <td>90th %
   </td>
   <td>Std Dev
   </td>
   <td>CV
   </td>
  </tr>
  <tr>
   <td>xm.optimizer_step
   </td>
   <td>418.54
   </td>
   <td>419.22
   </td>
   <td>430.40
   </td>
   <td>9.76
   </td>
   <td>0.02
   </td>
  </tr>
  <tr>
   <td>DDP
   </td>
   <td>395.97
   </td>
   <td>395.54
   </td>
   <td>407.13
   </td>
   <td>7.60
   </td>
   <td>0.02
   </td>
  </tr>
</table><p>The performance difference between our native approach for distributed data
parallel and DistributedDataParallel wrapper is: 1 - 395.97 / 418.54 = 5.39%.
This result seems reasonable given the DDP wrapper introduces extra overheads on
tracing the DDP runtime.</p>
</div>
<div class="section" id="mnist-with-fake-data">
<h3>MNIST with fake data<a class="headerlink" href="#mnist-with-fake-data" title="Permalink to this headline">¶</a></h3>
<p>The following results are collected with the command: <code class="docutils literal notranslate"><span class="pre">python</span>
<span class="pre">test/test_train_mp_mnist.py</span> <span class="pre">--fake_data</span></code> on a TPU VM V3-8 environment with ToT
PyTorch and PyTorch/XLA. And the statistical metrics are produced by using the
script in this <a class="reference external" href="https://github.com/pytorch/xla/pull/4107">pull request</a>. The
unit for the rate is images per second.</p>
<table>
  <tr>
   <td>Type
   </td>
   <td>Mean
   </td>
   <td>Median
   </td>
   <td>90th %
   </td>
   <td>Std Dev
   </td>
   <td>CV
   </td>
  </tr>
  <tr>
   <td>xm.optimizer_step
   </td>
   <td>17864.19
   </td>
   <td>20108.96
   </td>
   <td>24351.74
   </td>
   <td>5866.83
   </td>
   <td>0.33
   </td>
  </tr>
  <tr>
   <td>DDP
   </td>
   <td>10701.39
   </td>
   <td>11770.00
   </td>
   <td>14313.78
   </td>
   <td>3102.92
   </td>
   <td>0.29
   </td>
  </tr>
</table><p>The performance difference between our native approach for distributed data
parallel and DistributedDataParallel wrapper is: 1 - 14313.78 / 24351.74 =
41.22%. Here we compare 90th % instead since the dataset is small and first a
few rounds are heavily impacted by data loading. This slowdown is huge but makes
sense given the model is small. The additional DDP runtime tracing overhead is
hard to amortize.</p>
</div>
<div class="section" id="mnist-with-real-data">
<h3>MNIST with real data<a class="headerlink" href="#mnist-with-real-data" title="Permalink to this headline">¶</a></h3>
<p>The following results are collected with the command: <code class="docutils literal notranslate"><span class="pre">python</span>
<span class="pre">test/test_train_mp_mnist.py</span> <span class="pre">--logdir</span> <span class="pre">mnist/</span></code> on a TPU VM V3-8 environment with
ToT PyTorch and PyTorch/XLA.</p>
<a class="reference external image-reference" href="assets/ddp_md_mnist_with_real_data.png"><img alt="learning_curves" src="assets/ddp_md_mnist_with_real_data.png" /></a>
<p>And we can observe that the DDP wrapper converges slower than the native XLA
approach even though it still achieves a high accuracy rate at 97.48% at the
end. (The native approach achieves 99%.)</p>
</div>
</div>
<div class="section" id="disclaimer">
<h2>Disclaimer<a class="headerlink" href="#disclaimer" title="Permalink to this headline">¶</a></h2>
<p>This feature is still experimental and under active development. Use it in
cautions and feel free to file any bugs to the <a class="reference external" href="https://github.com/pytorch/xla/">xla github
repo</a>. For those who are interested in the
native xla data parallel approach, here is the
<a class="reference external" href="../API_GUIDE.md#running-on-multiple-xla-devices-with-multi-processing">tutorial</a>.</p>
<p>Here are some of the known issues that are under investigation:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">gradient_as_bucket_view=True</span></code> needs to be enforced.</p></li>
<li><p>There are some issues while being used with <code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>. <code class="docutils literal notranslate"><span class="pre">​​test_train_mp_mnist.py</span></code> with real data crashes before exiting.</p></li>
</ul>
</div>
</div>
<div class="section" id="how-to-run-with-pytorch-xla-gpu">
<h1>How to run with PyTorch/XLA:GPU<a class="headerlink" href="#how-to-run-with-pytorch-xla-gpu" title="Permalink to this headline">¶</a></h1>
<p>PyTorch/XLA enables PyTorch users to utilize the XLA compiler which supports accelerators including TPU, GPU, CPU and … This doc will go over the basic steps to run PyTorch/XLA on a nvidia gpu instance</p>
<div class="section" id="create-a-gpu-instance">
<h2>Create a GPU instance<a class="headerlink" href="#create-a-gpu-instance" title="Permalink to this headline">¶</a></h2>
<p>Pytorch/XLA currently publish prebuilt docker images and wheels with cuda11.2 and python 3.7/3.8. We recommend users to create a GPU instance with corresponding config. For a full list of docker images and wheels, please refer to <a class="reference external" href="https://github.com/pytorch/xla/tree/jackcao/gpu_doc#-available-images-and-wheels">this doc</a>.</p>
</div>
<div class="section" id="environment-setup">
<h2>Environment Setup<a class="headerlink" href="#environment-setup" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id8">
<h3>Docker<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>sudo docker pull gcr.io/tpu-pytorch/xla:nightly_3.8_cuda_11.2
sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent    software-properties-common
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
sudo docker run --gpus all -it -d gcr.io/tpu-pytorch/xla:nightly_3.7\8_cuda_11.2 bin/bash
sudo docker exec -it $(sudo docker ps | awk &#39;NR==2 { print $1 }&#39;) /bin/bash
</pre></div>
</div>
<p>Note that you need to restart the docker to make gpu devices visible in the docker container. After logging into the docker, you can use <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> to verify the device is setup correctly.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">pytorch</span><span class="p">)</span> <span class="n">root</span><span class="o">@</span><span class="mi">20</span><span class="n">ab2c7a2d06</span><span class="p">:</span><span class="o">/</span><span class="c1"># nvidia-smi</span>
<span class="n">Thu</span> <span class="n">Dec</span>  <span class="mi">8</span> <span class="mi">06</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">29</span> <span class="mi">2022</span>
<span class="o">+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">NVIDIA</span><span class="o">-</span><span class="n">SMI</span> <span class="mf">510.47</span><span class="o">.</span><span class="mi">03</span>    <span class="n">Driver</span> <span class="n">Version</span><span class="p">:</span> <span class="mf">510.47</span><span class="o">.</span><span class="mi">03</span>    <span class="n">CUDA</span> <span class="n">Version</span><span class="p">:</span> <span class="mf">11.6</span>     <span class="o">|</span>
<span class="o">|-------------------------------+----------------------+----------------------+</span>
<span class="o">|</span> <span class="n">GPU</span>  <span class="n">Name</span>        <span class="n">Persistence</span><span class="o">-</span><span class="n">M</span><span class="o">|</span> <span class="n">Bus</span><span class="o">-</span><span class="n">Id</span>        <span class="n">Disp</span><span class="o">.</span><span class="n">A</span> <span class="o">|</span> <span class="n">Volatile</span> <span class="n">Uncorr</span><span class="o">.</span> <span class="n">ECC</span> <span class="o">|</span>
<span class="o">|</span> <span class="n">Fan</span>  <span class="n">Temp</span>  <span class="n">Perf</span>  <span class="n">Pwr</span><span class="p">:</span><span class="n">Usage</span><span class="o">/</span><span class="n">Cap</span><span class="o">|</span>         <span class="n">Memory</span><span class="o">-</span><span class="n">Usage</span> <span class="o">|</span> <span class="n">GPU</span><span class="o">-</span><span class="n">Util</span>  <span class="n">Compute</span> <span class="n">M</span><span class="o">.</span> <span class="o">|</span>
<span class="o">|</span>                               <span class="o">|</span>                      <span class="o">|</span>               <span class="n">MIG</span> <span class="n">M</span><span class="o">.</span> <span class="o">|</span>
<span class="o">|===============================+======================+======================|</span>
<span class="o">|</span>   <span class="mi">0</span>  <span class="n">Tesla</span> <span class="n">V100</span><span class="o">-</span><span class="n">SXM2</span><span class="o">...</span>  <span class="n">Off</span>  <span class="o">|</span> <span class="mi">00000000</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mf">04.0</span> <span class="n">Off</span> <span class="o">|</span>                    <span class="mi">0</span> <span class="o">|</span>
<span class="o">|</span> <span class="n">N</span><span class="o">/</span><span class="n">A</span>   <span class="mi">36</span><span class="n">C</span>    <span class="n">P0</span>    <span class="mi">38</span><span class="n">W</span> <span class="o">/</span> <span class="mi">300</span><span class="n">W</span> <span class="o">|</span>      <span class="mi">0</span><span class="n">MiB</span> <span class="o">/</span> <span class="mi">16384</span><span class="n">MiB</span> <span class="o">|</span>      <span class="mi">1</span><span class="o">%</span>      <span class="n">Default</span> <span class="o">|</span>
<span class="o">|</span>                               <span class="o">|</span>                      <span class="o">|</span>                  <span class="n">N</span><span class="o">/</span><span class="n">A</span> <span class="o">|</span>
<span class="o">+-------------------------------+----------------------+----------------------+</span>

<span class="o">+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">Processes</span><span class="p">:</span>                                                                  <span class="o">|</span>
<span class="o">|</span>  <span class="n">GPU</span>   <span class="n">GI</span>   <span class="n">CI</span>        <span class="n">PID</span>   <span class="n">Type</span>   <span class="n">Process</span> <span class="n">name</span>                  <span class="n">GPU</span> <span class="n">Memory</span> <span class="o">|</span>
<span class="o">|</span>        <span class="n">ID</span>   <span class="n">ID</span>                                                   <span class="n">Usage</span>      <span class="o">|</span>
<span class="o">|=============================================================================|</span>
<span class="o">|</span>  <span class="n">No</span> <span class="n">running</span> <span class="n">processes</span> <span class="n">found</span>                                                 <span class="o">|</span>
<span class="o">+-----------------------------------------------------------------------------+</span>
</pre></div>
</div>
</div>
<div class="section" id="wheel">
<h3>Wheel<a class="headerlink" href="#wheel" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip3</span> <span class="n">install</span> <span class="n">torch</span><span class="o">=</span><span class="mf">1.13</span>
<span class="n">pip3</span> <span class="n">install</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">storage</span><span class="o">.</span><span class="n">googleapis</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">tpu</span><span class="o">-</span><span class="n">pytorch</span><span class="o">/</span><span class="n">wheels</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="mi">112</span><span class="o">/</span><span class="n">torch_xla</span><span class="o">-</span><span class="mf">1.13</span><span class="o">-</span><span class="n">cp37</span><span class="o">-</span><span class="n">cp37m</span><span class="o">-</span><span class="n">linux_x86_64</span><span class="o">.</span><span class="n">whl</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="run-a-simple-model">
<h2>Run a simple model<a class="headerlink" href="#run-a-simple-model" title="Permalink to this headline">¶</a></h2>
<p>In order to run below examples, you need to clone the pytorch/xla repo to access the imagenet example(We already clone it in our docker).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">pytorch</span><span class="p">)</span> <span class="n">root</span><span class="o">@</span><span class="mi">20</span><span class="n">ab2c7a2d06</span><span class="p">:</span><span class="o">/</span><span class="c1"># export GPU_NUM_DEVICES=1</span>
<span class="p">(</span><span class="n">pytorch</span><span class="p">)</span> <span class="n">root</span><span class="o">@</span><span class="mi">20</span><span class="n">ab2c7a2d06</span><span class="p">:</span><span class="o">/</span><span class="c1"># python pytorch/xla/test/test_train_mp_imagenet.py --fake_data</span>
<span class="o">==&gt;</span> <span class="n">Preparing</span> <span class="n">data</span><span class="o">..</span>
<span class="n">Epoch</span> <span class="mi">1</span> <span class="n">train</span> <span class="n">begin</span> <span class="mi">06</span><span class="p">:</span><span class="mi">12</span><span class="p">:</span><span class="mi">38</span>
<span class="mi">2022</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">08</span> <span class="mi">06</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mf">12.452874</span><span class="p">:</span> <span class="n">W</span>      <span class="mi">79</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">gpu_conv_algorithm_picker</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">729</span><span class="p">]</span> <span class="kc">None</span> <span class="n">of</span> <span class="n">the</span> <span class="n">algorithms</span> <span class="n">provided</span> <span class="n">by</span> <span class="n">cuDNN</span> <span class="n">heuristics</span> <span class="n">worked</span><span class="p">;</span> <span class="n">trying</span> <span class="n">fallback</span> <span class="n">algorithms</span><span class="o">.</span>  <span class="n">Conv</span><span class="p">:</span> <span class="p">(</span><span class="n">f32</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">]{</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">u8</span><span class="p">[</span><span class="mi">0</span><span class="p">]{</span><span class="mi">0</span><span class="p">})</span> <span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="p">(</span><span class="n">f32</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">14</span><span class="p">]{</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f32</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">}),</span> <span class="n">window</span><span class="o">=</span><span class="p">{</span><span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="n">x3</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="n">x2</span> <span class="n">pad</span><span class="o">=</span><span class="mi">1_1</span><span class="n">x1_1</span><span class="p">},</span> <span class="n">dim_labels</span><span class="o">=</span><span class="n">bf01_01io</span><span class="o">-&gt;</span><span class="n">bf01</span><span class="p">,</span> <span class="n">custom_call_target</span><span class="o">=</span><span class="s2">&quot;__cudnn$convBackwardInput&quot;</span><span class="p">,</span> <span class="n">backend_config</span><span class="o">=</span><span class="s2">&quot;{</span><span class="se">\&quot;</span><span class="s2">conv_result_scale</span><span class="se">\&quot;</span><span class="s2">:1,</span><span class="se">\&quot;</span><span class="s2">activation_mode</span><span class="se">\&quot;</span><span class="s2">:</span><span class="se">\&quot;</span><span class="s2">0</span><span class="se">\&quot;</span><span class="s2">,</span><span class="se">\&quot;</span><span class="s2">side_input_scale</span><span class="se">\&quot;</span><span class="s2">:0}&quot;</span>
<span class="mi">2022</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">08</span> <span class="mi">06</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mf">13.780992</span><span class="p">:</span> <span class="n">W</span>      <span class="mi">79</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">compiler</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">gpu_conv_algorithm_picker</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">729</span><span class="p">]</span> <span class="kc">None</span> <span class="n">of</span> <span class="n">the</span> <span class="n">algorithms</span> <span class="n">provided</span> <span class="n">by</span> <span class="n">cuDNN</span> <span class="n">heuristics</span> <span class="n">worked</span><span class="p">;</span> <span class="n">trying</span> <span class="n">fallback</span> <span class="n">algorithms</span><span class="o">.</span>  <span class="n">Conv</span><span class="p">:</span> <span class="p">(</span><span class="n">f32</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">56</span><span class="p">,</span><span class="mi">56</span><span class="p">]{</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">u8</span><span class="p">[</span><span class="mi">0</span><span class="p">]{</span><span class="mi">0</span><span class="p">})</span> <span class="n">custom</span><span class="o">-</span><span class="n">call</span><span class="p">(</span><span class="n">f32</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">]{</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">},</span> <span class="n">f32</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">]{</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">}),</span> <span class="n">window</span><span class="o">=</span><span class="p">{</span><span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="n">x3</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="n">x2</span> <span class="n">pad</span><span class="o">=</span><span class="mi">1_1</span><span class="n">x1_1</span><span class="p">},</span> <span class="n">dim_labels</span><span class="o">=</span><span class="n">bf01_01io</span><span class="o">-&gt;</span><span class="n">bf01</span><span class="p">,</span> <span class="n">custom_call_target</span><span class="o">=</span><span class="s2">&quot;__cudnn$convBackwardInput&quot;</span><span class="p">,</span> <span class="n">backend_config</span><span class="o">=</span><span class="s2">&quot;{</span><span class="se">\&quot;</span><span class="s2">conv_result_scale</span><span class="se">\&quot;</span><span class="s2">:1,</span><span class="se">\&quot;</span><span class="s2">activation_mode</span><span class="se">\&quot;</span><span class="s2">:</span><span class="se">\&quot;</span><span class="s2">0</span><span class="se">\&quot;</span><span class="s2">,</span><span class="se">\&quot;</span><span class="s2">side_input_scale</span><span class="se">\&quot;</span><span class="s2">:0}&quot;</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">0</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">6.89059</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">2.82</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">2.82</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mi">23</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">20</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">6.79297</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">117.16</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">45.84</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mi">36</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">40</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">6.43628</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">281.16</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">80.49</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mi">43</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">60</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">5.83108</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">346.88</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">108.82</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mi">49</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">80</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">4.99023</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">373.62</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">132.43</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mi">56</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">100</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">3.92699</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">384.33</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">152.40</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">14</span><span class="p">:</span><span class="mi">02</span>
<span class="o">|</span> <span class="n">Training</span> <span class="n">Device</span><span class="o">=</span><span class="n">xla</span><span class="p">:</span><span class="mi">0</span><span class="o">/</span><span class="mi">0</span> <span class="n">Epoch</span><span class="o">=</span><span class="mi">1</span> <span class="n">Step</span><span class="o">=</span><span class="mi">120</span> <span class="n">Loss</span><span class="o">=</span><span class="mf">2.68816</span> <span class="n">Rate</span><span class="o">=</span><span class="mf">388.35</span> <span class="n">GlobalRate</span><span class="o">=</span><span class="mf">169.49</span> <span class="n">Time</span><span class="o">=</span><span class="mi">06</span><span class="p">:</span><span class="mi">14</span><span class="p">:</span><span class="mi">09</span>
</pre></div>
</div>
</div>
<div class="section" id="amp-automatic-mixed-precision">
<h2>AMP (AUTOMATIC MIXED PRECISION)<a class="headerlink" href="#amp-automatic-mixed-precision" title="Permalink to this headline">¶</a></h2>
<p>AMP is very useful on GPU training and PyTorch/XLA reuse Cuda’s AMP rule. You can checkout our <a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_train_mp_mnist_amp.py">mnist example</a> and <a class="reference external" href="https://github.com/pytorch/xla/blob/master/test/test_train_mp_imagenet_amp.py">imagenet example</a>. Note that we also used a modified version of <a class="reference external" href="https://github.com/pytorch/xla/tree/master/torch_xla/amp/syncfree">optimizers</a> to avoid the additional sync between device and host.</p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright .

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">PyTorch on XLA Devices</a><ul>
<li><a class="reference internal" href="#creating-an-xla-tensor">Creating an XLA Tensor</a></li>
<li><a class="reference internal" href="#xla-tensors-are-pytorch-tensors">XLA Tensors are PyTorch Tensors</a></li>
<li><a class="reference internal" href="#running-models-on-xla-devices">Running Models on XLA Devices</a><ul>
<li><a class="reference internal" href="#running-on-a-single-xla-device">Running on a Single XLA Device</a></li>
<li><a class="reference internal" href="#running-on-multiple-xla-devices-with-multi-processing">Running on Multiple XLA Devices with Multi-processing</a></li>
<li><a class="reference internal" href="#running-on-tpu-pods">Running on TPU Pods</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id3">XLA Tensor Deep Dive</a><ul>
<li><a class="reference internal" href="#xla-tensors-are-lazy">XLA Tensors are Lazy</a></li>
<li><a class="reference internal" href="#xla-tensors-and-bfloat16">XLA Tensors and bFloat16</a></li>
<li><a class="reference internal" href="#memory-layout">Memory Layout</a></li>
<li><a class="reference internal" href="#moving-xla-tensors-to-and-from-the-cpu">Moving XLA Tensors to and from the CPU</a></li>
<li><a class="reference internal" href="#saving-and-loading-xla-tensors">Saving and Loading XLA Tensors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#further-reading">Further Reading</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch-xla-api">PyTorch/XLA API</a><ul>
<li><a class="reference internal" href="#module-torch_xla.core.xla_model">xla_model</a></li>
<li><a class="reference internal" href="#module-torch_xla.distributed.parallel_loader">distributed</a></li>
<li><a class="reference internal" href="#module-torch_xla.utils.tf_record_reader">utils</a></li>
<li><a class="reference internal" href="#test">test</a></li>
</ul>
</li>
<li><a class="reference internal" href="#troubleshooting">Troubleshooting</a><ul>
<li><a class="reference internal" href="#perform-a-auto-metrics-analysis">Perform A Auto-Metrics Analysis</a></li>
<li><a class="reference internal" href="#get-a-metrics-report">Get A Metrics Report</a></li>
<li><a class="reference internal" href="#understand-the-metrics-report">Understand The Metrics Report</a></li>
<li><a class="reference internal" href="#clar-the-metrics-report">Clar The Metrics Report</a></li>
<li><a class="reference internal" href="#performance-profiling">Performance Profiling</a></li>
<li><a class="reference internal" href="#known-performance-caveats">Known Performance Caveats</a></li>
<li><a class="reference internal" href="#xla-tensor-quirks">XLA Tensor Quirks</a></li>
<li><a class="reference internal" href="#more-debugging-tools">More Debugging Tools</a><ul>
<li><a class="reference internal" href="#environment-variables">Environment Variables</a></li>
<li><a class="reference internal" href="#retrieving-stack-traces">Retrieving Stack Traces</a></li>
</ul>
</li>
<li><a class="reference internal" href="#using-debug-run-py-to-collect-debug-information">Using debug_run.py To Collect Debug Information</a></li>
<li><a class="reference internal" href="#common-issues">Common Issues</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pjrt-runtime-beta">PJRT Runtime (Beta)</a><ul>
<li><a class="reference internal" href="#tl-dr">TL;DR</a></li>
<li><a class="reference internal" href="#benefits">Benefits</a></li>
<li><a class="reference internal" href="#quickstart">Quickstart</a><ul>
<li><a class="reference internal" href="#cpu">CPU</a></li>
<li><a class="reference internal" href="#tpu">TPU</a><ul>
<li><a class="reference internal" href="#pods">Pods</a></li>
<li><a class="reference internal" href="#docker">Docker</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gpu">GPU</a></li>
</ul>
</li>
<li><a class="reference internal" href="#differences-from-xrt">Differences from XRT</a><ul>
<li><a class="reference internal" href="#id6">Multithreading on TPU v2/v3</a></li>
<li><a class="reference internal" href="#changes-to-xm-rendezvous">Changes to xm.rendezvous</a></li>
<li><a class="reference internal" href="#pjrt-and-torch-distributed">PJRT and torch.distributed</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance">Performance</a><ul>
<li><a class="reference internal" href="#new-tpu-runtime">New TPU runtime</a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchdynamo-torch-compile-integration-in-pytorch-xla">TorchDynamo(torch.compile) integration in PyTorch XLA</a><ul>
<li><a class="reference internal" href="#inference">Inference</a></li>
<li><a class="reference internal" href="#training">Training</a></li>
<li><a class="reference internal" href="#take-away">Take away</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fully-sharded-data-parallel-fsdp-in-pytorch-xla">Fully Sharded Data Parallel (FSDP) in PyTorch XLA</a><ul>
<li><a class="reference internal" href="#example-training-scripts-on-mnist-and-imagenet">Example training scripts on MNIST and ImageNet</a><ul>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#clone-pytorch-xla-repo">Clone PyTorch/XLA repo</a></li>
<li><a class="reference internal" href="#train-mnist-on-v3-8-tpu">Train MNIST on v3-8 TPU</a></li>
<li><a class="reference internal" href="#train-imagenet-with-resnet-50-on-v3-8-tpu">Train ImageNet with ResNet-50 on v3-8 TPU</a></li>
</ul>
</li>
<li><a class="reference internal" href="#example-training-scripts-on-tpu-pod-with-10-billion-parameters">Example training scripts on TPU pod (with 10 billion parameters)</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-do-distributeddataparallel">How to do <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code></a><ul>
<li><a class="reference internal" href="#background-motivation">Background / Motivation</a></li>
<li><a class="reference internal" href="#how-to-use-distributeddataparallel">How to use DistributedDataParallel</a></li>
<li><a class="reference internal" href="#benchmarking">Benchmarking</a><ul>
<li><a class="reference internal" href="#resnet50-with-fake-data">Resnet50 with fake data</a></li>
<li><a class="reference internal" href="#mnist-with-fake-data">MNIST with fake data</a></li>
<li><a class="reference internal" href="#mnist-with-real-data">MNIST with real data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#disclaimer">Disclaimer</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-run-with-pytorch-xla-gpu">How to run with PyTorch/XLA:GPU</a><ul>
<li><a class="reference internal" href="#create-a-gpu-instance">Create a GPU instance</a></li>
<li><a class="reference internal" href="#environment-setup">Environment Setup</a><ul>
<li><a class="reference internal" href="#id8">Docker</a></li>
<li><a class="reference internal" href="#wheel">Wheel</a></li>
</ul>
</li>
<li><a class="reference internal" href="#run-a-simple-model">Run a simple model</a></li>
<li><a class="reference internal" href="#amp-automatic-mixed-precision">AMP (AUTOMATIC MIXED PRECISION)</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>