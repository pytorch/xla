{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_SHOW_DISPATCH_TRACE=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [call] op=[aten::ones], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::ones], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "   [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::ones], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::ones], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "   [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::ones], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::ones], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "   [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::ones], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::ones], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "   [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::abs], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::abs], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "   [call] op=[aten::abs.out], key=[CPU]\n",
      " [call] op=[aten::positive], key=[AutogradCPU]\n",
      " [call] op=[aten::neg], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::neg], key=[CPU]\n",
      " [call] op=[aten::is_nonzero], key=[AutogradCPU]\n",
      "  [call] op=[aten::item], key=[AutogradCPU]\n",
      "   [call] op=[aten::_local_scalar_dense], key=[AutogradCPU]\n",
      "    [redispatch] op=[aten::_local_scalar_dense], key=[CPU]\n",
      " [call] op=[aten::bitwise_not], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_not], key=[CPU]\n",
      " [call] op=[aten::pow.Tensor_Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::pow.Tensor_Tensor], key=[CPU]\n",
      " [call] op=[aten::mul.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::mul.Tensor], key=[CPU]\n",
      " [call] op=[aten::matmul], key=[AutogradCPU]\n",
      "  [call] op=[aten::dot], key=[AutogradCPU]\n",
      "   [redispatch] op=[aten::dot], key=[CPU]\n",
      "    [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "    [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::floor_divide], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::floor_divide], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      " [call] op=[aten::div.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::div.Tensor], key=[CPU]\n",
      " [call] op=[aten::remainder.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::remainder.Tensor], key=[CPU]\n",
      " [call] op=[aten::add.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::add.Tensor], key=[CPU]\n",
      " [call] op=[aten::lt.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::lt.Tensor], key=[CPU]\n",
      " [call] op=[aten::gt.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::gt.Tensor], key=[CPU]\n",
      " [call] op=[aten::ge.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::ge.Tensor], key=[CPU]\n",
      " [call] op=[aten::le.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::le.Tensor], key=[CPU]\n",
      " [call] op=[aten::ne.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::ne.Tensor], key=[CPU]\n",
      " [call] op=[aten::eq.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::eq.Tensor], key=[CPU]\n",
      " [call] op=[aten::sub.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::sub.Tensor], key=[CPU]\n",
      " [call] op=[aten::pow_.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::pow_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::pow_.Tensor], key=[CPU]\n",
      " [call] op=[aten::mul_.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::mul_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::mul_.Tensor], key=[CPU]\n",
      " [call] op=[aten::matmul], key=[AutogradCPU]\n",
      "  [call] op=[aten::dot], key=[AutogradCPU]\n",
      "   [redispatch] op=[aten::dot], key=[CPU]\n",
      "    [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      "    [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::floor_divide_.Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::floor_divide_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::floor_divide_.Tensor], key=[CPU]\n",
      " [call] op=[aten::div_.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::div_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::div_.Tensor], key=[CPU]\n",
      " [call] op=[aten::remainder_.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::remainder_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::remainder_.Tensor], key=[CPU]\n",
      " [call] op=[aten::add_.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::add_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::add_.Tensor], key=[CPU]\n",
      " [call] op=[aten::sub_.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::sub_.Tensor], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::sub_.Tensor], key=[CPU]\n",
      " [call] op=[aten::__and__.Tensor], key=[AutogradCPU]\n",
      "  [call] op=[aten::bitwise_and.Tensor], key=[AutogradCPU]\n",
      "   [redispatchBoxed] op=[aten::bitwise_and.Tensor], key=[CPU]\n",
      " [call] op=[aten::__or__.Tensor], key=[AutogradCPU]\n",
      "  [call] op=[aten::bitwise_or.Tensor], key=[AutogradCPU]\n",
      "   [redispatchBoxed] op=[aten::bitwise_or.Tensor], key=[CPU]\n",
      " [call] op=[aten::__xor__.Tensor], key=[AutogradCPU]\n",
      "  [call] op=[aten::bitwise_xor.Tensor], key=[AutogradCPU]\n",
      "   [redispatchBoxed] op=[aten::bitwise_xor.Tensor], key=[CPU]\n",
      " [call] op=[aten::__iand__.Tensor], key=[AutogradCPU]\n",
      "  [call] op=[aten::bitwise_and_.Tensor], key=[AutogradCPU]\n",
      "   [redispatchBoxed] op=[aten::bitwise_and_.Tensor], key=[ADInplaceOrView]\n",
      "    [redispatch] op=[aten::bitwise_and_.Tensor], key=[CPU]\n",
      " [call] op=[aten::__ixor__.Tensor], key=[AutogradCPU]\n",
      "  [call] op=[aten::bitwise_xor_.Tensor], key=[AutogradCPU]\n",
      "   [redispatchBoxed] op=[aten::bitwise_xor_.Tensor], key=[ADInplaceOrView]\n",
      "    [redispatch] op=[aten::bitwise_xor_.Tensor], key=[CPU]\n",
      " [call] op=[aten::__ior__.Tensor], key=[AutogradCPU]\n",
      "  [call] op=[aten::bitwise_or_.Tensor], key=[AutogradCPU]\n",
      "   [redispatchBoxed] op=[aten::bitwise_or_.Tensor], key=[ADInplaceOrView]\n",
      "    [redispatch] op=[aten::bitwise_or_.Tensor], key=[CPU]\n",
      " [call] op=[aten::__lshift__.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::__lshift__.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      " [call] op=[aten::__rshift__.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::__rshift__.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      " [call] op=[aten::__ilshift__.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::__ilshift__.Scalar], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::__ilshift__.Scalar], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::__irshift__.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::__irshift__.Scalar], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::__irshift__.Scalar], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::select.int], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::select.int], key=[ADInplaceOrView]\n",
      "   [redispatch] op=[aten::select.int], key=[CPU]\n",
      "    [call] op=[aten::as_strided], key=[CPU]\n",
      " [call] op=[aten::pow.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::pow.Scalar], key=[CPU]\n",
      "   [call] op=[aten::result_type.Scalar_Tensor], key=[CPU]\n",
      "    [call] op=[aten::result_type.Scalar], key=[CPU]\n",
      "   [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::mul.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::mul.Tensor], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::floor_divide], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::floor_divide], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      " [call] op=[aten::reciprocal], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::reciprocal], key=[CPU]\n",
      " [call] op=[aten::mul.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::mul.Tensor], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::remainder.Scalar_Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::remainder.Scalar_Tensor], key=[CPU]\n",
      "   [call] op=[aten::remainder.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::add.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::add.Tensor], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::gt.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::gt.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::lt.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::lt.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::le.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::le.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::ge.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::ge.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::ne.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::ne.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::eq.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::eq.Scalar], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::rsub.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::rsub.Scalar], key=[CPU]\n",
      "   [call] op=[aten::sub.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::pow.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::pow.Scalar], key=[CPU]\n",
      "   [call] op=[aten::result_type.Scalar_Tensor], key=[CPU]\n",
      "    [call] op=[aten::result_type.Scalar], key=[CPU]\n",
      "   [call] op=[aten::fill_.Scalar], key=[CPU]\n",
      " [call] op=[aten::mul.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::mul.Tensor], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::floor_divide], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::floor_divide], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[CPU]\n",
      " [call] op=[aten::reciprocal], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::reciprocal], key=[CPU]\n",
      " [call] op=[aten::mul.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::mul.Tensor], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::remainder.Scalar_Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::remainder.Scalar_Tensor], key=[CPU]\n",
      "   [call] op=[aten::remainder.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::add.Tensor], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::add.Tensor], key=[CPU]\n",
      "   [call] op=[aten::to.dtype], key=[CPU]\n",
      "    [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "     [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "      [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "       [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "      [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::rsub.Scalar], key=[AutogradCPU]\n",
      "  [redispatch] op=[aten::rsub.Scalar], key=[CPU]\n",
      "   [call] op=[aten::sub.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_and.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_and.Scalar], key=[CPU]\n",
      "   [call] op=[aten::bitwise_and.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_or.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_or.Scalar], key=[CPU]\n",
      "   [call] op=[aten::bitwise_or.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_xor.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_xor.Scalar], key=[CPU]\n",
      "   [call] op=[aten::bitwise_xor.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_and.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_and.Scalar], key=[CPU]\n",
      "   [call] op=[aten::bitwise_and.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_xor.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_xor.Scalar], key=[CPU]\n",
      "   [call] op=[aten::bitwise_xor.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_or.Scalar], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_or.Scalar], key=[CPU]\n",
      "   [call] op=[aten::bitwise_or.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_left_shift.Scalar_Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_left_shift.Scalar_Tensor], key=[CPU]\n",
      "   [call] op=[aten::bitwise_left_shift.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_right_shift.Scalar_Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_right_shift.Scalar_Tensor], key=[CPU]\n",
      "   [call] op=[aten::bitwise_right_shift.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_left_shift.Scalar_Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_left_shift.Scalar_Tensor], key=[CPU]\n",
      "   [call] op=[aten::bitwise_left_shift.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      " [call] op=[aten::bitwise_right_shift.Scalar_Tensor], key=[AutogradCPU]\n",
      "  [redispatchBoxed] op=[aten::bitwise_right_shift.Scalar_Tensor], key=[CPU]\n",
      "   [call] op=[aten::bitwise_right_shift.Tensor], key=[CPU]\n",
      "    [call] op=[aten::to.dtype], key=[CPU]\n",
      "     [call] op=[aten::_to_copy], key=[BackendSelect]\n",
      "      [redispatch] op=[aten::_to_copy], key=[CPU]\n",
      "       [call] op=[aten::empty_strided], key=[BackendSelect]\n",
      "        [redispatch] op=[aten::empty_strided], key=[CPU]\n",
      "       [call] op=[aten::copy_], key=[CPU]\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    }
   ],
   "source": [
    "%env TORCH_SHOW_DISPATCH_TRACE=1\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "device = torch_xla.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3,1]{2,1,0} aten::view(%0), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %2 = f32[1,3,3]{0,2,1} aten::permute(%1), xla_shape=f32[1,3,3]{0,2,1}\n",
      "  %3 = f32[3,3,1]{2,0,1} aten::permute(%2), xla_shape=f32[3,3,1]{2,0,1}\n",
      "  %4 = f32[1,3,3]{2,1,0} aten::view(%3), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %5 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %6 = f32[3,3,1]{2,1,0} aten::view(%5), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %7 = f32[3,1,3]{1,2,0} aten::permute(%6), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %8 = f32[3,3,1]{2,1,0} aten::permute(%7), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %9 = f32[1,3,3]{2,1,0} aten::view(%8), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %10 = f32[1,3,3]{2,1,0} aten::matmul(%9, %4), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %11 = f32[3,1,3]{2,1,0} aten::view(%10), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %12 = f32[3,3,1]{1,2,0} aten::permute(%11), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %13 = f32[3,3]{1,0} aten::view(%12), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "device = torch_xla.device()\n",
    "x = torch.randn(3, 3, requires_grad=False, device=device)\n",
    "y = torch.randn(3, 3, requires_grad=False, device=device)\n",
    "torch_xla.sync()\n",
    "out = torch.einsum('...n,mn->...m', x, y)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3,1]{2,1,0} aten::view(%0), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %2 = f32[1,3,3]{0,2,1} aten::permute(%1), xla_shape=f32[1,3,3]{0,2,1}\n",
      "  %3 = f32[3,3,1]{2,0,1} aten::permute(%2), xla_shape=f32[3,3,1]{2,0,1}\n",
      "  %4 = f32[1,3,3]{2,1,0} aten::view(%3), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %5 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %6 = f32[3,3,1]{2,1,0} aten::view(%5), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %7 = f32[3,1,3]{1,2,0} aten::permute(%6), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %8 = f32[3,3,1]{2,1,0} aten::permute(%7), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %9 = f32[1,3,3]{2,1,0} aten::view(%8), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %10 = f32[1,3,3]{2,1,0} aten::matmul(%9, %4), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %11 = f32[3,1,3]{2,1,0} aten::view(%10), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %12 = f32[3,3,1]{1,2,0} aten::permute(%11), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %13 = f32[3,3]{1,0} aten::view(%12), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "device = torch_xla.device()\n",
    "x = torch.randn(3, 3, requires_grad=True, device=device)\n",
    "y = torch.randn(3, 3, requires_grad=True, device=device)\n",
    "torch_xla.sync()\n",
    "out = torch.einsum('...n,mn->...m', x, y)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Does creating with gradient enabled make a difference?\n",
      "-- Test1\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3,1]{2,1,0} aten::view(%0), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %2 = f32[1,3,3]{0,1,2} aten::permute(%1), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %3 = f32[3,3,1]{2,1,0} aten::permute(%2), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %4 = f32[1,3,3]{2,1,0} aten::view(%3), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %5 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %6 = f32[3,3,1]{2,1,0} aten::view(%5), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %7 = f32[3,1,3]{1,2,0} aten::permute(%6), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %8 = f32[3,3,1]{2,1,0} aten::permute(%7), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %9 = f32[1,3,3]{2,1,0} aten::view(%8), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %10 = f32[1,3,3]{2,1,0} aten::matmul(%9, %4), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %11 = f32[3,1,3]{2,1,0} aten::view(%10), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %12 = f32[3,3,1]{1,2,0} aten::permute(%11), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %13 = f32[3,3]{1,0} aten::view(%12), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Test2\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3,1]{2,1,0} aten::view(%0), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %2 = f32[1,3,3]{0,1,2} aten::permute(%1), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %3 = f32[3,3,1]{2,1,0} aten::permute(%2), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %4 = f32[1,3,3]{2,1,0} aten::view(%3), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %5 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %6 = f32[3,3,1]{2,1,0} aten::view(%5), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %7 = f32[3,1,3]{1,2,0} aten::permute(%6), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %8 = f32[3,3,1]{2,1,0} aten::permute(%7), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %9 = f32[1,3,3]{2,1,0} aten::view(%8), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %10 = f32[1,3,3]{2,1,0} aten::matmul(%9, %4), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %11 = f32[3,1,3]{2,1,0} aten::view(%10), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %12 = f32[3,3,1]{1,2,0} aten::permute(%11), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %13 = f32[3,3]{1,0} aten::view(%12), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Test3\n",
      "IR {\n",
      "  %0 = s64[] xla::device_data(), xla_shape=s64[]\n",
      "  %1 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %2 = s64[] aten::mul(%1, %0), xla_shape=s64[]\n",
      "  %3 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %4 = s64[] aten::add(%3, %2), xla_shape=s64[]\n",
      "  %5 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %6 = s64[] aten::mul(%5, %4), xla_shape=s64[]\n",
      "  %7 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %8 = s64[] aten::add(%7, %6), xla_shape=s64[]\n",
      "  %9 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %10 = f32[3,3]{1,0} aten::expand(%9), xla_shape=f32[3,3]{1,0}\n",
      "  %11 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %12 = f32[3,3]{1,0} aten::expand(%11), xla_shape=f32[3,3]{1,0}\n",
      "  %13 = f32[3,3]{1,0} aten::normal(%12, %10, %8), xla_shape=f32[3,3]{1,0}\n",
      "  %14 = f32[3,3,1]{2,1,0} aten::view(%13), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %15 = f32[1,3,3]{0,1,2} aten::permute(%14), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %16 = f32[3,3,1]{2,1,0} aten::permute(%15), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %17 = f32[1,3,3]{2,1,0} aten::view(%16), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %18 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %19 = f32[3,3]{1,0} aten::expand(%18), xla_shape=f32[3,3]{1,0}\n",
      "  %20 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %21 = f32[3,3]{1,0} aten::expand(%20), xla_shape=f32[3,3]{1,0}\n",
      "  %22 = f32[3,3]{1,0} aten::normal(%21, %19, %4), xla_shape=f32[3,3]{1,0}\n",
      "  %23 = f32[3,3,1]{2,1,0} aten::view(%22), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %24 = f32[3,1,3]{1,2,0} aten::permute(%23), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %25 = f32[3,3,1]{2,1,0} aten::permute(%24), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %26 = f32[1,3,3]{2,1,0} aten::view(%25), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %27 = f32[1,3,3]{2,1,0} aten::matmul(%26, %17), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %28 = f32[3,1,3]{2,1,0} aten::view(%27), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %29 = f32[3,3,1]{1,2,0} aten::permute(%28), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %30 = f32[3,3]{1,0} aten::view(%29), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Test4\n",
      "IR {\n",
      "  %0 = s64[] xla::device_data(), xla_shape=s64[]\n",
      "  %1 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %2 = s64[] aten::mul(%1, %0), xla_shape=s64[]\n",
      "  %3 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %4 = s64[] aten::add(%3, %2), xla_shape=s64[]\n",
      "  %5 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %6 = s64[] aten::mul(%5, %4), xla_shape=s64[]\n",
      "  %7 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %8 = s64[] aten::add(%7, %6), xla_shape=s64[]\n",
      "  %9 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %10 = s64[] aten::mul(%9, %8), xla_shape=s64[]\n",
      "  %11 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %12 = s64[] aten::add(%11, %10), xla_shape=s64[]\n",
      "  %13 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %14 = s64[] aten::mul(%13, %12), xla_shape=s64[]\n",
      "  %15 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %16 = s64[] aten::add(%15, %14), xla_shape=s64[]\n",
      "  %17 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %18 = f32[3,3]{1,0} aten::expand(%17), xla_shape=f32[3,3]{1,0}\n",
      "  %19 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %20 = f32[3,3]{1,0} aten::expand(%19), xla_shape=f32[3,3]{1,0}\n",
      "  %21 = f32[3,3]{1,0} aten::normal(%20, %18, %16), xla_shape=f32[3,3]{1,0}\n",
      "  %22 = f32[3,3,1]{2,1,0} aten::view(%21), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %23 = f32[1,3,3]{0,1,2} aten::permute(%22), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %24 = f32[3,3,1]{2,1,0} aten::permute(%23), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %25 = f32[1,3,3]{2,1,0} aten::view(%24), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %26 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %27 = f32[3,3]{1,0} aten::expand(%26), xla_shape=f32[3,3]{1,0}\n",
      "  %28 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %29 = f32[3,3]{1,0} aten::expand(%28), xla_shape=f32[3,3]{1,0}\n",
      "  %30 = f32[3,3]{1,0} aten::normal(%29, %27, %12), xla_shape=f32[3,3]{1,0}\n",
      "  %31 = f32[3,3,1]{2,1,0} aten::view(%30), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %32 = f32[3,1,3]{1,2,0} aten::permute(%31), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %33 = f32[3,3,1]{2,1,0} aten::permute(%32), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %34 = f32[1,3,3]{2,1,0} aten::view(%33), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %35 = f32[1,3,3]{2,1,0} aten::matmul(%34, %25), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %36 = f32[3,1,3]{2,1,0} aten::view(%35), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %37 = f32[3,3,1]{1,2,0} aten::permute(%36), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %38 = f32[3,3]{1,0} aten::view(%37), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Does creating with gradient enabled make a difference?\")\n",
    "\n",
    "print(\"-- Test1\")\n",
    "x_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "y_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Test2\")\n",
    "x_2= torch.randn(3, 3, requires_grad=True)\n",
    "y_2= torch.randn(3, 3, requires_grad=True)\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    out_einsum = torch.einsum('...m,mn->...n', x_2.to('xla'), y_2.to('xla'))\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Test3\")\n",
    "x_1= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "y_1= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Test4\")\n",
    "x_2= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "y_2= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    out_einsum = torch.einsum('...m,mn->...n', x_2.to('xla'), y_2.to('xla'))\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem like gradient made a difference. However, device creation mattered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Is there a different if we create in the CPU, and then transfer to XLA?\n",
      "-- Check 1\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3,1]{2,1,0} aten::view(%0), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %2 = f32[1,3,3]{0,1,2} aten::permute(%1), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %3 = f32[3,3,1]{2,1,0} aten::permute(%2), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %4 = f32[1,3,3]{2,1,0} aten::view(%3), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %5 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %6 = f32[3,3,1]{2,1,0} aten::view(%5), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %7 = f32[3,1,3]{1,2,0} aten::permute(%6), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %8 = f32[3,3,1]{2,1,0} aten::permute(%7), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %9 = f32[1,3,3]{2,1,0} aten::view(%8), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %10 = f32[1,3,3]{2,1,0} aten::matmul(%9, %4), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %11 = f32[3,1,3]{2,1,0} aten::view(%10), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %12 = f32[3,3,1]{1,2,0} aten::permute(%11), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %13 = f32[3,3]{1,0} aten::view(%12), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Check 2\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3,1]{2,1,0} aten::view(%0), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %2 = f32[1,3,3]{0,1,2} aten::permute(%1), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %3 = f32[3,3,1]{2,1,0} aten::permute(%2), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %4 = f32[1,3,3]{2,1,0} aten::view(%3), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %5 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %6 = f32[3,3,1]{2,1,0} aten::view(%5), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %7 = f32[3,1,3]{1,2,0} aten::permute(%6), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %8 = f32[3,3,1]{2,1,0} aten::permute(%7), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %9 = f32[1,3,3]{2,1,0} aten::view(%8), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %10 = f32[1,3,3]{2,1,0} aten::matmul(%9, %4), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %11 = f32[3,1,3]{2,1,0} aten::view(%10), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %12 = f32[3,3,1]{1,2,0} aten::permute(%11), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %13 = f32[3,3]{1,0} aten::view(%12), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Check 3\n",
      "IR {\n",
      "  %0 = s64[] xla::device_data(), xla_shape=s64[]\n",
      "  %1 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %2 = s64[] aten::mul(%1, %0), xla_shape=s64[]\n",
      "  %3 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %4 = s64[] aten::add(%3, %2), xla_shape=s64[]\n",
      "  %5 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %6 = s64[] aten::mul(%5, %4), xla_shape=s64[]\n",
      "  %7 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %8 = s64[] aten::add(%7, %6), xla_shape=s64[]\n",
      "  %9 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %10 = s64[] aten::mul(%9, %8), xla_shape=s64[]\n",
      "  %11 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %12 = s64[] aten::add(%11, %10), xla_shape=s64[]\n",
      "  %13 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %14 = s64[] aten::mul(%13, %12), xla_shape=s64[]\n",
      "  %15 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %16 = s64[] aten::add(%15, %14), xla_shape=s64[]\n",
      "  %17 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %18 = s64[] aten::mul(%17, %16), xla_shape=s64[]\n",
      "  %19 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %20 = s64[] aten::add(%19, %18), xla_shape=s64[]\n",
      "  %21 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %22 = s64[] aten::mul(%21, %20), xla_shape=s64[]\n",
      "  %23 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %24 = s64[] aten::add(%23, %22), xla_shape=s64[]\n",
      "  %25 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %26 = f32[3,3]{1,0} aten::expand(%25), xla_shape=f32[3,3]{1,0}\n",
      "  %27 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %28 = f32[3,3]{1,0} aten::expand(%27), xla_shape=f32[3,3]{1,0}\n",
      "  %29 = f32[3,3]{1,0} aten::normal(%28, %26, %24), xla_shape=f32[3,3]{1,0}\n",
      "  %30 = f32[3,3,1]{2,1,0} aten::view(%29), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %31 = f32[1,3,3]{0,1,2} aten::permute(%30), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %32 = f32[3,3,1]{2,1,0} aten::permute(%31), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %33 = f32[1,3,3]{2,1,0} aten::view(%32), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %34 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %35 = f32[3,3]{1,0} aten::expand(%34), xla_shape=f32[3,3]{1,0}\n",
      "  %36 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %37 = f32[3,3]{1,0} aten::expand(%36), xla_shape=f32[3,3]{1,0}\n",
      "  %38 = f32[3,3]{1,0} aten::normal(%37, %35, %20), xla_shape=f32[3,3]{1,0}\n",
      "  %39 = f32[3,3,1]{2,1,0} aten::view(%38), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %40 = f32[3,1,3]{1,2,0} aten::permute(%39), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %41 = f32[3,3,1]{2,1,0} aten::permute(%40), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %42 = f32[1,3,3]{2,1,0} aten::view(%41), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %43 = f32[1,3,3]{2,1,0} aten::matmul(%42, %33), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %44 = f32[3,1,3]{2,1,0} aten::view(%43), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %45 = f32[3,3,1]{1,2,0} aten::permute(%44), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %46 = f32[3,3]{1,0} aten::view(%45), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Check 4\n",
      "IR {\n",
      "  %0 = s64[] xla::device_data(), xla_shape=s64[]\n",
      "  %1 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %2 = s64[] aten::mul(%1, %0), xla_shape=s64[]\n",
      "  %3 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %4 = s64[] aten::add(%3, %2), xla_shape=s64[]\n",
      "  %5 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %6 = s64[] aten::mul(%5, %4), xla_shape=s64[]\n",
      "  %7 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %8 = s64[] aten::add(%7, %6), xla_shape=s64[]\n",
      "  %9 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %10 = s64[] aten::mul(%9, %8), xla_shape=s64[]\n",
      "  %11 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %12 = s64[] aten::add(%11, %10), xla_shape=s64[]\n",
      "  %13 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %14 = s64[] aten::mul(%13, %12), xla_shape=s64[]\n",
      "  %15 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %16 = s64[] aten::add(%15, %14), xla_shape=s64[]\n",
      "  %17 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %18 = s64[] aten::mul(%17, %16), xla_shape=s64[]\n",
      "  %19 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %20 = s64[] aten::add(%19, %18), xla_shape=s64[]\n",
      "  %21 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %22 = s64[] aten::mul(%21, %20), xla_shape=s64[]\n",
      "  %23 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %24 = s64[] aten::add(%23, %22), xla_shape=s64[]\n",
      "  %25 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %26 = s64[] aten::mul(%25, %24), xla_shape=s64[]\n",
      "  %27 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %28 = s64[] aten::add(%27, %26), xla_shape=s64[]\n",
      "  %29 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %30 = s64[] aten::mul(%29, %28), xla_shape=s64[]\n",
      "  %31 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %32 = s64[] aten::add(%31, %30), xla_shape=s64[]\n",
      "  %33 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %34 = f32[3,3]{1,0} aten::expand(%33), xla_shape=f32[3,3]{1,0}\n",
      "  %35 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %36 = f32[3,3]{1,0} aten::expand(%35), xla_shape=f32[3,3]{1,0}\n",
      "  %37 = f32[3,3]{1,0} aten::normal(%36, %34, %32), xla_shape=f32[3,3]{1,0}\n",
      "  %38 = f32[3,3,1]{2,1,0} aten::view(%37), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %39 = f32[1,3,3]{0,1,2} aten::permute(%38), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %40 = f32[3,3,1]{2,1,0} aten::permute(%39), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %41 = f32[1,3,3]{2,1,0} aten::view(%40), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %42 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %43 = f32[3,3]{1,0} aten::expand(%42), xla_shape=f32[3,3]{1,0}\n",
      "  %44 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %45 = f32[3,3]{1,0} aten::expand(%44), xla_shape=f32[3,3]{1,0}\n",
      "  %46 = f32[3,3]{1,0} aten::normal(%45, %43, %28), xla_shape=f32[3,3]{1,0}\n",
      "  %47 = f32[3,3,1]{2,1,0} aten::view(%46), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %48 = f32[3,1,3]{1,2,0} aten::permute(%47), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %49 = f32[3,3,1]{2,1,0} aten::permute(%48), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %50 = f32[1,3,3]{2,1,0} aten::view(%49), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %51 = f32[1,3,3]{2,1,0} aten::matmul(%50, %41), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %52 = f32[3,1,3]{2,1,0} aten::view(%51), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %53 = f32[3,3,1]{1,2,0} aten::permute(%52), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %54 = f32[3,3]{1,0} aten::view(%53), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Check 5\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3,1]{2,1,0} aten::view(%0), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %2 = f32[1,3,3]{0,1,2} aten::permute(%1), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %3 = f32[3,3,1]{2,1,0} aten::permute(%2), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %4 = f32[1,3,3]{2,1,0} aten::view(%3), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %5 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %6 = f32[3,3,1]{2,1,0} aten::view(%5), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %7 = f32[3,1,3]{1,2,0} aten::permute(%6), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %8 = f32[3,3,1]{2,1,0} aten::permute(%7), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %9 = f32[1,3,3]{2,1,0} aten::view(%8), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %10 = f32[1,3,3]{2,1,0} aten::matmul(%9, %4), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %11 = f32[3,1,3]{2,1,0} aten::view(%10), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %12 = f32[3,3,1]{1,2,0} aten::permute(%11), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %13 = f32[3,3]{1,0} aten::view(%12), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Is there a different if we create in the CPU, and then transfer to XLA?\")\n",
    "print(\"-- Check 1\")\n",
    "x_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "y_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Check 2\")\n",
    "x_1= torch.randn(3, 3, requires_grad=True).to('xla')\n",
    "y_1= torch.randn(3, 3, requires_grad=True).to('xla')\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Check 3\")\n",
    "with torch_xla.runtime.xla_device():\n",
    "  x_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "  y_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Check 4\")\n",
    "with torch_xla.runtime.xla_device():\n",
    "  x_1= torch.randn(3, 3, requires_grad=True).to('xla')\n",
    "  y_1= torch.randn(3, 3, requires_grad=True).to('xla')\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Check 5\")\n",
    "x_1= torch.randn(3, 3, requires_grad=True)\n",
    "y_1= torch.randn(3, 3, requires_grad=True)\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1.to('xla'), y_1.to('xla'))\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "It seems like creating the tensor in the xla_device is causing the failure. If we create it, and then transition to the TPU, it succeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Does creating with gradient enabled make a difference?\n",
      "-- Test1\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3,1]{2,1,0} aten::view(%0), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %2 = f32[1,3,3]{0,1,2} aten::permute(%1), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %3 = f32[3,3,1]{2,1,0} aten::permute(%2), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %4 = f32[1,3,3]{2,1,0} aten::view(%3), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %5 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %6 = f32[3,3,1]{2,1,0} aten::view(%5), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %7 = f32[3,1,3]{1,2,0} aten::permute(%6), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %8 = f32[3,3,1]{2,1,0} aten::permute(%7), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %9 = f32[1,3,3]{2,1,0} aten::view(%8), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %10 = f32[1,3,3]{2,1,0} aten::matmul(%9, %4), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %11 = f32[3,1,3]{2,1,0} aten::view(%10), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %12 = f32[3,3,1]{1,2,0} aten::permute(%11), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %13 = f32[3,3]{1,0} aten::view(%12), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Test2\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3,1]{2,1,0} aten::view(%0), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %2 = f32[1,3,3]{0,1,2} aten::permute(%1), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %3 = f32[3,3,1]{2,1,0} aten::permute(%2), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %4 = f32[1,3,3]{2,1,0} aten::view(%3), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %5 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %6 = f32[3,3,1]{2,1,0} aten::view(%5), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %7 = f32[3,1,3]{1,2,0} aten::permute(%6), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %8 = f32[3,3,1]{2,1,0} aten::permute(%7), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %9 = f32[1,3,3]{2,1,0} aten::view(%8), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %10 = f32[1,3,3]{2,1,0} aten::matmul(%9, %4), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %11 = f32[3,1,3]{2,1,0} aten::view(%10), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %12 = f32[3,3,1]{1,2,0} aten::permute(%11), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %13 = f32[3,3]{1,0} aten::view(%12), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Test3\n",
      "IR {\n",
      "  %0 = s64[] xla::device_data(), xla_shape=s64[]\n",
      "  %1 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %2 = s64[] aten::mul(%1, %0), xla_shape=s64[]\n",
      "  %3 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %4 = s64[] aten::add(%3, %2), xla_shape=s64[]\n",
      "  %5 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %6 = s64[] aten::mul(%5, %4), xla_shape=s64[]\n",
      "  %7 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %8 = s64[] aten::add(%7, %6), xla_shape=s64[]\n",
      "  %9 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %10 = s64[] aten::mul(%9, %8), xla_shape=s64[]\n",
      "  %11 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %12 = s64[] aten::add(%11, %10), xla_shape=s64[]\n",
      "  %13 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %14 = s64[] aten::mul(%13, %12), xla_shape=s64[]\n",
      "  %15 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %16 = s64[] aten::add(%15, %14), xla_shape=s64[]\n",
      "  %17 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %18 = s64[] aten::mul(%17, %16), xla_shape=s64[]\n",
      "  %19 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %20 = s64[] aten::add(%19, %18), xla_shape=s64[]\n",
      "  %21 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %22 = s64[] aten::mul(%21, %20), xla_shape=s64[]\n",
      "  %23 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %24 = s64[] aten::add(%23, %22), xla_shape=s64[]\n",
      "  %25 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %26 = s64[] aten::mul(%25, %24), xla_shape=s64[]\n",
      "  %27 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %28 = s64[] aten::add(%27, %26), xla_shape=s64[]\n",
      "  %29 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %30 = s64[] aten::mul(%29, %28), xla_shape=s64[]\n",
      "  %31 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %32 = s64[] aten::add(%31, %30), xla_shape=s64[]\n",
      "  %33 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %34 = s64[] aten::mul(%33, %32), xla_shape=s64[]\n",
      "  %35 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %36 = s64[] aten::add(%35, %34), xla_shape=s64[]\n",
      "  %37 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %38 = s64[] aten::mul(%37, %36), xla_shape=s64[]\n",
      "  %39 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %40 = s64[] aten::add(%39, %38), xla_shape=s64[]\n",
      "  %41 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %42 = f32[3,3]{1,0} aten::expand(%41), xla_shape=f32[3,3]{1,0}\n",
      "  %43 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %44 = f32[3,3]{1,0} aten::expand(%43), xla_shape=f32[3,3]{1,0}\n",
      "  %45 = f32[3,3]{1,0} aten::normal(%44, %42, %40), xla_shape=f32[3,3]{1,0}\n",
      "  %46 = f32[3,3,1]{2,1,0} aten::view(%45), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %47 = f32[1,3,3]{0,1,2} aten::permute(%46), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %48 = f32[3,3,1]{2,1,0} aten::permute(%47), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %49 = f32[1,3,3]{2,1,0} aten::view(%48), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %50 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %51 = f32[3,3]{1,0} aten::expand(%50), xla_shape=f32[3,3]{1,0}\n",
      "  %52 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %53 = f32[3,3]{1,0} aten::expand(%52), xla_shape=f32[3,3]{1,0}\n",
      "  %54 = f32[3,3]{1,0} aten::normal(%53, %51, %36), xla_shape=f32[3,3]{1,0}\n",
      "  %55 = f32[3,3,1]{2,1,0} aten::view(%54), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %56 = f32[3,1,3]{1,2,0} aten::permute(%55), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %57 = f32[3,3,1]{2,1,0} aten::permute(%56), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %58 = f32[1,3,3]{2,1,0} aten::view(%57), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %59 = f32[1,3,3]{2,1,0} aten::matmul(%58, %49), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %60 = f32[3,1,3]{2,1,0} aten::view(%59), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %61 = f32[3,3,1]{1,2,0} aten::permute(%60), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %62 = f32[3,3]{1,0} aten::view(%61), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Test4\n",
      "IR {\n",
      "  %0 = s64[] xla::device_data(), xla_shape=s64[]\n",
      "  %1 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %2 = s64[] aten::mul(%1, %0), xla_shape=s64[]\n",
      "  %3 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %4 = s64[] aten::add(%3, %2), xla_shape=s64[]\n",
      "  %5 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %6 = s64[] aten::mul(%5, %4), xla_shape=s64[]\n",
      "  %7 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %8 = s64[] aten::add(%7, %6), xla_shape=s64[]\n",
      "  %9 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %10 = s64[] aten::mul(%9, %8), xla_shape=s64[]\n",
      "  %11 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %12 = s64[] aten::add(%11, %10), xla_shape=s64[]\n",
      "  %13 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %14 = s64[] aten::mul(%13, %12), xla_shape=s64[]\n",
      "  %15 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %16 = s64[] aten::add(%15, %14), xla_shape=s64[]\n",
      "  %17 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %18 = s64[] aten::mul(%17, %16), xla_shape=s64[]\n",
      "  %19 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %20 = s64[] aten::add(%19, %18), xla_shape=s64[]\n",
      "  %21 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %22 = s64[] aten::mul(%21, %20), xla_shape=s64[]\n",
      "  %23 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %24 = s64[] aten::add(%23, %22), xla_shape=s64[]\n",
      "  %25 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %26 = s64[] aten::mul(%25, %24), xla_shape=s64[]\n",
      "  %27 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %28 = s64[] aten::add(%27, %26), xla_shape=s64[]\n",
      "  %29 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %30 = s64[] aten::mul(%29, %28), xla_shape=s64[]\n",
      "  %31 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %32 = s64[] aten::add(%31, %30), xla_shape=s64[]\n",
      "  %33 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %34 = s64[] aten::mul(%33, %32), xla_shape=s64[]\n",
      "  %35 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %36 = s64[] aten::add(%35, %34), xla_shape=s64[]\n",
      "  %37 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %38 = s64[] aten::mul(%37, %36), xla_shape=s64[]\n",
      "  %39 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %40 = s64[] aten::add(%39, %38), xla_shape=s64[]\n",
      "  %41 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %42 = s64[] aten::mul(%41, %40), xla_shape=s64[]\n",
      "  %43 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %44 = s64[] aten::add(%43, %42), xla_shape=s64[]\n",
      "  %45 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %46 = s64[] aten::mul(%45, %44), xla_shape=s64[]\n",
      "  %47 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %48 = s64[] aten::add(%47, %46), xla_shape=s64[]\n",
      "  %49 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %50 = f32[3,3]{1,0} aten::expand(%49), xla_shape=f32[3,3]{1,0}\n",
      "  %51 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %52 = f32[3,3]{1,0} aten::expand(%51), xla_shape=f32[3,3]{1,0}\n",
      "  %53 = f32[3,3]{1,0} aten::normal(%52, %50, %48), xla_shape=f32[3,3]{1,0}\n",
      "  %54 = f32[3,3,1]{2,1,0} aten::view(%53), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %55 = f32[1,3,3]{0,1,2} aten::permute(%54), xla_shape=f32[1,3,3]{0,1,2}\n",
      "  %56 = f32[3,3,1]{2,1,0} aten::permute(%55), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %57 = f32[1,3,3]{2,1,0} aten::view(%56), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %58 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %59 = f32[3,3]{1,0} aten::expand(%58), xla_shape=f32[3,3]{1,0}\n",
      "  %60 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %61 = f32[3,3]{1,0} aten::expand(%60), xla_shape=f32[3,3]{1,0}\n",
      "  %62 = f32[3,3]{1,0} aten::normal(%61, %59, %44), xla_shape=f32[3,3]{1,0}\n",
      "  %63 = f32[3,3,1]{2,1,0} aten::view(%62), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %64 = f32[3,1,3]{1,2,0} aten::permute(%63), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %65 = f32[3,3,1]{2,1,0} aten::permute(%64), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %66 = f32[1,3,3]{2,1,0} aten::view(%65), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %67 = f32[1,3,3]{2,1,0} aten::matmul(%66, %57), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %68 = f32[3,1,3]{2,1,0} aten::view(%67), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %69 = f32[3,3,1]{1,2,0} aten::permute(%68), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %70 = f32[3,3]{1,0} aten::view(%69), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Does creating with gradient enabled make a difference?\")\n",
    "\n",
    "print(\"-- Test1\")\n",
    "x_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "y_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Test2\")\n",
    "x_2= torch.randn(3, 3, requires_grad=True)\n",
    "y_2= torch.randn(3, 3, requires_grad=True)\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_2.to('xla'), y_2.to('xla'))\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Test3\")\n",
    "x_1= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "y_1= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Test4\")\n",
    "x_2= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "y_2= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_2.to('xla'), y_2.to('xla'))\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_SHOW_DISPATCH_TRACE=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [call] op=[aten::zeros], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::zeros], key=[XLA]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[XLA]\n",
      "   [call] op=[aten::zero_], key=[Functionalize]\n",
      "    [call] op=[aten::zero_], key=[Meta]\n",
      "    [call] op=[aten::zero], key=[XLA]\n",
      "     [call] op=[aten::clone], key=[XLA]\n",
      "     [call] op=[aten::zero_], key=[XLA]\n",
      "    [call] op=[aten::_propagate_xla_data], key=[XLA]\n",
      " [call] op=[aten::zeros], key=[BackendSelect]\n",
      "  [redispatch] op=[aten::zeros], key=[XLA]\n",
      "   [call] op=[aten::empty.memory_format], key=[BackendSelect]\n",
      "    [redispatch] op=[aten::empty.memory_format], key=[XLA]\n",
      "   [call] op=[aten::zero_], key=[Functionalize]\n",
      "    [call] op=[aten::zero_], key=[Meta]\n",
      "    [call] op=[aten::zero], key=[XLA]\n",
      "     [call] op=[aten::clone], key=[XLA]\n",
      "     [call] op=[aten::zero_], key=[XLA]\n",
      "    [call] op=[aten::_propagate_xla_data], key=[XLA]\n"
     ]
    }
   ],
   "source": [
    "%env TORCH_SHOW_DISPATCH_TRACE=1\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.library import custom_op\n",
    "import time\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "device = torch_xla.device()\n",
    "\n",
    "@custom_op(\"xla::custom_linear_forward123\", schema=\"(Tensor input, Tensor weight) -> Tensor\", mutates_args=())\n",
    "def custom_linear_forward123(input: Tensor, weight: Tensor):\n",
    "    return torch.einsum('...n,mn->...m', input, weight)\n",
    "\n",
    "X = torch.zeros(3, 3, requires_grad=False, device='xla')\n",
    "Y = torch.zeros(3, 3, requires_grad=False, device='xla')\n",
    "\n",
    "def test_lowering(func):\n",
    "  time.sleep(1)\n",
    "  out = func(X, Y)\n",
    "  time.sleep(1)\n",
    "  ir = torch_xla._XLAC._get_xla_tensors_text([out])\n",
    "  if 'einsum' not in ir:\n",
    "    print(\"!!!!!!!!!!WRONG!!!!!!!!!!! Did not find einsum in lowering\")\n",
    "    print(\"IR:\")\n",
    "    print(ir)\n",
    "  else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_SHOW_DISPATCH_TRACE=1\n"
     ]
    }
   ],
   "source": [
    "%env TORCH_SHOW_DISPATCH_TRACE=1\n",
    "@custom_op(\"xla::custom_max_pool\", schema=\"(Tensor input) -> Tensor\", mutates_args=())\n",
    "def custom_max_pool(input: Tensor):\n",
    "    return torch.max_pool2d(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_SHOW_DISPATCH_TRACE=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [callBoxed] op=[xla::custom_linear_forward123], key=[AutogradXLA]\n",
      "  [redispatchBoxed] op=[xla::custom_linear_forward123], key=[Functionalize]\n",
      "   [callBoxed] op=[xla::custom_linear_forward123], key=[XLA]\n",
      "    [call] op=[aten::einsum], key=[XLA]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%env TORCH_SHOW_DISPATCH_TRACE=1\n",
    "\n",
    "test_lowering(lambda a, b: custom_linear_forward123(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [call] op=[aten::einsum], key=[AutogradXLA]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "test_lowering(lambda a, b: torch.einsum('...n,mn->...m', a, b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
