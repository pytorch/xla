{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "device = torch_xla.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einsum: Call native function\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %2 = f32[3,3]{1,0} aten::einsum(%1, %0), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "device = torch_xla.device()\n",
    "x = torch.randn(3, 3, requires_grad=False, device=device)\n",
    "y = torch.randn(3, 3, requires_grad=False, device=device)\n",
    "torch_xla.sync()\n",
    "out = torch.einsum('...n,mn->...m', x, y)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einsum: Calling lowered eisum\n",
      "Einsum: Build lowered eisum\n",
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %2 = f32[3,3]{1,0} aten::einsum(%1, %0), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "device = torch_xla.device()\n",
    "x = torch.randn(3, 3, requires_grad=True, device=device)\n",
    "y = torch.randn(3, 3, requires_grad=True, device=device)\n",
    "torch_xla.sync()\n",
    "out = torch.einsum('...n,mn->...m', x, y)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Does creating with gradient enabled make a difference?\n",
      "-- Test1\n",
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %2 = f32[3,3]{1,0} aten::einsum(%1, %0), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Test2\n",
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %2 = f32[3,3]{1,0} aten::einsum(%1, %0), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Test3\n",
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "IR {\n",
      "  %0 = s64[] xla::device_data(), xla_shape=s64[]\n",
      "  %1 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %2 = s64[] aten::mul(%1, %0), xla_shape=s64[]\n",
      "  %3 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %4 = s64[] aten::add(%3, %2), xla_shape=s64[]\n",
      "  %5 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %6 = s64[] aten::mul(%5, %4), xla_shape=s64[]\n",
      "  %7 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %8 = s64[] aten::add(%7, %6), xla_shape=s64[]\n",
      "  %9 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %10 = s64[] aten::mul(%9, %8), xla_shape=s64[]\n",
      "  %11 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %12 = s64[] aten::add(%11, %10), xla_shape=s64[]\n",
      "  %13 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %14 = s64[] aten::mul(%13, %12), xla_shape=s64[]\n",
      "  %15 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %16 = s64[] aten::add(%15, %14), xla_shape=s64[]\n",
      "  %17 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %18 = s64[] aten::mul(%17, %16), xla_shape=s64[]\n",
      "  %19 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %20 = s64[] aten::add(%19, %18), xla_shape=s64[]\n",
      "  %21 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %22 = s64[] aten::mul(%21, %20), xla_shape=s64[]\n",
      "  %23 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %24 = s64[] aten::add(%23, %22), xla_shape=s64[]\n",
      "  %25 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %26 = s64[] aten::mul(%25, %24), xla_shape=s64[]\n",
      "  %27 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %28 = s64[] aten::add(%27, %26), xla_shape=s64[]\n",
      "  %29 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %30 = s64[] aten::mul(%29, %28), xla_shape=s64[]\n",
      "  %31 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %32 = s64[] aten::add(%31, %30), xla_shape=s64[]\n",
      "  %33 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %34 = s64[] aten::mul(%33, %32), xla_shape=s64[]\n",
      "  %35 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %36 = s64[] aten::add(%35, %34), xla_shape=s64[]\n",
      "  %37 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %38 = s64[] aten::mul(%37, %36), xla_shape=s64[]\n",
      "  %39 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %40 = s64[] aten::add(%39, %38), xla_shape=s64[]\n",
      "  %41 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %42 = s64[] aten::mul(%41, %40), xla_shape=s64[]\n",
      "  %43 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %44 = s64[] aten::add(%43, %42), xla_shape=s64[]\n",
      "  %45 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %46 = s64[] aten::mul(%45, %44), xla_shape=s64[]\n",
      "  %47 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %48 = s64[] aten::add(%47, %46), xla_shape=s64[]\n",
      "  %49 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %50 = s64[] aten::mul(%49, %48), xla_shape=s64[]\n",
      "  %51 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %52 = s64[] aten::add(%51, %50), xla_shape=s64[]\n",
      "  %53 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %54 = s64[] aten::mul(%53, %52), xla_shape=s64[]\n",
      "  %55 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %56 = s64[] aten::add(%55, %54), xla_shape=s64[]\n",
      "  %57 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %58 = s64[] aten::mul(%57, %56), xla_shape=s64[]\n",
      "  %59 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %60 = s64[] aten::add(%59, %58), xla_shape=s64[]\n",
      "  %61 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %62 = s64[] aten::mul(%61, %60), xla_shape=s64[]\n",
      "  %63 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %64 = s64[] aten::add(%63, %62), xla_shape=s64[]\n",
      "  %65 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %66 = s64[] aten::mul(%65, %64), xla_shape=s64[]\n",
      "  %67 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %68 = s64[] aten::add(%67, %66), xla_shape=s64[]\n",
      "  %69 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %70 = s64[] aten::mul(%69, %68), xla_shape=s64[]\n",
      "  %71 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %72 = s64[] aten::add(%71, %70), xla_shape=s64[]\n",
      "  %73 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %74 = s64[] aten::mul(%73, %72), xla_shape=s64[]\n",
      "  %75 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %76 = s64[] aten::add(%75, %74), xla_shape=s64[]\n",
      "  %77 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %78 = s64[] aten::mul(%77, %76), xla_shape=s64[]\n",
      "  %79 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %80 = s64[] aten::add(%79, %78), xla_shape=s64[]\n",
      "  %81 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %82 = f32[3,3]{1,0} aten::expand(%81), xla_shape=f32[3,3]{1,0}\n",
      "  %83 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %84 = f32[3,3]{1,0} aten::expand(%83), xla_shape=f32[3,3]{1,0}\n",
      "  %85 = f32[3,3]{1,0} aten::normal(%84, %82, %80), xla_shape=f32[3,3]{1,0}\n",
      "  %86 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %87 = f32[3,3]{1,0} aten::expand(%86), xla_shape=f32[3,3]{1,0}\n",
      "  %88 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %89 = f32[3,3]{1,0} aten::expand(%88), xla_shape=f32[3,3]{1,0}\n",
      "  %90 = f32[3,3]{1,0} aten::normal(%89, %87, %76), xla_shape=f32[3,3]{1,0}\n",
      "  %91 = f32[3,3]{1,0} aten::einsum(%90, %85), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Test4\n",
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "IR {\n",
      "  %0 = s64[] xla::device_data(), xla_shape=s64[]\n",
      "  %1 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %2 = s64[] aten::mul(%1, %0), xla_shape=s64[]\n",
      "  %3 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %4 = s64[] aten::add(%3, %2), xla_shape=s64[]\n",
      "  %5 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %6 = s64[] aten::mul(%5, %4), xla_shape=s64[]\n",
      "  %7 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %8 = s64[] aten::add(%7, %6), xla_shape=s64[]\n",
      "  %9 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %10 = s64[] aten::mul(%9, %8), xla_shape=s64[]\n",
      "  %11 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %12 = s64[] aten::add(%11, %10), xla_shape=s64[]\n",
      "  %13 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %14 = s64[] aten::mul(%13, %12), xla_shape=s64[]\n",
      "  %15 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %16 = s64[] aten::add(%15, %14), xla_shape=s64[]\n",
      "  %17 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %18 = s64[] aten::mul(%17, %16), xla_shape=s64[]\n",
      "  %19 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %20 = s64[] aten::add(%19, %18), xla_shape=s64[]\n",
      "  %21 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %22 = s64[] aten::mul(%21, %20), xla_shape=s64[]\n",
      "  %23 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %24 = s64[] aten::add(%23, %22), xla_shape=s64[]\n",
      "  %25 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %26 = s64[] aten::mul(%25, %24), xla_shape=s64[]\n",
      "  %27 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %28 = s64[] aten::add(%27, %26), xla_shape=s64[]\n",
      "  %29 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %30 = s64[] aten::mul(%29, %28), xla_shape=s64[]\n",
      "  %31 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %32 = s64[] aten::add(%31, %30), xla_shape=s64[]\n",
      "  %33 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %34 = s64[] aten::mul(%33, %32), xla_shape=s64[]\n",
      "  %35 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %36 = s64[] aten::add(%35, %34), xla_shape=s64[]\n",
      "  %37 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %38 = s64[] aten::mul(%37, %36), xla_shape=s64[]\n",
      "  %39 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %40 = s64[] aten::add(%39, %38), xla_shape=s64[]\n",
      "  %41 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %42 = s64[] aten::mul(%41, %40), xla_shape=s64[]\n",
      "  %43 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %44 = s64[] aten::add(%43, %42), xla_shape=s64[]\n",
      "  %45 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %46 = s64[] aten::mul(%45, %44), xla_shape=s64[]\n",
      "  %47 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %48 = s64[] aten::add(%47, %46), xla_shape=s64[]\n",
      "  %49 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %50 = s64[] aten::mul(%49, %48), xla_shape=s64[]\n",
      "  %51 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %52 = s64[] aten::add(%51, %50), xla_shape=s64[]\n",
      "  %53 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %54 = s64[] aten::mul(%53, %52), xla_shape=s64[]\n",
      "  %55 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %56 = s64[] aten::add(%55, %54), xla_shape=s64[]\n",
      "  %57 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %58 = s64[] aten::mul(%57, %56), xla_shape=s64[]\n",
      "  %59 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %60 = s64[] aten::add(%59, %58), xla_shape=s64[]\n",
      "  %61 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %62 = s64[] aten::mul(%61, %60), xla_shape=s64[]\n",
      "  %63 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %64 = s64[] aten::add(%63, %62), xla_shape=s64[]\n",
      "  %65 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %66 = s64[] aten::mul(%65, %64), xla_shape=s64[]\n",
      "  %67 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %68 = s64[] aten::add(%67, %66), xla_shape=s64[]\n",
      "  %69 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %70 = s64[] aten::mul(%69, %68), xla_shape=s64[]\n",
      "  %71 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %72 = s64[] aten::add(%71, %70), xla_shape=s64[]\n",
      "  %73 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %74 = s64[] aten::mul(%73, %72), xla_shape=s64[]\n",
      "  %75 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %76 = s64[] aten::add(%75, %74), xla_shape=s64[]\n",
      "  %77 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %78 = s64[] aten::mul(%77, %76), xla_shape=s64[]\n",
      "  %79 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %80 = s64[] aten::add(%79, %78), xla_shape=s64[]\n",
      "  %81 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %82 = s64[] aten::mul(%81, %80), xla_shape=s64[]\n",
      "  %83 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %84 = s64[] aten::add(%83, %82), xla_shape=s64[]\n",
      "  %85 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %86 = s64[] aten::mul(%85, %84), xla_shape=s64[]\n",
      "  %87 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %88 = s64[] aten::add(%87, %86), xla_shape=s64[]\n",
      "  %89 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %90 = f32[3,3]{1,0} aten::expand(%89), xla_shape=f32[3,3]{1,0}\n",
      "  %91 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %92 = f32[3,3]{1,0} aten::expand(%91), xla_shape=f32[3,3]{1,0}\n",
      "  %93 = f32[3,3]{1,0} aten::normal(%92, %90, %88), xla_shape=f32[3,3]{1,0}\n",
      "  %94 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %95 = f32[3,3]{1,0} aten::expand(%94), xla_shape=f32[3,3]{1,0}\n",
      "  %96 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %97 = f32[3,3]{1,0} aten::expand(%96), xla_shape=f32[3,3]{1,0}\n",
      "  %98 = f32[3,3]{1,0} aten::normal(%97, %95, %84), xla_shape=f32[3,3]{1,0}\n",
      "  %99 = f32[3,3]{1,0} aten::einsum(%98, %93), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Does creating with gradient enabled make a difference?\")\n",
    "\n",
    "print(\"-- Test1\")\n",
    "x_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "y_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Test2\")\n",
    "x_2= torch.randn(3, 3, requires_grad=True)\n",
    "y_2= torch.randn(3, 3, requires_grad=True)\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    out_einsum = torch.einsum('...m,mn->...n', x_2.to('xla'), y_2.to('xla'))\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Test3\")\n",
    "x_1= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "y_1= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Test4\")\n",
    "x_2= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "y_2= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "with torch.enable_grad():\n",
    "  with torch_xla.runtime.xla_device():\n",
    "    out_einsum = torch.einsum('...m,mn->...n', x_2.to('xla'), y_2.to('xla'))\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem like gradient made a difference. However, device creation mattered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Is there a different if we create in the CPU, and then transfer to XLA\n",
      "-- Check 1\n",
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %2 = f32[3,3]{1,0} aten::einsum(%1, %0), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Check 2\n",
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %2 = f32[3,3]{1,0} aten::einsum(%1, %0), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Check 3\n",
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "IR {\n",
      "  %0 = s64[] xla::device_data(), xla_shape=s64[]\n",
      "  %1 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %2 = s64[] aten::mul(%1, %0), xla_shape=s64[]\n",
      "  %3 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %4 = s64[] aten::add(%3, %2), xla_shape=s64[]\n",
      "  %5 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %6 = s64[] aten::mul(%5, %4), xla_shape=s64[]\n",
      "  %7 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %8 = s64[] aten::add(%7, %6), xla_shape=s64[]\n",
      "  %9 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %10 = s64[] aten::mul(%9, %8), xla_shape=s64[]\n",
      "  %11 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %12 = s64[] aten::add(%11, %10), xla_shape=s64[]\n",
      "  %13 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %14 = s64[] aten::mul(%13, %12), xla_shape=s64[]\n",
      "  %15 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %16 = s64[] aten::add(%15, %14), xla_shape=s64[]\n",
      "  %17 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %18 = s64[] aten::mul(%17, %16), xla_shape=s64[]\n",
      "  %19 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %20 = s64[] aten::add(%19, %18), xla_shape=s64[]\n",
      "  %21 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %22 = s64[] aten::mul(%21, %20), xla_shape=s64[]\n",
      "  %23 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %24 = s64[] aten::add(%23, %22), xla_shape=s64[]\n",
      "  %25 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %26 = s64[] aten::mul(%25, %24), xla_shape=s64[]\n",
      "  %27 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %28 = s64[] aten::add(%27, %26), xla_shape=s64[]\n",
      "  %29 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %30 = s64[] aten::mul(%29, %28), xla_shape=s64[]\n",
      "  %31 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %32 = s64[] aten::add(%31, %30), xla_shape=s64[]\n",
      "  %33 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %34 = s64[] aten::mul(%33, %32), xla_shape=s64[]\n",
      "  %35 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %36 = s64[] aten::add(%35, %34), xla_shape=s64[]\n",
      "  %37 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %38 = s64[] aten::mul(%37, %36), xla_shape=s64[]\n",
      "  %39 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %40 = s64[] aten::add(%39, %38), xla_shape=s64[]\n",
      "  %41 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %42 = s64[] aten::mul(%41, %40), xla_shape=s64[]\n",
      "  %43 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %44 = s64[] aten::add(%43, %42), xla_shape=s64[]\n",
      "  %45 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %46 = s64[] aten::mul(%45, %44), xla_shape=s64[]\n",
      "  %47 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %48 = s64[] aten::add(%47, %46), xla_shape=s64[]\n",
      "  %49 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %50 = s64[] aten::mul(%49, %48), xla_shape=s64[]\n",
      "  %51 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %52 = s64[] aten::add(%51, %50), xla_shape=s64[]\n",
      "  %53 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %54 = s64[] aten::mul(%53, %52), xla_shape=s64[]\n",
      "  %55 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %56 = s64[] aten::add(%55, %54), xla_shape=s64[]\n",
      "  %57 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %58 = s64[] aten::mul(%57, %56), xla_shape=s64[]\n",
      "  %59 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %60 = s64[] aten::add(%59, %58), xla_shape=s64[]\n",
      "  %61 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %62 = s64[] aten::mul(%61, %60), xla_shape=s64[]\n",
      "  %63 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %64 = s64[] aten::add(%63, %62), xla_shape=s64[]\n",
      "  %65 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %66 = f32[3,3]{1,0} aten::expand(%65), xla_shape=f32[3,3]{1,0}\n",
      "  %67 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %68 = f32[3,3]{1,0} aten::expand(%67), xla_shape=f32[3,3]{1,0}\n",
      "  %69 = f32[3,3]{1,0} aten::normal(%68, %66, %64), xla_shape=f32[3,3]{1,0}\n",
      "  %70 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %71 = f32[3,3]{1,0} aten::expand(%70), xla_shape=f32[3,3]{1,0}\n",
      "  %72 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %73 = f32[3,3]{1,0} aten::expand(%72), xla_shape=f32[3,3]{1,0}\n",
      "  %74 = f32[3,3]{1,0} aten::normal(%73, %71, %60), xla_shape=f32[3,3]{1,0}\n",
      "  %75 = f32[3,3]{1,0} aten::einsum(%74, %69), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Check 4\n",
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "IR {\n",
      "  %0 = s64[] xla::device_data(), xla_shape=s64[]\n",
      "  %1 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %2 = s64[] aten::mul(%1, %0), xla_shape=s64[]\n",
      "  %3 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %4 = s64[] aten::add(%3, %2), xla_shape=s64[]\n",
      "  %5 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %6 = s64[] aten::mul(%5, %4), xla_shape=s64[]\n",
      "  %7 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %8 = s64[] aten::add(%7, %6), xla_shape=s64[]\n",
      "  %9 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %10 = s64[] aten::mul(%9, %8), xla_shape=s64[]\n",
      "  %11 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %12 = s64[] aten::add(%11, %10), xla_shape=s64[]\n",
      "  %13 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %14 = s64[] aten::mul(%13, %12), xla_shape=s64[]\n",
      "  %15 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %16 = s64[] aten::add(%15, %14), xla_shape=s64[]\n",
      "  %17 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %18 = s64[] aten::mul(%17, %16), xla_shape=s64[]\n",
      "  %19 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %20 = s64[] aten::add(%19, %18), xla_shape=s64[]\n",
      "  %21 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %22 = s64[] aten::mul(%21, %20), xla_shape=s64[]\n",
      "  %23 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %24 = s64[] aten::add(%23, %22), xla_shape=s64[]\n",
      "  %25 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %26 = s64[] aten::mul(%25, %24), xla_shape=s64[]\n",
      "  %27 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %28 = s64[] aten::add(%27, %26), xla_shape=s64[]\n",
      "  %29 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %30 = s64[] aten::mul(%29, %28), xla_shape=s64[]\n",
      "  %31 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %32 = s64[] aten::add(%31, %30), xla_shape=s64[]\n",
      "  %33 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %34 = s64[] aten::mul(%33, %32), xla_shape=s64[]\n",
      "  %35 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %36 = s64[] aten::add(%35, %34), xla_shape=s64[]\n",
      "  %37 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %38 = s64[] aten::mul(%37, %36), xla_shape=s64[]\n",
      "  %39 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %40 = s64[] aten::add(%39, %38), xla_shape=s64[]\n",
      "  %41 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %42 = s64[] aten::mul(%41, %40), xla_shape=s64[]\n",
      "  %43 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %44 = s64[] aten::add(%43, %42), xla_shape=s64[]\n",
      "  %45 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %46 = s64[] aten::mul(%45, %44), xla_shape=s64[]\n",
      "  %47 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %48 = s64[] aten::add(%47, %46), xla_shape=s64[]\n",
      "  %49 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %50 = s64[] aten::mul(%49, %48), xla_shape=s64[]\n",
      "  %51 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %52 = s64[] aten::add(%51, %50), xla_shape=s64[]\n",
      "  %53 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %54 = s64[] aten::mul(%53, %52), xla_shape=s64[]\n",
      "  %55 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %56 = s64[] aten::add(%55, %54), xla_shape=s64[]\n",
      "  %57 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %58 = s64[] aten::mul(%57, %56), xla_shape=s64[]\n",
      "  %59 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %60 = s64[] aten::add(%59, %58), xla_shape=s64[]\n",
      "  %61 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %62 = s64[] aten::mul(%61, %60), xla_shape=s64[]\n",
      "  %63 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %64 = s64[] aten::add(%63, %62), xla_shape=s64[]\n",
      "  %65 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %66 = s64[] aten::mul(%65, %64), xla_shape=s64[]\n",
      "  %67 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %68 = s64[] aten::add(%67, %66), xla_shape=s64[]\n",
      "  %69 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %70 = s64[] aten::mul(%69, %68), xla_shape=s64[]\n",
      "  %71 = s64[] prim::Constant(), xla_shape=s64[]\n",
      "  %72 = s64[] aten::add(%71, %70), xla_shape=s64[]\n",
      "  %73 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %74 = f32[3,3]{1,0} aten::expand(%73), xla_shape=f32[3,3]{1,0}\n",
      "  %75 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %76 = f32[3,3]{1,0} aten::expand(%75), xla_shape=f32[3,3]{1,0}\n",
      "  %77 = f32[3,3]{1,0} aten::normal(%76, %74, %72), xla_shape=f32[3,3]{1,0}\n",
      "  %78 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %79 = f32[3,3]{1,0} aten::expand(%78), xla_shape=f32[3,3]{1,0}\n",
      "  %80 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %81 = f32[3,3]{1,0} aten::expand(%80), xla_shape=f32[3,3]{1,0}\n",
      "  %82 = f32[3,3]{1,0} aten::normal(%81, %79, %68), xla_shape=f32[3,3]{1,0}\n",
      "  %83 = f32[3,3]{1,0} aten::einsum(%82, %77), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n",
      "-- Check 5\n",
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "IR {\n",
      "  %0 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %1 = f32[3,3]{1,0} xla::device_data(), xla_shape=f32[3,3]{1,0}\n",
      "  %2 = f32[3,3]{1,0} aten::einsum(%1, %0), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Is there a different if we create in the CPU, and then transfer to XLA?\")\n",
    "print(\"-- Check 1\")\n",
    "x_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "y_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Check 2\")\n",
    "x_1= torch.randn(3, 3, requires_grad=True).to('xla')\n",
    "y_1= torch.randn(3, 3, requires_grad=True).to('xla')\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Check 3\")\n",
    "with torch_xla.runtime.xla_device():\n",
    "  x_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "  y_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Check 4\")\n",
    "with torch_xla.runtime.xla_device():\n",
    "  x_1= torch.randn(3, 3, requires_grad=True).to('xla')\n",
    "  y_1= torch.randn(3, 3, requires_grad=True).to('xla')\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Check 5\")\n",
    "x_1= torch.randn(3, 3, requires_grad=True)\n",
    "y_1= torch.randn(3, 3, requires_grad=True)\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1.to('xla'), y_1.to('xla'))\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "It seems like creating the tensor in the xla_device is causing the failure. If we create it, and then transition to the TPU, it succeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Does creating with gradient enabled make a difference?\n",
      "-- Test1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PyTorch is not linked with support for xla devices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-- Does creating with gradient enabled make a difference?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-- Test1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m x_1\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxla\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m y_1\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_xla\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mxla_device():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PyTorch is not linked with support for xla devices"
     ]
    }
   ],
   "source": [
    "print(\"-- Does creating with gradient enabled make a difference?\")\n",
    "\n",
    "print(\"-- Test1\")\n",
    "x_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "y_1= torch.randn(3, 3, requires_grad=False).to('xla')\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Test2\")\n",
    "x_2= torch.randn(3, 3, requires_grad=True)\n",
    "y_2= torch.randn(3, 3, requires_grad=True)\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_2.to('xla'), y_2.to('xla'))\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Test3\")\n",
    "x_1= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "y_1= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_1, y_1)\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))\n",
    "\n",
    "print(\"-- Test4\")\n",
    "x_2= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "y_2= torch.randn(3, 3, requires_grad=False, device=device)\n",
    "with torch_xla.runtime.xla_device():\n",
    "  out_einsum = torch.einsum('...m,mn->...n', x_2.to('xla'), y_2.to('xla'))\n",
    "print(torch_xla._XLAC._get_xla_tensors_text([out_einsum]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TORCH_SHOW_DISPATCH_TRACE=1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.library import custom_op\n",
    "import time\n",
    "import torch_xla\n",
    "import torch_xla.runtime\n",
    "device = torch_xla.device()\n",
    "%env TORCH_SHOW_DISPATCH_TRACE=1\n",
    "\n",
    "@custom_op(\"xla::custom_linear_forward123\", schema=\"(Tensor input, Tensor weight) -> Tensor\", mutates_args=())\n",
    "def custom_linear_forward123(input: Tensor, weight: Tensor):\n",
    "    return torch.einsum('...n,mn->...m', input, weight)\n",
    "\n",
    "X = torch.zeros(3, 3, requires_grad=False, device='xla')\n",
    "Y = torch.zeros(3, 3, requires_grad=False, device='xla')\n",
    "\n",
    "def test_lowering(func):\n",
    "  time.sleep(1)\n",
    "  out = func(X, Y)\n",
    "  time.sleep(1)\n",
    "  ir = torch_xla._XLAC._get_xla_tensors_text([out])\n",
    "  if 'einsum' not in ir:\n",
    "    print(\"!!!!!!!!!!WRONG!!!!!!!!!!! Did not find einsum in lowering\")\n",
    "    print(\"IR:\")\n",
    "    print(ir)\n",
    "  else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@custom_op(\"xla::custom_max_pool\", schema=\"(Tensor input) -> Tensor\", mutates_args=())\n",
    "def custom_max_pool(input: Tensor):\n",
    "    return torch.max_pool2d(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einsum: Call native function\n",
      "Einsum: Call tensor method call\n",
      "Einsum: Build lowered eisum\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "test_lowering(lambda a, b: custom_linear_forward123(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!WRONG!!!!!!!!!!! Did not find einsum in lowering\n",
      "IR:\n",
      "IR {\n",
      "  %0 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %1 = f32[3,3]{1,0} aten::expand(%0), xla_shape=f32[3,3]{1,0}\n",
      "  %2 = f32[3,3,1]{2,1,0} aten::view(%1), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %3 = f32[1,3,3]{0,2,1} aten::permute(%2), xla_shape=f32[1,3,3]{0,2,1}\n",
      "  %4 = f32[3,3,1]{2,0,1} aten::permute(%3), xla_shape=f32[3,3,1]{2,0,1}\n",
      "  %5 = f32[1,3,3]{2,1,0} aten::view(%4), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %6 = f32[] prim::Constant(), xla_shape=f32[]\n",
      "  %7 = f32[3,3]{1,0} aten::expand(%6), xla_shape=f32[3,3]{1,0}\n",
      "  %8 = f32[3,3,1]{2,1,0} aten::view(%7), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %9 = f32[3,1,3]{1,2,0} aten::permute(%8), xla_shape=f32[3,1,3]{1,2,0}\n",
      "  %10 = f32[3,3,1]{2,1,0} aten::permute(%9), xla_shape=f32[3,3,1]{2,1,0}\n",
      "  %11 = f32[1,3,3]{2,1,0} aten::view(%10), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %12 = f32[1,3,3]{2,1,0} aten::matmul(%11, %5), xla_shape=f32[1,3,3]{2,1,0}\n",
      "  %13 = f32[3,1,3]{2,1,0} aten::view(%12), xla_shape=f32[3,1,3]{2,1,0}\n",
      "  %14 = f32[3,3,1]{1,2,0} aten::permute(%13), xla_shape=f32[3,3,1]{1,2,0}\n",
      "  %15 = f32[3,3]{1,0} aten::view(%14), xla_shape=f32[3,3]{1,0}, ROOT=0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lowering(lambda a, b: torch.einsum('...n,mn->...m', a, b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
