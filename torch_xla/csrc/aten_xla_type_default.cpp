// Autogenerated file by gen_backend_stubs.py. Do not edit directly!
#include <torch_xla/csrc/aten_xla_type_default.h>

#include <ATen/Context.h>
#include <torch/library.h>
#include <ATen/CPUGeneratorImpl.h>

#include <tensorflow/compiler/xla/xla_client/debug_macros.h>
#include <tensorflow/compiler/xla/xla_client/metrics.h>
#include <tensorflow/compiler/xla/xla_client/tf_logging.h>
#include <torch_xla/csrc/aten_xla_bridge.h>
#include <torch_xla/csrc/XLANativeFunctions.h>
#include <torch_xla/csrc/function_call_tracker.h>

namespace torch_xla {

// convenience helpers for extracting out an optional c10::Device

c10::optional<c10::Device> get_device_arg(at::Tensor tensor) {
    return tensor.device();
}

c10::optional<c10::Device> get_device_arg(c10::optional<at::Tensor> tensor) {
    return tensor ? c10::optional<c10::Device>((*tensor).device()) : c10::nullopt;
}

c10::optional<c10::Device> get_device_arg(std::vector<at::Tensor> tensors) {
    return tensors.size() > 0 ? c10::optional<c10::Device>(tensors[0].device()) : c10::nullopt;
}

c10::optional<c10::Device> get_device_arg(at::TensorList tensors) {
    return tensors.size() > 0 ? c10::optional<c10::Device>(tensors[0].device()) : c10::nullopt;
}

c10::optional<c10::Device> get_device_arg(c10::optional<c10::Device> device) {
    return device;
}

c10::optional<c10::Device> get_device_arg(c10::Device device) {
    return c10::optional<c10::Device>(device);
}

// convenience helpers for converting tensors to an optional device

at::Tensor to_device_opt(const at::Tensor tensor, c10::optional<c10::Device> device) {
    return device ? tensor.to(*device) : tensor;
}

std::vector<at::Tensor> to_device_opt(const std::vector<at::Tensor>& tensors, c10::optional<c10::Device> device) {
    std::vector<at::Tensor> output_tensors;
    for (const auto& t : tensors) {
        output_tensors.push_back(to_device_opt(t, device));
    }
    return output_tensors;
}

// convenience helper for converting tensors to cpu

std::vector<at::Tensor> to_cpu(const at::TensorList& tensors) {
    // We can't just call at::to_cpu() on the entire list of Tensors
    // Because it will break on undefined tensors. Separate out undefined tensors first.
    std::vector<at::Tensor> cpu_tensors(tensors.size());
    std::vector<at::Tensor> valid_tensors;
    std::vector<bool> to_translate(tensors.size());
    for (size_t i = 0; i < tensors.size(); ++i) {
        const at::Tensor& tensor = tensors[i];
        if (tensor.defined()) {
            to_translate[i] = true;
            valid_tensors.push_back(tensor);
        } else {
            cpu_tensors[i] = tensor;
        }
    }
    auto cpu_valid_tensors = at::_to_cpu(valid_tensors);
    for (size_t i = 0, defined_pos = 0; i < tensors.size(); ++i) {
        if (to_translate[i]) {
            cpu_tensors[i] = std::move(cpu_valid_tensors[defined_pos++]);
        }
    }
  return cpu_tensors;
}

std::vector<c10::optional<at::Tensor>> to_cpu(const std::vector<c10::optional<at::Tensor>>& tensors) {
    std::vector<c10::optional<at::Tensor>> opt_tensors(tensors.size());
    std::vector<at::Tensor> materialized_tensors;
    std::vector<bool> to_translate(tensors.size());
    for (size_t i = 0; i < tensors.size(); ++i) {
        auto tensor = tensors[i];
        if (tensor.has_value()) {
            to_translate[i] = true;
            materialized_tensors.push_back(*tensor);
        }
    }
    auto aten_materialized_tensors = to_cpu(materialized_tensors);
    for (size_t i = 0, defined_pos = 0; i < tensors.size(); ++i) {
        if (to_translate[i]) {
          opt_tensors[i] =
          std::move(aten_materialized_tensors[defined_pos++]);
        }
    }
    return opt_tensors;
}

void AtenXlaTypeDefault::_assert_async(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_assert_async", 1);
  TF_VLOG(3) << "XLA _assert_async :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_assert_async(external_tensors[0]);
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::_fused_dropout(const at::Tensor & self, double p, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_fused_dropout", 1);
  TF_VLOG(3) << "XLA _fused_dropout :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_fused_dropout(external_tensors[0], p, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::_masked_scale(const at::Tensor & self, const at::Tensor & mask, double scale) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_masked_scale", 1);
  TF_VLOG(3) << "XLA _masked_scale :" << " self=" << self.toString() << " mask=" << mask.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mask};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_masked_scale(external_tensors[0], external_tensors[1], scale);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::abs(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::abs", 1);
  TF_VLOG(3) << "XLA abs :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::abs(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::abs_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::abs_out", 1);
  TF_VLOG(3) << "XLA abs_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::abs_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::angle(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::angle", 1);
  TF_VLOG(3) << "XLA angle :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::angle(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::angle_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::angle_out", 1);
  TF_VLOG(3) << "XLA angle_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::angle_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::view_as_real(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::view_as_real", 1);
  TF_VLOG(3) << "XLA view_as_real :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::view_as_real(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::view_as_complex(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::view_as_complex", 1);
  TF_VLOG(3) << "XLA view_as_complex :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::view_as_complex(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::sgn_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sgn_out", 1);
  TF_VLOG(3) << "XLA sgn_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sgn_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::conj_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::conj_out", 1);
  TF_VLOG(3) << "XLA conj_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::conj_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::acos(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::acos", 1);
  TF_VLOG(3) << "XLA acos :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::acos(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::acos_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::acos_out", 1);
  TF_VLOG(3) << "XLA acos_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::acos_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::add", 1);
  TF_VLOG(3) << "XLA add :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::add(external_tensors[0], external_tensors[1], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::add_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::add_out", 1);
  TF_VLOG(3) << "XLA add_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::add_outf(external_tensors[0], external_tensors[1], alpha, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::_add_relu(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_add_relu", 1);
  TF_VLOG(3) << "XLA _add_relu :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_add_relu(external_tensors[0], external_tensors[1], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::_add_relu_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_add_relu_out", 1);
  TF_VLOG(3) << "XLA _add_relu_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_add_relu_outf(external_tensors[0], external_tensors[1], alpha, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::_add_relu_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_add_relu_", 1);
  TF_VLOG(3) << "XLA _add_relu_ :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_add_relu_(external_tensors[0], external_tensors[1], alpha);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::add(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::add", 1);
  TF_VLOG(3) << "XLA add :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::add(external_tensors[0], other, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::addmv_out(const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::addmv_out", 1);
  TF_VLOG(3) << "XLA addmv_out :" << " self=" << self.toString() << " mat=" << mat.toString() << " vec=" << vec.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mat, vec, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::addmv_outf(external_tensors[0], external_tensors[1], external_tensors[2], beta, alpha, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::all(const at::Tensor & self, int64_t dim, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::all", 1);
  TF_VLOG(3) << "XLA all :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::all(external_tensors[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::all_out(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::all_out", 1);
  TF_VLOG(3) << "XLA all_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::all_outf(external_tensors[0], dim, keepdim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::any(const at::Tensor & self, int64_t dim, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::any", 1);
  TF_VLOG(3) << "XLA any :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::any(external_tensors[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::any_out(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::any_out", 1);
  TF_VLOG(3) << "XLA any_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::any_outf(external_tensors[0], dim, keepdim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::arange_out(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::arange_out", 1);
  TF_VLOG(3) << "XLA arange_out :" << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::arange_outf(start, end, step, external_tensors[0]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::argmax(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::argmax", 1);
  TF_VLOG(3) << "XLA argmax :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::argmax(external_tensors[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::argmax_out(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::argmax_out", 1);
  TF_VLOG(3) << "XLA argmax_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::argmax_outf(external_tensors[0], dim, keepdim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::argmin(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::argmin", 1);
  TF_VLOG(3) << "XLA argmin :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::argmin(external_tensors[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::argmin_out(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::argmin_out", 1);
  TF_VLOG(3) << "XLA argmin_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::argmin_outf(external_tensors[0], dim, keepdim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::acosh(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::acosh", 1);
  TF_VLOG(3) << "XLA acosh :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::acosh(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::acosh_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::acosh_out", 1);
  TF_VLOG(3) << "XLA acosh_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::acosh_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::asinh(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::asinh", 1);
  TF_VLOG(3) << "XLA asinh :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::asinh(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::asinh_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::asinh_out", 1);
  TF_VLOG(3) << "XLA asinh_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::asinh_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::atanh(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::atanh", 1);
  TF_VLOG(3) << "XLA atanh :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::atanh(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::atanh_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::atanh_out", 1);
  TF_VLOG(3) << "XLA atanh_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::atanh_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::as_strided(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::as_strided", 1);
  TF_VLOG(3) << "XLA as_strided :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::as_strided(external_tensors[0], size, stride, storage_offset);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

const at::Tensor & AtenXlaTypeDefault::as_strided_(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::as_strided_", 1);
  TF_VLOG(3) << "XLA as_strided_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::as_strided_(external_tensors[0], size, stride, storage_offset);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::asin(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::asin", 1);
  TF_VLOG(3) << "XLA asin :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::asin(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::asin_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::asin_out", 1);
  TF_VLOG(3) << "XLA asin_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::asin_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::atan(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::atan", 1);
  TF_VLOG(3) << "XLA atan :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::atan(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::atan_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::atan_out", 1);
  TF_VLOG(3) << "XLA atan_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::atan_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::baddbmm(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::baddbmm", 1);
  TF_VLOG(3) << "XLA baddbmm :" << " self=" << self.toString() << " batch1=" << batch1.toString() << " batch2=" << batch2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, batch1, batch2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::baddbmm(external_tensors[0], external_tensors[1], external_tensors[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::baddbmm_out(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::baddbmm_out", 1);
  TF_VLOG(3) << "XLA baddbmm_out :" << " self=" << self.toString() << " batch1=" << batch1.toString() << " batch2=" << batch2.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, batch1, batch2, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::baddbmm_outf(external_tensors[0], external_tensors[1], external_tensors[2], beta, alpha, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::baddbmm_(at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::baddbmm_", 1);
  TF_VLOG(3) << "XLA baddbmm_ :" << " self=" << self.toString() << " batch1=" << batch1.toString() << " batch2=" << batch2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, batch1, batch2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].baddbmm_(external_tensors[1], external_tensors[2], beta, alpha);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::quantized_batch_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & var, double eps, double output_scale, int64_t output_zero_point) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::quantized_batch_norm", 1);
  TF_VLOG(3) << "XLA quantized_batch_norm :" << " input=" << input.toString() << " mean=" << mean.toString() << " var=" << var.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, mean, var};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight, bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::quantized_batch_norm(external_tensors[0], external_tensors_opt[0], external_tensors_opt[1], external_tensors[1], external_tensors[2], eps, output_scale, output_zero_point);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

at::Tensor AtenXlaTypeDefault::bernoulli(const at::Tensor & self, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bernoulli", 1);
  TF_VLOG(3) << "XLA bernoulli :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bernoulli(external_tensors[0], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::bernoulli_out(const at::Tensor & self, c10::optional<at::Generator> generator, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bernoulli_out", 1);
  TF_VLOG(3) << "XLA bernoulli_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bernoulli_outf(external_tensors[0], generator, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::bernoulli_(at::Tensor & self, const at::Tensor & p, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bernoulli_", 1);
  TF_VLOG(3) << "XLA bernoulli_ :" << " self=" << self.toString() << " p=" << p.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, p};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].bernoulli_(external_tensors[1], generator);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::bernoulli_(at::Tensor & self, double p, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bernoulli_", 1);
  TF_VLOG(3) << "XLA bernoulli_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].bernoulli_(p, generator);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::binary_cross_entropy(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::binary_cross_entropy", 1);
  TF_VLOG(3) << "XLA binary_cross_entropy :" << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::binary_cross_entropy(external_tensors[0], external_tensors[1], external_tensors_opt[0], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::binary_cross_entropy_out(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::binary_cross_entropy_out", 1);
  TF_VLOG(3) << "XLA binary_cross_entropy_out :" << " self=" << self.toString() << " target=" << target.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::binary_cross_entropy_outf(external_tensors[0], external_tensors[1], external_tensors_opt[0], reduction, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::binary_cross_entropy_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::binary_cross_entropy_backward", 1);
  TF_VLOG(3) << "XLA binary_cross_entropy_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::binary_cross_entropy_backward(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors_opt[0], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::binary_cross_entropy_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::binary_cross_entropy_backward_out", 1);
  TF_VLOG(3) << "XLA binary_cross_entropy_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::binary_cross_entropy_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors_opt[0], reduction, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::binary_cross_entropy_with_logits(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & pos_weight, int64_t reduction) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::binary_cross_entropy_with_logits", 1);
  TF_VLOG(3) << "XLA binary_cross_entropy_with_logits :" << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight, pos_weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::binary_cross_entropy_with_logits(external_tensors[0], external_tensors[1], external_tensors_opt[0], external_tensors_opt[1], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::bincount(const at::Tensor & self, const c10::optional<at::Tensor> & weights, int64_t minlength) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bincount", 1);
  TF_VLOG(3) << "XLA bincount :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weights};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::bincount(external_tensors[0], external_tensors_opt[0], minlength);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::bitwise_not_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bitwise_not_out", 1);
  TF_VLOG(3) << "XLA bitwise_not_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bitwise_not_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::copysign_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::copysign_out", 1);
  TF_VLOG(3) << "XLA copysign_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::copysign_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::logical_not_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logical_not_out", 1);
  TF_VLOG(3) << "XLA logical_not_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logical_not_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::logical_xor_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logical_xor_out", 1);
  TF_VLOG(3) << "XLA logical_xor_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logical_xor_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::logical_and_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logical_and_out", 1);
  TF_VLOG(3) << "XLA logical_and_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logical_and_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::logical_or_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logical_or_out", 1);
  TF_VLOG(3) << "XLA logical_or_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logical_or_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::bmm(const at::Tensor & self, const at::Tensor & mat2) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bmm", 1);
  TF_VLOG(3) << "XLA bmm :" << " self=" << self.toString() << " mat2=" << mat2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mat2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bmm(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::bmm_out(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bmm_out", 1);
  TF_VLOG(3) << "XLA bmm_out :" << " self=" << self.toString() << " mat2=" << mat2.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mat2, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bmm_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::_bmm(const at::Tensor & self, const at::Tensor & mat2, bool deterministic) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_bmm", 1);
  TF_VLOG(3) << "XLA _bmm :" << " self=" << self.toString() << " mat2=" << mat2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mat2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_bmm(external_tensors[0], external_tensors[1], deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::_bmm_out(const at::Tensor & self, const at::Tensor & mat2, bool deterministic, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_bmm_out", 1);
  TF_VLOG(3) << "XLA _bmm_out :" << " self=" << self.toString() << " mat2=" << mat2.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mat2, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_bmm_outf(external_tensors[0], external_tensors[1], deterministic, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::cat(at::TensorList tensors, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cat", 1);
  TF_VLOG(3) << "XLA cat :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cat(l_tensors, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

at::Tensor AtenXlaTypeDefault::ceil(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ceil", 1);
  TF_VLOG(3) << "XLA ceil :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ceil(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::ceil_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ceil_out", 1);
  TF_VLOG(3) << "XLA ceil_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ceil_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::clamp(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::clamp", 1);
  TF_VLOG(3) << "XLA clamp :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::clamp(external_tensors[0], min, max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::clamp_out(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::clamp_out", 1);
  TF_VLOG(3) << "XLA clamp_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::clamp_outf(external_tensors[0], min, max, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::clamp(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::clamp", 1);
  TF_VLOG(3) << "XLA clamp :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {min, max};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::clamp(external_tensors[0], external_tensors_opt[0], external_tensors_opt[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::clamp_out(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::clamp_out", 1);
  TF_VLOG(3) << "XLA clamp_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {min, max};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::clamp_outf(external_tensors[0], external_tensors_opt[0], external_tensors_opt[1], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::clamp_max(const at::Tensor & self, const at::Scalar & max) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::clamp_max", 1);
  TF_VLOG(3) << "XLA clamp_max :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::clamp_max(external_tensors[0], max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::clamp_max_out(const at::Tensor & self, const at::Scalar & max, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::clamp_max_out", 1);
  TF_VLOG(3) << "XLA clamp_max_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::clamp_max_outf(external_tensors[0], max, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::clamp_max_out(const at::Tensor & self, const at::Tensor & max, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::clamp_max_out", 1);
  TF_VLOG(3) << "XLA clamp_max_out :" << " self=" << self.toString() << " max=" << max.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, max, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::clamp_max_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::clamp_min(const at::Tensor & self, const at::Scalar & min) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::clamp_min", 1);
  TF_VLOG(3) << "XLA clamp_min :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::clamp_min(external_tensors[0], min);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::clamp_min_out(const at::Tensor & self, const at::Scalar & min, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::clamp_min_out", 1);
  TF_VLOG(3) << "XLA clamp_min_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::clamp_min_outf(external_tensors[0], min, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::clamp_min_out(const at::Tensor & self, const at::Tensor & min, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::clamp_min_out", 1);
  TF_VLOG(3) << "XLA clamp_min_out :" << " self=" << self.toString() << " min=" << min.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, min, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::clamp_min_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::complex_out(const at::Tensor & real, const at::Tensor & imag, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::complex_out", 1);
  TF_VLOG(3) << "XLA complex_out :" << " real=" << real.toString() << " imag=" << imag.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {real, imag, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::complex_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::polar_out(const at::Tensor & abs, const at::Tensor & angle, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::polar_out", 1);
  TF_VLOG(3) << "XLA polar_out :" << " abs=" << abs.toString() << " angle=" << angle.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {abs, angle, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::polar_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::constant_pad_nd(const at::Tensor & self, at::IntArrayRef pad, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::constant_pad_nd", 1);
  TF_VLOG(3) << "XLA constant_pad_nd :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::constant_pad_nd(external_tensors[0], pad, value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::convolution_overrideable(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::convolution_overrideable", 1);
  TF_VLOG(3) << "XLA convolution_overrideable :" << " input=" << input.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::convolution_overrideable(external_tensors[0], external_tensors[1], external_tensors_opt[0], stride, padding, dilation, transposed, output_padding, groups);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::convolution_backward_overrideable(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::convolution_backward_overrideable", 1);
  TF_VLOG(3) << "XLA convolution_backward_overrideable :" << " grad_output=" << grad_output.toString() << " input=" << input.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, input, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::convolution_backward_overrideable(external_tensors[0], external_tensors[1], external_tensors[2], stride, padding, dilation, transposed, output_padding, groups, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_output)));
}

at::Tensor AtenXlaTypeDefault::_copy_from(const at::Tensor & self, const at::Tensor & dst, bool non_blocking) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_copy_from", 1);
  TF_VLOG(3) << "XLA _copy_from :" << " self=" << self.toString() << " dst=" << dst.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, dst};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_copy_from(external_tensors[0], external_tensors[1], non_blocking);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_copy_from_and_resize(const at::Tensor & self, const at::Tensor & dst) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_copy_from_and_resize", 1);
  TF_VLOG(3) << "XLA _copy_from_and_resize :" << " self=" << self.toString() << " dst=" << dst.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, dst};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_copy_from_and_resize(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::cos(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cos", 1);
  TF_VLOG(3) << "XLA cos :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cos(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::cos_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cos_out", 1);
  TF_VLOG(3) << "XLA cos_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cos_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::cosh(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cosh", 1);
  TF_VLOG(3) << "XLA cosh :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cosh(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::cosh_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cosh_out", 1);
  TF_VLOG(3) << "XLA cosh_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cosh_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::count_nonzero(const at::Tensor & self, at::IntArrayRef dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::count_nonzero", 1);
  TF_VLOG(3) << "XLA count_nonzero :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::count_nonzero(external_tensors[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

void AtenXlaTypeDefault::_cummax_helper(const at::Tensor & self, at::Tensor & values, at::Tensor & indices, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_cummax_helper", 1);
  TF_VLOG(3) << "XLA _cummax_helper :" << " self=" << self.toString() << " values=" << values.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, values, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_cummax_helper(external_tensors[0], external_tensors[1], external_tensors[2], dim);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

}

void AtenXlaTypeDefault::_cummin_helper(const at::Tensor & self, at::Tensor & values, at::Tensor & indices, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_cummin_helper", 1);
  TF_VLOG(3) << "XLA _cummin_helper :" << " self=" << self.toString() << " values=" << values.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, values, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_cummin_helper(external_tensors[0], external_tensors[1], external_tensors[2], dim);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

}

at::Tensor AtenXlaTypeDefault::cumprod(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cumprod", 1);
  TF_VLOG(3) << "XLA cumprod :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cumprod(external_tensors[0], dim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::cumsum(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cumsum", 1);
  TF_VLOG(3) << "XLA cumsum :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cumsum(external_tensors[0], dim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::_ctc_loss(const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, bool zero_infinity) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_ctc_loss", 1);
  TF_VLOG(3) << "XLA _ctc_loss :" << " log_probs=" << log_probs.toString() << " targets=" << targets.toString();
  std::vector<at::Tensor> external_tensors_tensors = {log_probs, targets};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_ctc_loss(external_tensors[0], external_tensors[1], input_lengths, target_lengths, blank, zero_infinity);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(log_probs)), to_device_opt(std::get<1>(x_result), get_device_arg(log_probs)));
}

at::Tensor AtenXlaTypeDefault::_ctc_loss_backward(const at::Tensor & grad, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, const at::Tensor & neg_log_likelihood, const at::Tensor & log_alpha, int64_t blank, bool zero_infinity) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_ctc_loss_backward", 1);
  TF_VLOG(3) << "XLA _ctc_loss_backward :" << " grad=" << grad.toString() << " log_probs=" << log_probs.toString() << " targets=" << targets.toString() << " neg_log_likelihood=" << neg_log_likelihood.toString() << " log_alpha=" << log_alpha.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad, log_probs, targets, neg_log_likelihood, log_alpha};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_ctc_loss_backward(external_tensors[0], external_tensors[1], external_tensors[2], input_lengths, target_lengths, external_tensors[3], external_tensors[4], blank, zero_infinity);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad));
}

at::Tensor AtenXlaTypeDefault::diagonal(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::diagonal", 1);
  TF_VLOG(3) << "XLA diagonal :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::diagonal(external_tensors[0], offset, dim1, dim2);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::div(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::div", 1);
  TF_VLOG(3) << "XLA div :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::div(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::div_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::div_out", 1);
  TF_VLOG(3) << "XLA div_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::div_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::div(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::div", 1);
  TF_VLOG(3) << "XLA div :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::div(external_tensors[0], external_tensors[1], rounding_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::div_out(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::div_out", 1);
  TF_VLOG(3) << "XLA div_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::div_outf(external_tensors[0], external_tensors[1], rounding_mode, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::div(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::div", 1);
  TF_VLOG(3) << "XLA div :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::div(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::dot(const at::Tensor & self, const at::Tensor & tensor) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::dot", 1);
  TF_VLOG(3) << "XLA dot :" << " self=" << self.toString() << " tensor=" << tensor.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, tensor};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::dot(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::vdot(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::vdot", 1);
  TF_VLOG(3) << "XLA vdot :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::vdot(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::embedding(const at::Tensor & weight, const at::Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::embedding", 1);
  TF_VLOG(3) << "XLA embedding :" << " weight=" << weight.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {weight, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::embedding(external_tensors[0], external_tensors[1], padding_idx, scale_grad_by_freq, sparse);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(weight));
}

at::Tensor AtenXlaTypeDefault::embedding_dense_backward(const at::Tensor & grad_output, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::embedding_dense_backward", 1);
  TF_VLOG(3) << "XLA embedding_dense_backward :" << " grad_output=" << grad_output.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::embedding_dense_backward(external_tensors[0], external_tensors[1], num_weights, padding_idx, scale_grad_by_freq);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::embedding_renorm_(at::Tensor & self, const at::Tensor & indices, double max_norm, double norm_type) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::embedding_renorm_", 1);
  TF_VLOG(3) << "XLA embedding_renorm_ :" << " self=" << self.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::embedding_renorm_(external_tensors[0], external_tensors[1], max_norm, norm_type);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::_embedding_bag_forward_only(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_embedding_bag_forward_only", 1);
  TF_VLOG(3) << "XLA _embedding_bag_forward_only :" << " weight=" << weight.toString() << " indices=" << indices.toString() << " offsets=" << offsets.toString();
  std::vector<at::Tensor> external_tensors_tensors = {weight, indices, offsets};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {per_sample_weights};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::_embedding_bag_forward_only(external_tensors[0], external_tensors[1], external_tensors[2], scale_grad_by_freq, mode, sparse, external_tensors_opt[0], include_last_offset, padding_idx);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(weight)), to_device_opt(std::get<1>(x_result), get_device_arg(weight)), to_device_opt(std::get<2>(x_result), get_device_arg(weight)), to_device_opt(std::get<3>(x_result), get_device_arg(weight)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::_embedding_bag(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_embedding_bag", 1);
  TF_VLOG(3) << "XLA _embedding_bag :" << " weight=" << weight.toString() << " indices=" << indices.toString() << " offsets=" << offsets.toString();
  std::vector<at::Tensor> external_tensors_tensors = {weight, indices, offsets};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {per_sample_weights};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::_embedding_bag(external_tensors[0], external_tensors[1], external_tensors[2], scale_grad_by_freq, mode, sparse, external_tensors_opt[0], include_last_offset, padding_idx);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(weight)), to_device_opt(std::get<1>(x_result), get_device_arg(weight)), to_device_opt(std::get<2>(x_result), get_device_arg(weight)), to_device_opt(std::get<3>(x_result), get_device_arg(weight)));
}

at::Tensor AtenXlaTypeDefault::_embedding_bag_dense_backward(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_embedding_bag_dense_backward", 1);
  TF_VLOG(3) << "XLA _embedding_bag_dense_backward :" << " grad=" << grad.toString() << " indices=" << indices.toString() << " offset2bag=" << offset2bag.toString() << " bag_size=" << bag_size.toString() << " maximum_indices=" << maximum_indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad, indices, offset2bag, bag_size, maximum_indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {per_sample_weights};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::_embedding_bag_dense_backward(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors[3], external_tensors[4], num_weights, scale_grad_by_freq, mode, external_tensors_opt[0], padding_idx);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad));
}

at::Tensor AtenXlaTypeDefault::_embedding_bag_per_sample_weights_backward(const at::Tensor & grad, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, int64_t mode, int64_t padding_idx) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_embedding_bag_per_sample_weights_backward", 1);
  TF_VLOG(3) << "XLA _embedding_bag_per_sample_weights_backward :" << " grad=" << grad.toString() << " weight=" << weight.toString() << " indices=" << indices.toString() << " offsets=" << offsets.toString() << " offset2bag=" << offset2bag.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad, weight, indices, offsets, offset2bag};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_embedding_bag_per_sample_weights_backward(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors[3], external_tensors[4], mode, padding_idx);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad));
}

at::Tensor AtenXlaTypeDefault::empty(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::empty", 1);
  TF_VLOG(3) << "XLA empty :";
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::empty(size, dtype, layout, device, pin_memory, memory_format);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(device));
}

at::Tensor AtenXlaTypeDefault::_empty_affine_quantized(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_empty_affine_quantized", 1);
  TF_VLOG(3) << "XLA _empty_affine_quantized :";
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_empty_affine_quantized(size, dtype, layout, device, pin_memory, scale, zero_point, memory_format);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(device));
}

at::Tensor AtenXlaTypeDefault::_empty_per_channel_affine_quantized(at::IntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_empty_per_channel_affine_quantized", 1);
  TF_VLOG(3) << "XLA _empty_per_channel_affine_quantized :" << " scales=" << scales.toString() << " zero_points=" << zero_points.toString();
  std::vector<at::Tensor> external_tensors_tensors = {scales, zero_points};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_empty_per_channel_affine_quantized(size, external_tensors[0], external_tensors[1], axis, dtype, layout, device, pin_memory, memory_format);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(scales));
}

const at::Tensor & AtenXlaTypeDefault::resize_(const at::Tensor & self, at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::resize_", 1);
  TF_VLOG(3) << "XLA resize_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].resize_(size, memory_format);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::empty_quantized(at::IntArrayRef size, const at::Tensor & qtensor) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::empty_quantized", 1);
  TF_VLOG(3) << "XLA empty_quantized :" << " qtensor=" << qtensor.toString();
  std::vector<at::Tensor> external_tensors_tensors = {qtensor};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::empty_quantized(size, external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(qtensor));
}

at::Tensor AtenXlaTypeDefault::empty_strided(at::IntArrayRef size, at::IntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::empty_strided", 1);
  TF_VLOG(3) << "XLA empty_strided :";
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::empty_strided(size, stride, dtype, layout, device, pin_memory);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(device));
}

at::Tensor AtenXlaTypeDefault::erf(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::erf", 1);
  TF_VLOG(3) << "XLA erf :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::erf(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::erf_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::erf_out", 1);
  TF_VLOG(3) << "XLA erf_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::erf_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::erfc(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::erfc", 1);
  TF_VLOG(3) << "XLA erfc :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::erfc(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::erfc_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::erfc_out", 1);
  TF_VLOG(3) << "XLA erfc_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::erfc_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::exp(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::exp", 1);
  TF_VLOG(3) << "XLA exp :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::exp(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::exp_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::exp_out", 1);
  TF_VLOG(3) << "XLA exp_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::exp_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::exp2_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::exp2_out", 1);
  TF_VLOG(3) << "XLA exp2_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::exp2_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::expm1(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::expm1", 1);
  TF_VLOG(3) << "XLA expm1 :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::expm1(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::expm1_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::expm1_out", 1);
  TF_VLOG(3) << "XLA expm1_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::expm1_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::expand(const at::Tensor & self, at::IntArrayRef size, bool implicit) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::expand", 1);
  TF_VLOG(3) << "XLA expand :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].expand(size, implicit);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::eye_out(int64_t n, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::eye_out", 1);
  TF_VLOG(3) << "XLA eye_out :" << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::eye_outf(n, external_tensors[0]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::eye_out(int64_t n, int64_t m, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::eye_out", 1);
  TF_VLOG(3) << "XLA eye_out :" << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::eye_outf(n, m, external_tensors[0]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::fill_(at::Tensor & self, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fill_", 1);
  TF_VLOG(3) << "XLA fill_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fill_(external_tensors[0], value);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::fill_(at::Tensor & self, const at::Tensor & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fill_", 1);
  TF_VLOG(3) << "XLA fill_ :" << " self=" << self.toString() << " value=" << value.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, value};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fill_(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::floor(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::floor", 1);
  TF_VLOG(3) << "XLA floor :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::floor(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::floor_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::floor_out", 1);
  TF_VLOG(3) << "XLA floor_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::floor_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::floor_divide(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::floor_divide", 1);
  TF_VLOG(3) << "XLA floor_divide :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::floor_divide(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::floor_divide_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::floor_divide_out", 1);
  TF_VLOG(3) << "XLA floor_divide_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::floor_divide_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::floor_divide_(at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::floor_divide_", 1);
  TF_VLOG(3) << "XLA floor_divide_ :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].floor_divide_(external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::frac(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::frac", 1);
  TF_VLOG(3) << "XLA frac :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::frac(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::frac_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::frac_out", 1);
  TF_VLOG(3) << "XLA frac_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::frac_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::from_file(c10::string_view filename, c10::optional<bool> shared, c10::optional<int64_t> size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::from_file", 1);
  TF_VLOG(3) << "XLA from_file :";
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::from_file(filename, shared, size, dtype, layout, device, pin_memory);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(device));
}

at::Tensor & AtenXlaTypeDefault::gcd_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::gcd_out", 1);
  TF_VLOG(3) << "XLA gcd_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::gcd_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::lcm_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lcm_out", 1);
  TF_VLOG(3) << "XLA lcm_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lcm_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::grid_sampler_2d(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::grid_sampler_2d", 1);
  TF_VLOG(3) << "XLA grid_sampler_2d :" << " input=" << input.toString() << " grid=" << grid.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, grid};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::grid_sampler_2d(external_tensors[0], external_tensors[1], interpolation_mode, padding_mode, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::grid_sampler_2d_backward(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::grid_sampler_2d_backward", 1);
  TF_VLOG(3) << "XLA grid_sampler_2d_backward :" << " grad_output=" << grad_output.toString() << " input=" << input.toString() << " grid=" << grid.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, input, grid};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::grid_sampler_2d_backward(external_tensors[0], external_tensors[1], external_tensors[2], interpolation_mode, padding_mode, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)));
}

at::Tensor AtenXlaTypeDefault::grid_sampler_3d(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::grid_sampler_3d", 1);
  TF_VLOG(3) << "XLA grid_sampler_3d :" << " input=" << input.toString() << " grid=" << grid.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, grid};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::grid_sampler_3d(external_tensors[0], external_tensors[1], interpolation_mode, padding_mode, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::grid_sampler_3d_backward(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::grid_sampler_3d_backward", 1);
  TF_VLOG(3) << "XLA grid_sampler_3d_backward :" << " grad_output=" << grad_output.toString() << " input=" << input.toString() << " grid=" << grid.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, input, grid};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::grid_sampler_3d_backward(external_tensors[0], external_tensors[1], external_tensors[2], interpolation_mode, padding_mode, align_corners);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::native_group_norm_backward(const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, int64_t N, int64_t C, int64_t HxW, int64_t group, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::native_group_norm_backward", 1);
  TF_VLOG(3) << "XLA native_group_norm_backward :" << " grad_out=" << grad_out.toString() << " input=" << input.toString() << " mean=" << mean.toString() << " rstd=" << rstd.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_out, input, mean, rstd};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::native_group_norm_backward(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors[3], external_tensors_opt[0], N, C, HxW, group, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_out)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_out)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_out)));
}

at::Tensor AtenXlaTypeDefault::_fft_r2c(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_fft_r2c", 1);
  TF_VLOG(3) << "XLA _fft_r2c :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_fft_r2c(external_tensors[0], dim, normalization, onesided);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::_fft_r2c_out(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_fft_r2c_out", 1);
  TF_VLOG(3) << "XLA _fft_r2c_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_fft_r2c_outf(external_tensors[0], dim, normalization, onesided, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::_fft_c2r(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, int64_t last_dim_size) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_fft_c2r", 1);
  TF_VLOG(3) << "XLA _fft_c2r :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_fft_c2r(external_tensors[0], dim, normalization, last_dim_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::_fft_c2r_out(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, int64_t last_dim_size, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_fft_c2r_out", 1);
  TF_VLOG(3) << "XLA _fft_c2r_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_fft_c2r_outf(external_tensors[0], dim, normalization, last_dim_size, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::_fft_c2c(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool forward) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_fft_c2c", 1);
  TF_VLOG(3) << "XLA _fft_c2c :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_fft_c2c(external_tensors[0], dim, normalization, forward);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::_fft_c2c_out(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool forward, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_fft_c2c_out", 1);
  TF_VLOG(3) << "XLA _fft_c2c_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_fft_c2c_outf(external_tensors[0], dim, normalization, forward, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::index(const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::index", 1);
  TF_VLOG(3) << "XLA index :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::index(external_tensors[0], indices);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::index_copy_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::index_copy_", 1);
  TF_VLOG(3) << "XLA index_copy_ :" << " self=" << self.toString() << " index=" << index.toString() << " source=" << source.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index, source};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].index_copy_(dim, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::index_put_(at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::index_put_", 1);
  TF_VLOG(3) << "XLA index_put_ :" << " self=" << self.toString() << " values=" << values.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, values};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::index_put_(external_tensors[0], indices, external_tensors[1], accumulate);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::_index_put_impl_(at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate, bool unsafe) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_index_put_impl_", 1);
  TF_VLOG(3) << "XLA _index_put_impl_ :" << " self=" << self.toString() << " values=" << values.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, values};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_index_put_impl_(external_tensors[0], indices, external_tensors[1], accumulate, unsafe);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::inverse(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::inverse", 1);
  TF_VLOG(3) << "XLA inverse :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::inverse(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_inverse_helper(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_inverse_helper", 1);
  TF_VLOG(3) << "XLA _inverse_helper :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_inverse_helper(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::isnan(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::isnan", 1);
  TF_VLOG(3) << "XLA isnan :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::isnan(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::kl_div(const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::kl_div", 1);
  TF_VLOG(3) << "XLA kl_div :" << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::kl_div(external_tensors[0], external_tensors[1], reduction, log_target);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::kl_div_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::kl_div_backward", 1);
  TF_VLOG(3) << "XLA kl_div_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::kl_div_backward(external_tensors[0], external_tensors[1], external_tensors[2], reduction, log_target);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::kthvalue(const at::Tensor & self, int64_t k, int64_t dim, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::kthvalue", 1);
  TF_VLOG(3) << "XLA kthvalue :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::kthvalue(external_tensors[0], k, dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::kthvalue_out(const at::Tensor & self, int64_t k, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::kthvalue_out", 1);
  TF_VLOG(3) << "XLA kthvalue_out :" << " self=" << self.toString() << " values=" << values.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, values, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::kthvalue_outf(external_tensors[0], k, dim, keepdim, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::native_layer_norm_backward(const at::Tensor & grad_out, const at::Tensor & input, at::IntArrayRef normalized_shape, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::native_layer_norm_backward", 1);
  TF_VLOG(3) << "XLA native_layer_norm_backward :" << " grad_out=" << grad_out.toString() << " input=" << input.toString() << " mean=" << mean.toString() << " rstd=" << rstd.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_out, input, mean, rstd};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight, bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::native_layer_norm_backward(external_tensors[0], external_tensors[1], normalized_shape, external_tensors[2], external_tensors[3], external_tensors_opt[0], external_tensors_opt[1], output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_out)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_out)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_out)));
}

at::Tensor & AtenXlaTypeDefault::nan_to_num_out(const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nan_to_num_out", 1);
  TF_VLOG(3) << "XLA nan_to_num_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::nan_to_num_outf(external_tensors[0], nan, posinf, neginf, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::mkldnn_linear(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mkldnn_linear", 1);
  TF_VLOG(3) << "XLA mkldnn_linear :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::mkldnn_linear(external_tensors[0], external_tensors[1], external_tensors_opt[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::mkldnn_linear_backward_input(at::IntArrayRef input_size, const at::Tensor & grad_output, const at::Tensor & weight) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mkldnn_linear_backward_input", 1);
  TF_VLOG(3) << "XLA mkldnn_linear_backward_input :" << " grad_output=" << grad_output.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mkldnn_linear_backward_input(input_size, external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::mkldnn_linear_backward_weights(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, bool bias_defined) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mkldnn_linear_backward_weights", 1);
  TF_VLOG(3) << "XLA mkldnn_linear_backward_weights :" << " grad_output=" << grad_output.toString() << " input=" << input.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, input, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mkldnn_linear_backward_weights(external_tensors[0], external_tensors[1], external_tensors[2], bias_defined);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::mkldnn_linear_backward(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mkldnn_linear_backward", 1);
  TF_VLOG(3) << "XLA mkldnn_linear_backward :" << " self=" << self.toString() << " grad_output=" << grad_output.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, grad_output, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mkldnn_linear_backward(external_tensors[0], external_tensors[1], external_tensors[2], output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

at::Tensor & AtenXlaTypeDefault::linspace_out(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::linspace_out", 1);
  TF_VLOG(3) << "XLA linspace_out :" << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::linspace_outf(start, end, steps, external_tensors[0]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::log(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log", 1);
  TF_VLOG(3) << "XLA log :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::log(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::log_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log_out", 1);
  TF_VLOG(3) << "XLA log_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::log_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::log10(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log10", 1);
  TF_VLOG(3) << "XLA log10 :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::log10(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::log10_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log10_out", 1);
  TF_VLOG(3) << "XLA log10_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::log10_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::log1p(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log1p", 1);
  TF_VLOG(3) << "XLA log1p :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::log1p(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::log1p_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log1p_out", 1);
  TF_VLOG(3) << "XLA log1p_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::log1p_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::log2(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log2", 1);
  TF_VLOG(3) << "XLA log2 :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::log2(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::log2_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log2_out", 1);
  TF_VLOG(3) << "XLA log2_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::log2_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::logaddexp_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logaddexp_out", 1);
  TF_VLOG(3) << "XLA logaddexp_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logaddexp_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::logaddexp2_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logaddexp2_out", 1);
  TF_VLOG(3) << "XLA logaddexp2_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logaddexp2_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::xlogy(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::xlogy", 1);
  TF_VLOG(3) << "XLA xlogy :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::xlogy(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::xlogy_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::xlogy_out", 1);
  TF_VLOG(3) << "XLA xlogy_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::xlogy_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::xlogy_(at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::xlogy_", 1);
  TF_VLOG(3) << "XLA xlogy_ :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::xlogy_(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::xlogy(const at::Scalar & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::xlogy", 1);
  TF_VLOG(3) << "XLA xlogy :" << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::xlogy(self, external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(other));
}

at::Tensor & AtenXlaTypeDefault::xlogy_out(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::xlogy_out", 1);
  TF_VLOG(3) << "XLA xlogy_out :" << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::xlogy_outf(self, external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::xlogy(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::xlogy", 1);
  TF_VLOG(3) << "XLA xlogy :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::xlogy(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::xlogy_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::xlogy_out", 1);
  TF_VLOG(3) << "XLA xlogy_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::xlogy_outf(external_tensors[0], other, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::xlogy_(at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::xlogy_", 1);
  TF_VLOG(3) << "XLA xlogy_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::xlogy_(external_tensors[0], other);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::logdet(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logdet", 1);
  TF_VLOG(3) << "XLA logdet :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logdet(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::logspace_out(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, double base, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logspace_out", 1);
  TF_VLOG(3) << "XLA logspace_out :" << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logspace_outf(start, end, steps, base, external_tensors[0]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::_log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_log_softmax", 1);
  TF_VLOG(3) << "XLA _log_softmax :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_log_softmax(external_tensors[0], dim, half_to_float);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_log_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_log_softmax_backward_data", 1);
  TF_VLOG(3) << "XLA _log_softmax_backward_data :" << " grad_output=" << grad_output.toString() << " output=" << output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_log_softmax_backward_data(external_tensors[0], external_tensors[1], dim, external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::_logcumsumexp(const at::Tensor & self, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_logcumsumexp", 1);
  TF_VLOG(3) << "XLA _logcumsumexp :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_logcumsumexp(external_tensors[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::_logcumsumexp_out(const at::Tensor & self, int64_t dim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_logcumsumexp_out", 1);
  TF_VLOG(3) << "XLA _logcumsumexp_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_logcumsumexp_outf(external_tensors[0], dim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::logsumexp(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logsumexp", 1);
  TF_VLOG(3) << "XLA logsumexp :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logsumexp(external_tensors[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::matrix_exp(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::matrix_exp", 1);
  TF_VLOG(3) << "XLA matrix_exp :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::matrix_exp(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::_aminmax(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_aminmax", 1);
  TF_VLOG(3) << "XLA _aminmax :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_aminmax(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::_aminmax(const at::Tensor & self, int64_t dim, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_aminmax", 1);
  TF_VLOG(3) << "XLA _aminmax :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_aminmax(external_tensors[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::_compute_linear_combination(const at::Tensor & input, const at::Tensor & coefficients) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_compute_linear_combination", 1);
  TF_VLOG(3) << "XLA _compute_linear_combination :" << " input=" << input.toString() << " coefficients=" << coefficients.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, coefficients};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_compute_linear_combination(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

at::Tensor & AtenXlaTypeDefault::_compute_linear_combination_out(const at::Tensor & input, const at::Tensor & coefficients, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_compute_linear_combination_out", 1);
  TF_VLOG(3) << "XLA _compute_linear_combination_out :" << " input=" << input.toString() << " coefficients=" << coefficients.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, coefficients, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_compute_linear_combination_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::max(const at::Tensor & self, int64_t dim, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max", 1);
  TF_VLOG(3) << "XLA max :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max(external_tensors[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::max_out(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_out", 1);
  TF_VLOG(3) << "XLA max_out :" << " self=" << self.toString() << " max=" << max.toString() << " max_values=" << max_values.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, max, max_values};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_outf(external_tensors[0], dim, keepdim, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(max, max_values);
}

at::Tensor & AtenXlaTypeDefault::amax_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::amax_out", 1);
  TF_VLOG(3) << "XLA amax_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::amax_outf(external_tensors[0], dim, keepdim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::mkldnn_max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mkldnn_max_pool2d", 1);
  TF_VLOG(3) << "XLA mkldnn_max_pool2d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mkldnn_max_pool2d(external_tensors[0], kernel_size, stride, padding, dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::mkldnn_max_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mkldnn_max_pool2d_backward", 1);
  TF_VLOG(3) << "XLA mkldnn_max_pool2d_backward :" << " grad_output=" << grad_output.toString() << " output=" << output.toString() << " input=" << input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, output, input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mkldnn_max_pool2d_backward(external_tensors[0], external_tensors[1], external_tensors[2], kernel_size, stride, padding, dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::mkldnn_max_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mkldnn_max_pool3d", 1);
  TF_VLOG(3) << "XLA mkldnn_max_pool3d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mkldnn_max_pool3d(external_tensors[0], kernel_size, stride, padding, dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::mkldnn_max_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mkldnn_max_pool3d_backward", 1);
  TF_VLOG(3) << "XLA mkldnn_max_pool3d_backward :" << " grad_output=" << grad_output.toString() << " output=" << output.toString() << " input=" << input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, output, input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mkldnn_max_pool3d_backward(external_tensors[0], external_tensors[1], external_tensors[2], kernel_size, stride, padding, dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::quantized_max_pool1d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::quantized_max_pool1d", 1);
  TF_VLOG(3) << "XLA quantized_max_pool1d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::quantized_max_pool1d(external_tensors[0], kernel_size, stride, padding, dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::quantized_max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::quantized_max_pool2d", 1);
  TF_VLOG(3) << "XLA quantized_max_pool2d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::quantized_max_pool2d(external_tensors[0], kernel_size, stride, padding, dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::mean(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mean", 1);
  TF_VLOG(3) << "XLA mean :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mean(external_tensors[0], dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::mean(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mean", 1);
  TF_VLOG(3) << "XLA mean :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mean(external_tensors[0], dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::mean_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mean_out", 1);
  TF_VLOG(3) << "XLA mean_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mean_outf(external_tensors[0], dim, keepdim, dtype, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::median(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::median", 1);
  TF_VLOG(3) << "XLA median :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::median(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::median_out(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::median_out", 1);
  TF_VLOG(3) << "XLA median_out :" << " self=" << self.toString() << " values=" << values.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, values, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::median_outf(external_tensors[0], dim, keepdim, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}

at::Tensor AtenXlaTypeDefault::nanmedian(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nanmedian", 1);
  TF_VLOG(3) << "XLA nanmedian :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::nanmedian(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::nanmedian_out(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nanmedian_out", 1);
  TF_VLOG(3) << "XLA nanmedian_out :" << " self=" << self.toString() << " values=" << values.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, values, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::nanmedian_outf(external_tensors[0], dim, keepdim, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::min(const at::Tensor & self, int64_t dim, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::min", 1);
  TF_VLOG(3) << "XLA min :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::min(external_tensors[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::min_out(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::min_out", 1);
  TF_VLOG(3) << "XLA min_out :" << " self=" << self.toString() << " min=" << min.toString() << " min_indices=" << min_indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, min, min_indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::min_outf(external_tensors[0], dim, keepdim, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(min, min_indices);
}

at::Tensor & AtenXlaTypeDefault::amin_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::amin_out", 1);
  TF_VLOG(3) << "XLA amin_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::amin_outf(external_tensors[0], dim, keepdim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::miopen_batch_norm(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double exponential_average_factor, double epsilon) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_batch_norm", 1);
  TF_VLOG(3) << "XLA miopen_batch_norm :" << " input=" << input.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias, running_mean, running_var};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::miopen_batch_norm(external_tensors[0], external_tensors[1], external_tensors_opt[0], external_tensors_opt[1], external_tensors_opt[2], training, exponential_average_factor, epsilon);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(input)), to_device_opt(std::get<1>(x_result), get_device_arg(input)), to_device_opt(std::get<2>(x_result), get_device_arg(input)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::miopen_batch_norm_backward(const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var, double epsilon) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_batch_norm_backward", 1);
  TF_VLOG(3) << "XLA miopen_batch_norm_backward :" << " input=" << input.toString() << " grad_output=" << grad_output.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, grad_output, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {running_mean, running_var, save_mean, save_var};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::miopen_batch_norm_backward(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors_opt[0], external_tensors_opt[1], external_tensors_opt[2], external_tensors_opt[3], epsilon);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(input)), to_device_opt(std::get<1>(x_result), get_device_arg(input)), to_device_opt(std::get<2>(x_result), get_device_arg(input)));
}

at::Tensor AtenXlaTypeDefault::miopen_convolution(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_convolution", 1);
  TF_VLOG(3) << "XLA miopen_convolution :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::miopen_convolution(external_tensors[0], external_tensors[1], external_tensors_opt[0], padding, stride, dilation, groups, benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::miopen_convolution_backward_input(at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_convolution_backward_input", 1);
  TF_VLOG(3) << "XLA miopen_convolution_backward_input :" << " grad_output=" << grad_output.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::miopen_convolution_backward_input(self_size, external_tensors[0], external_tensors[1], padding, stride, dilation, groups, benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::miopen_convolution_backward(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_convolution_backward", 1);
  TF_VLOG(3) << "XLA miopen_convolution_backward :" << " self=" << self.toString() << " grad_output=" << grad_output.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, grad_output, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::miopen_convolution_backward(external_tensors[0], external_tensors[1], external_tensors[2], padding, stride, dilation, groups, benchmark, deterministic, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::miopen_convolution_backward_bias(const at::Tensor & grad_output) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_convolution_backward_bias", 1);
  TF_VLOG(3) << "XLA miopen_convolution_backward_bias :" << " grad_output=" << grad_output.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::miopen_convolution_backward_bias(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::miopen_convolution_backward_weight(at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_convolution_backward_weight", 1);
  TF_VLOG(3) << "XLA miopen_convolution_backward_weight :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::miopen_convolution_backward_weight(weight_size, external_tensors[0], external_tensors[1], padding, stride, dilation, groups, benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::miopen_convolution_transpose(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_convolution_transpose", 1);
  TF_VLOG(3) << "XLA miopen_convolution_transpose :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::miopen_convolution_transpose(external_tensors[0], external_tensors[1], external_tensors_opt[0], padding, output_padding, stride, dilation, groups, benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::miopen_convolution_transpose_backward(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_convolution_transpose_backward", 1);
  TF_VLOG(3) << "XLA miopen_convolution_transpose_backward :" << " self=" << self.toString() << " grad_output=" << grad_output.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, grad_output, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::miopen_convolution_transpose_backward(external_tensors[0], external_tensors[1], external_tensors[2], padding, output_padding, stride, dilation, groups, benchmark, deterministic, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::miopen_convolution_transpose_backward_input(const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_convolution_transpose_backward_input", 1);
  TF_VLOG(3) << "XLA miopen_convolution_transpose_backward_input :" << " grad_output=" << grad_output.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::miopen_convolution_transpose_backward_input(external_tensors[0], external_tensors[1], padding, stride, dilation, groups, benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::miopen_convolution_transpose_backward_weight(at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_convolution_transpose_backward_weight", 1);
  TF_VLOG(3) << "XLA miopen_convolution_transpose_backward_weight :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::miopen_convolution_transpose_backward_weight(weight_size, external_tensors[0], external_tensors[1], padding, stride, dilation, groups, benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::miopen_depthwise_convolution(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_depthwise_convolution", 1);
  TF_VLOG(3) << "XLA miopen_depthwise_convolution :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::miopen_depthwise_convolution(external_tensors[0], external_tensors[1], external_tensors_opt[0], padding, stride, dilation, groups, benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::miopen_depthwise_convolution_backward_input(at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_depthwise_convolution_backward_input", 1);
  TF_VLOG(3) << "XLA miopen_depthwise_convolution_backward_input :" << " grad_output=" << grad_output.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::miopen_depthwise_convolution_backward_input(self_size, external_tensors[0], external_tensors[1], padding, stride, dilation, groups, benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::miopen_depthwise_convolution_backward(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_depthwise_convolution_backward", 1);
  TF_VLOG(3) << "XLA miopen_depthwise_convolution_backward :" << " self=" << self.toString() << " grad_output=" << grad_output.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, grad_output, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::miopen_depthwise_convolution_backward(external_tensors[0], external_tensors[1], external_tensors[2], padding, stride, dilation, groups, benchmark, deterministic, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::miopen_depthwise_convolution_backward_weight(at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_depthwise_convolution_backward_weight", 1);
  TF_VLOG(3) << "XLA miopen_depthwise_convolution_backward_weight :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::miopen_depthwise_convolution_backward_weight(weight_size, external_tensors[0], external_tensors[1], padding, stride, dilation, groups, benchmark, deterministic);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::miopen_rnn(const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_rnn", 1);
  TF_VLOG(3) << "XLA miopen_rnn :" << " input=" << input.toString() << " hx=" << hx.toString();
  auto l_weight = to_cpu(weight);
  std::vector<at::Tensor> external_tensors_tensors = {input, hx};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {cx, dropout_state};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::miopen_rnn(external_tensors[0], l_weight, weight_stride0, external_tensors[1], external_tensors_opt[0], mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, external_tensors_opt[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(input)), to_device_opt(std::get<1>(x_result), get_device_arg(input)), to_device_opt(std::get<2>(x_result), get_device_arg(input)), to_device_opt(std::get<3>(x_result), get_device_arg(input)), to_device_opt(std::get<4>(x_result), get_device_arg(input)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,std::vector<at::Tensor>> AtenXlaTypeDefault::miopen_rnn_backward(const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, std::array<bool,4> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::miopen_rnn_backward", 1);
  TF_VLOG(3) << "XLA miopen_rnn_backward :" << " input=" << input.toString() << " weight_buf=" << weight_buf.toString() << " hx=" << hx.toString() << " output=" << output.toString() << " reserve=" << reserve.toString();
  auto l_weight = to_cpu(weight);
  std::vector<at::Tensor> external_tensors_tensors = {input, weight_buf, hx, output, reserve};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {cx, grad_output, grad_hy, grad_cy, dropout_state};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::miopen_rnn_backward(external_tensors[0], l_weight, weight_stride0, external_tensors[1], external_tensors[2], external_tensors_opt[0], external_tensors[3], external_tensors_opt[1], external_tensors_opt[2], external_tensors_opt[3], mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, external_tensors_opt[4], external_tensors[4], output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor,std::vector<at::Tensor>>(to_device_opt(std::get<0>(x_result), get_device_arg(input)), to_device_opt(std::get<1>(x_result), get_device_arg(input)), to_device_opt(std::get<2>(x_result), get_device_arg(input)), to_device_opt(std::get<3>(x_result), get_device_arg(input)));
}

at::Tensor AtenXlaTypeDefault::mm(const at::Tensor & self, const at::Tensor & mat2) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mm", 1);
  TF_VLOG(3) << "XLA mm :" << " self=" << self.toString() << " mat2=" << mat2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mat2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mm(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::mm_out(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mm_out", 1);
  TF_VLOG(3) << "XLA mm_out :" << " self=" << self.toString() << " mat2=" << mat2.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mat2, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mm_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::_sparse_sparse_matmul(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_sparse_sparse_matmul", 1);
  TF_VLOG(3) << "XLA _sparse_sparse_matmul :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_sparse_sparse_matmul(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_sparse_mask_helper(const at::Tensor & t, const at::Tensor & mask_indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_sparse_mask_helper", 1);
  TF_VLOG(3) << "XLA _sparse_mask_helper :" << " t=" << t.toString() << " mask_indices=" << mask_indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {t, mask_indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_sparse_mask_helper(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(t));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::mode(const at::Tensor & self, int64_t dim, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mode", 1);
  TF_VLOG(3) << "XLA mode :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mode(external_tensors[0], dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::mul(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mul", 1);
  TF_VLOG(3) << "XLA mul :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mul(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::mul_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mul_out", 1);
  TF_VLOG(3) << "XLA mul_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mul_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::mul(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mul", 1);
  TF_VLOG(3) << "XLA mul :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mul(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::mv(const at::Tensor & self, const at::Tensor & vec) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mv", 1);
  TF_VLOG(3) << "XLA mv :" << " self=" << self.toString() << " vec=" << vec.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, vec};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mv(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::mv_out(const at::Tensor & self, const at::Tensor & vec, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mv_out", 1);
  TF_VLOG(3) << "XLA mv_out :" << " self=" << self.toString() << " vec=" << vec.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, vec, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mv_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::narrow_copy_out(const at::Tensor & self, int64_t dim, int64_t start, int64_t length, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::narrow_copy_out", 1);
  TF_VLOG(3) << "XLA narrow_copy_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::narrow_copy_outf(external_tensors[0], dim, start, length, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::native_batch_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::native_batch_norm", 1);
  TF_VLOG(3) << "XLA native_batch_norm :" << " input=" << input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight, bias, running_mean, running_var};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::native_batch_norm(external_tensors[0], external_tensors_opt[0], external_tensors_opt[1], external_tensors_opt[2], external_tensors_opt[3], training, momentum, eps);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(input)), to_device_opt(std::get<1>(x_result), get_device_arg(input)), to_device_opt(std::get<2>(x_result), get_device_arg(input)));
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> AtenXlaTypeDefault::native_batch_norm_out(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::native_batch_norm_out", 1);
  TF_VLOG(3) << "XLA native_batch_norm_out :" << " input=" << input.toString() << " out=" << out.toString() << " save_mean=" << save_mean.toString() << " save_invstd=" << save_invstd.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, out, save_mean, save_invstd};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight, bias, running_mean, running_var};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::native_batch_norm_outf(external_tensors[0], external_tensors_opt[0], external_tensors_opt[1], external_tensors_opt[2], external_tensors_opt[3], training, momentum, eps, external_tensors[1], external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {1, 2, 3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(out, save_mean, save_invstd);
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::batch_norm_stats(const at::Tensor & input, double eps) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::batch_norm_stats", 1);
  TF_VLOG(3) << "XLA batch_norm_stats :" << " input=" << input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::batch_norm_stats(external_tensors[0], eps);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(input)), to_device_opt(std::get<1>(x_result), get_device_arg(input)));
}

at::Tensor AtenXlaTypeDefault::batch_norm_elemt(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & invstd, double eps) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::batch_norm_elemt", 1);
  TF_VLOG(3) << "XLA batch_norm_elemt :" << " input=" << input.toString() << " mean=" << mean.toString() << " invstd=" << invstd.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, mean, invstd};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight, bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::batch_norm_elemt(external_tensors[0], external_tensors_opt[0], external_tensors_opt[1], external_tensors[1], external_tensors[2], eps);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

at::Tensor & AtenXlaTypeDefault::batch_norm_elemt_out(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & invstd, double eps, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::batch_norm_elemt_out", 1);
  TF_VLOG(3) << "XLA batch_norm_elemt_out :" << " input=" << input.toString() << " mean=" << mean.toString() << " invstd=" << invstd.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, mean, invstd, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight, bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::batch_norm_elemt_outf(external_tensors[0], external_tensors_opt[0], external_tensors_opt[1], external_tensors[1], external_tensors[2], eps, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::batch_norm_gather_stats(const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, int64_t count) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::batch_norm_gather_stats", 1);
  TF_VLOG(3) << "XLA batch_norm_gather_stats :" << " input=" << input.toString() << " mean=" << mean.toString() << " invstd=" << invstd.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, mean, invstd};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {running_mean, running_var};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::batch_norm_gather_stats(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors_opt[0], external_tensors_opt[1], momentum, eps, count);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(input)), to_device_opt(std::get<1>(x_result), get_device_arg(input)));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::batch_norm_gather_stats_with_counts(const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, const at::Tensor & counts) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::batch_norm_gather_stats_with_counts", 1);
  TF_VLOG(3) << "XLA batch_norm_gather_stats_with_counts :" << " input=" << input.toString() << " mean=" << mean.toString() << " invstd=" << invstd.toString() << " counts=" << counts.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, mean, invstd, counts};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {running_mean, running_var};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::batch_norm_gather_stats_with_counts(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors_opt[0], external_tensors_opt[1], momentum, eps, external_tensors[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(input)), to_device_opt(std::get<1>(x_result), get_device_arg(input)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::native_batch_norm_backward(const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::native_batch_norm_backward", 1);
  TF_VLOG(3) << "XLA native_batch_norm_backward :" << " grad_out=" << grad_out.toString() << " input=" << input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_out, input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight, running_mean, running_var, save_mean, save_invstd};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::native_batch_norm_backward(external_tensors[0], external_tensors[1], external_tensors_opt[0], external_tensors_opt[1], external_tensors_opt[2], external_tensors_opt[3], external_tensors_opt[4], train, eps, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_out)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_out)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_out)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::batch_norm_backward_reduce(const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, bool input_g, bool weight_g, bool bias_g) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::batch_norm_backward_reduce", 1);
  TF_VLOG(3) << "XLA batch_norm_backward_reduce :" << " grad_out=" << grad_out.toString() << " input=" << input.toString() << " mean=" << mean.toString() << " invstd=" << invstd.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_out, input, mean, invstd};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::batch_norm_backward_reduce(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors[3], external_tensors_opt[0], input_g, weight_g, bias_g);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_out)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_out)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_out)), to_device_opt(std::get<3>(x_result), get_device_arg(grad_out)));
}

at::Tensor AtenXlaTypeDefault::batch_norm_backward_elemt(const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, const at::Tensor & mean_dy, const at::Tensor & mean_dy_xmu, const at::Tensor & count) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::batch_norm_backward_elemt", 1);
  TF_VLOG(3) << "XLA batch_norm_backward_elemt :" << " grad_out=" << grad_out.toString() << " input=" << input.toString() << " mean=" << mean.toString() << " invstd=" << invstd.toString() << " mean_dy=" << mean_dy.toString() << " mean_dy_xmu=" << mean_dy_xmu.toString() << " count=" << count.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_out, input, mean, invstd, mean_dy, mean_dy_xmu, count};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::batch_norm_backward_elemt(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors[3], external_tensors_opt[0], external_tensors[4], external_tensors[5], external_tensors[6]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_out));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::batch_norm_update_stats(const at::Tensor & input, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::batch_norm_update_stats", 1);
  TF_VLOG(3) << "XLA batch_norm_update_stats :" << " input=" << input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {running_mean, running_var};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::batch_norm_update_stats(external_tensors[0], external_tensors_opt[0], external_tensors_opt[1], momentum);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(input)), to_device_opt(std::get<1>(x_result), get_device_arg(input)));
}

at::Tensor AtenXlaTypeDefault::_cdist_forward(const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_cdist_forward", 1);
  TF_VLOG(3) << "XLA _cdist_forward :" << " x1=" << x1.toString() << " x2=" << x2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {x1, x2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_cdist_forward(external_tensors[0], external_tensors[1], p, compute_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(x1));
}

at::Tensor AtenXlaTypeDefault::_cdist_backward(const at::Tensor & grad, const at::Tensor & x1, const at::Tensor & x2, double p, const at::Tensor & cdist) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_cdist_backward", 1);
  TF_VLOG(3) << "XLA _cdist_backward :" << " grad=" << grad.toString() << " x1=" << x1.toString() << " x2=" << x2.toString() << " cdist=" << cdist.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad, x1, x2, cdist};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_cdist_backward(external_tensors[0], external_tensors[1], external_tensors[2], p, external_tensors[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad));
}

at::Tensor AtenXlaTypeDefault::_pdist_forward(const at::Tensor & self, double p) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_pdist_forward", 1);
  TF_VLOG(3) << "XLA _pdist_forward :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_pdist_forward(external_tensors[0], p);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_pdist_backward(const at::Tensor & grad, const at::Tensor & self, double p, const at::Tensor & pdist) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_pdist_backward", 1);
  TF_VLOG(3) << "XLA _pdist_backward :" << " grad=" << grad.toString() << " self=" << self.toString() << " pdist=" << pdist.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad, self, pdist};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_pdist_backward(external_tensors[0], external_tensors[1], p, external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad));
}

at::Tensor AtenXlaTypeDefault::permute(const at::Tensor & self, at::IntArrayRef dims) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::permute", 1);
  TF_VLOG(3) << "XLA permute :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::permute(external_tensors[0], dims);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::channel_shuffle(const at::Tensor & self, int64_t groups) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::channel_shuffle", 1);
  TF_VLOG(3) << "XLA channel_shuffle :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::channel_shuffle(external_tensors[0], groups);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::randperm_out(int64_t n, c10::optional<at::Generator> generator, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::randperm_out", 1);
  TF_VLOG(3) << "XLA randperm_out :" << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::randperm_outf(n, generator, external_tensors[0]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::range_out(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::range_out", 1);
  TF_VLOG(3) << "XLA range_out :" << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::range_outf(start, end, step, external_tensors[0]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::reciprocal(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::reciprocal", 1);
  TF_VLOG(3) << "XLA reciprocal :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::reciprocal(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::reciprocal_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::reciprocal_out", 1);
  TF_VLOG(3) << "XLA reciprocal_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::reciprocal_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::neg(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::neg", 1);
  TF_VLOG(3) << "XLA neg :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::neg(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::neg_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::neg_out", 1);
  TF_VLOG(3) << "XLA neg_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::neg_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::repeat(const at::Tensor & self, at::IntArrayRef repeats) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::repeat", 1);
  TF_VLOG(3) << "XLA repeat :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].repeat(repeats);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::repeat_interleave(const at::Tensor & repeats, c10::optional<int64_t> output_size) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::repeat_interleave", 1);
  TF_VLOG(3) << "XLA repeat_interleave :" << " repeats=" << repeats.toString();
  std::vector<at::Tensor> external_tensors_tensors = {repeats};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::repeat_interleave(external_tensors[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(repeats));
}

at::Tensor AtenXlaTypeDefault::_mkldnn_reshape(const at::Tensor & self, at::IntArrayRef shape) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_mkldnn_reshape", 1);
  TF_VLOG(3) << "XLA _mkldnn_reshape :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_mkldnn_reshape(external_tensors[0], shape);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::round(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::round", 1);
  TF_VLOG(3) << "XLA round :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::round(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::round_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::round_out", 1);
  TF_VLOG(3) << "XLA round_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::round_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::relu(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::relu", 1);
  TF_VLOG(3) << "XLA relu :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::relu(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::relu_(at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::relu_", 1);
  TF_VLOG(3) << "XLA relu_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::relu_(external_tensors[0]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::prelu(const at::Tensor & self, const at::Tensor & weight) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::prelu", 1);
  TF_VLOG(3) << "XLA prelu :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::prelu(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::prelu_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::prelu_backward", 1);
  TF_VLOG(3) << "XLA prelu_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::prelu_backward(external_tensors[0], external_tensors[1], external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)));
}

at::Tensor AtenXlaTypeDefault::gelu(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::gelu", 1);
  TF_VLOG(3) << "XLA gelu :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::gelu(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::gelu_backward(const at::Tensor & grad, const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::gelu_backward", 1);
  TF_VLOG(3) << "XLA gelu_backward :" << " grad=" << grad.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::gelu_backward(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad));
}

at::Tensor AtenXlaTypeDefault::hardshrink(const at::Tensor & self, const at::Scalar & lambd) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardshrink", 1);
  TF_VLOG(3) << "XLA hardshrink :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardshrink(external_tensors[0], lambd);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::hardshrink_backward(const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardshrink_backward", 1);
  TF_VLOG(3) << "XLA hardshrink_backward :" << " grad_out=" << grad_out.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_out, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardshrink_backward(external_tensors[0], external_tensors[1], lambd);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_out));
}

at::Tensor AtenXlaTypeDefault::rsqrt(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::rsqrt", 1);
  TF_VLOG(3) << "XLA rsqrt :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::rsqrt(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::rsqrt_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::rsqrt_out", 1);
  TF_VLOG(3) << "XLA rsqrt_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::rsqrt_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::select(const at::Tensor & self, int64_t dim, int64_t index) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::select", 1);
  TF_VLOG(3) << "XLA select :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::select(external_tensors[0], dim, index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::silu_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::silu_out", 1);
  TF_VLOG(3) << "XLA silu_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::silu_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::sigmoid(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sigmoid", 1);
  TF_VLOG(3) << "XLA sigmoid :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sigmoid(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::sigmoid_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sigmoid_out", 1);
  TF_VLOG(3) << "XLA sigmoid_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sigmoid_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::logit(const at::Tensor & self, c10::optional<double> eps) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logit", 1);
  TF_VLOG(3) << "XLA logit :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logit(external_tensors[0], eps);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::logit_out(const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logit_out", 1);
  TF_VLOG(3) << "XLA logit_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logit_outf(external_tensors[0], eps, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::logit_(at::Tensor & self, c10::optional<double> eps) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logit_", 1);
  TF_VLOG(3) << "XLA logit_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logit_(external_tensors[0], eps);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::sin(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sin", 1);
  TF_VLOG(3) << "XLA sin :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sin(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::sin_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sin_out", 1);
  TF_VLOG(3) << "XLA sin_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sin_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::sinc_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sinc_out", 1);
  TF_VLOG(3) << "XLA sinc_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sinc_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::sinh(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sinh", 1);
  TF_VLOG(3) << "XLA sinh :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sinh(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::sinh_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sinh_out", 1);
  TF_VLOG(3) << "XLA sinh_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sinh_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::slice(const at::Tensor & self, int64_t dim, c10::optional<int64_t> start, c10::optional<int64_t> end, int64_t step) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slice", 1);
  TF_VLOG(3) << "XLA slice :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::slice(external_tensors[0], dim, start, end, step);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_softmax", 1);
  TF_VLOG(3) << "XLA _softmax :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_softmax(external_tensors[0], dim, half_to_float);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_softmax_backward_data", 1);
  TF_VLOG(3) << "XLA _softmax_backward_data :" << " grad_output=" << grad_output.toString() << " output=" << output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_softmax_backward_data(external_tensors[0], external_tensors[1], dim, external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

std::vector<at::Tensor> AtenXlaTypeDefault::split(const at::Tensor & self, int64_t split_size, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::split", 1);
  TF_VLOG(3) << "XLA split :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::split(external_tensors[0], split_size, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::vector<at::Tensor> AtenXlaTypeDefault::split_with_sizes(const at::Tensor & self, at::IntArrayRef split_sizes, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::split_with_sizes", 1);
  TF_VLOG(3) << "XLA split_with_sizes :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::split_with_sizes(external_tensors[0], split_sizes, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::squeeze(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::squeeze", 1);
  TF_VLOG(3) << "XLA squeeze :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::squeeze(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::squeeze_(at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::squeeze_", 1);
  TF_VLOG(3) << "XLA squeeze_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].squeeze_();
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::squeeze(const at::Tensor & self, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::squeeze", 1);
  TF_VLOG(3) << "XLA squeeze :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::squeeze(external_tensors[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::squeeze_(at::Tensor & self, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::squeeze_", 1);
  TF_VLOG(3) << "XLA squeeze_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].squeeze_(dim);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::sspaddmm_out(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sspaddmm_out", 1);
  TF_VLOG(3) << "XLA sspaddmm_out :" << " self=" << self.toString() << " mat1=" << mat1.toString() << " mat2=" << mat2.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mat1, mat2, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sspaddmm_outf(external_tensors[0], external_tensors[1], external_tensors[2], beta, alpha, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::stack(at::TensorList tensors, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::stack", 1);
  TF_VLOG(3) << "XLA stack :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::stack(l_tensors, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

at::Tensor AtenXlaTypeDefault::sum(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sum", 1);
  TF_VLOG(3) << "XLA sum :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sum(external_tensors[0], dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::sum(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sum", 1);
  TF_VLOG(3) << "XLA sum :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sum(external_tensors[0], dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::sum_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sum_out", 1);
  TF_VLOG(3) << "XLA sum_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sum_outf(external_tensors[0], dim, keepdim, dtype, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::nansum(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nansum", 1);
  TF_VLOG(3) << "XLA nansum :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::nansum(external_tensors[0], dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::nansum(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nansum", 1);
  TF_VLOG(3) << "XLA nansum :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::nansum(external_tensors[0], dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::nansum_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nansum_out", 1);
  TF_VLOG(3) << "XLA nansum_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::nansum_outf(external_tensors[0], dim, keepdim, dtype, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::sqrt(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sqrt", 1);
  TF_VLOG(3) << "XLA sqrt :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sqrt(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::sqrt_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sqrt_out", 1);
  TF_VLOG(3) << "XLA sqrt_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sqrt_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::square_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::square_out", 1);
  TF_VLOG(3) << "XLA square_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::square_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::std(const at::Tensor & self, bool unbiased) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::std", 1);
  TF_VLOG(3) << "XLA std :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::std(external_tensors[0], unbiased);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::std(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::std", 1);
  TF_VLOG(3) << "XLA std :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::std(external_tensors[0], dim, unbiased, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::std(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::std", 1);
  TF_VLOG(3) << "XLA std :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::std(external_tensors[0], dim, correction, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::std_out(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::std_out", 1);
  TF_VLOG(3) << "XLA std_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::std_outf(external_tensors[0], dim, correction, keepdim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::std_mean(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::std_mean", 1);
  TF_VLOG(3) << "XLA std_mean :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::std_mean(external_tensors[0], dim, correction, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::prod(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::prod", 1);
  TF_VLOG(3) << "XLA prod :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::prod(external_tensors[0], dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::prod(const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::prod", 1);
  TF_VLOG(3) << "XLA prod :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::prod(external_tensors[0], dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::prod_out(const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::prod_out", 1);
  TF_VLOG(3) << "XLA prod_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::prod_outf(external_tensors[0], dim, keepdim, dtype, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::t(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::t", 1);
  TF_VLOG(3) << "XLA t :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::t(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::t_(at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::t_", 1);
  TF_VLOG(3) << "XLA t_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].t_();
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::tan(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::tan", 1);
  TF_VLOG(3) << "XLA tan :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::tan(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::tan_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::tan_out", 1);
  TF_VLOG(3) << "XLA tan_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::tan_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::tanh(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::tanh", 1);
  TF_VLOG(3) << "XLA tanh :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::tanh(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::tanh_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::tanh_out", 1);
  TF_VLOG(3) << "XLA tanh_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::tanh_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::tensordot_out(const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::tensordot_out", 1);
  TF_VLOG(3) << "XLA tensordot_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::tensordot_outf(external_tensors[0], external_tensors[1], dims_self, dims_other, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::threshold(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::threshold", 1);
  TF_VLOG(3) << "XLA threshold :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::threshold(external_tensors[0], threshold, value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::threshold_out(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::threshold_out", 1);
  TF_VLOG(3) << "XLA threshold_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::threshold_outf(external_tensors[0], threshold, value, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::threshold_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::threshold_backward", 1);
  TF_VLOG(3) << "XLA threshold_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::threshold_backward(external_tensors[0], external_tensors[1], threshold);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::threshold_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::threshold_backward_out", 1);
  TF_VLOG(3) << "XLA threshold_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::threshold_backward_outf(external_tensors[0], external_tensors[1], threshold, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::transpose(const at::Tensor & self, int64_t dim0, int64_t dim1) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::transpose", 1);
  TF_VLOG(3) << "XLA transpose :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::transpose(external_tensors[0], dim0, dim1);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::transpose_(at::Tensor & self, int64_t dim0, int64_t dim1) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::transpose_", 1);
  TF_VLOG(3) << "XLA transpose_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].transpose_(dim0, dim1);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::_mkldnn_transpose(const at::Tensor & self, int64_t dim0, int64_t dim1) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_mkldnn_transpose", 1);
  TF_VLOG(3) << "XLA _mkldnn_transpose :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_mkldnn_transpose(external_tensors[0], dim0, dim1);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::_mkldnn_transpose_(at::Tensor & self, int64_t dim0, int64_t dim1) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_mkldnn_transpose_", 1);
  TF_VLOG(3) << "XLA _mkldnn_transpose_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_mkldnn_transpose_(external_tensors[0], dim0, dim1);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::flip(const at::Tensor & self, at::IntArrayRef dims) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::flip", 1);
  TF_VLOG(3) << "XLA flip :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::flip(external_tensors[0], dims);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::roll(const at::Tensor & self, at::IntArrayRef shifts, at::IntArrayRef dims) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::roll", 1);
  TF_VLOG(3) << "XLA roll :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::roll(external_tensors[0], shifts, dims);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_trilinear(const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_trilinear", 1);
  TF_VLOG(3) << "XLA _trilinear :" << " i1=" << i1.toString() << " i2=" << i2.toString() << " i3=" << i3.toString();
  std::vector<at::Tensor> external_tensors_tensors = {i1, i2, i3};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_trilinear(external_tensors[0], external_tensors[1], external_tensors[2], expand1, expand2, expand3, sumdim, unroll_dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(i1));
}

at::Tensor AtenXlaTypeDefault::trunc(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::trunc", 1);
  TF_VLOG(3) << "XLA trunc :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::trunc(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::trunc_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::trunc_out", 1);
  TF_VLOG(3) << "XLA trunc_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::trunc_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::_unique(const at::Tensor & self, bool sorted, bool return_inverse) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_unique", 1);
  TF_VLOG(3) << "XLA _unique :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_unique(external_tensors[0], sorted, return_inverse);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::unique_dim(const at::Tensor & self, int64_t dim, bool sorted, bool return_inverse, bool return_counts) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::unique_dim", 1);
  TF_VLOG(3) << "XLA unique_dim :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::unique_dim(external_tensors[0], dim, sorted, return_inverse, return_counts);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::unique_consecutive(const at::Tensor & self, bool return_inverse, bool return_counts, c10::optional<int64_t> dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::unique_consecutive", 1);
  TF_VLOG(3) << "XLA unique_consecutive :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::unique_consecutive(external_tensors[0], return_inverse, return_counts, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::unique_dim_consecutive(const at::Tensor & self, int64_t dim, bool return_inverse, bool return_counts) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::unique_dim_consecutive", 1);
  TF_VLOG(3) << "XLA unique_dim_consecutive :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::unique_dim_consecutive(external_tensors[0], dim, return_inverse, return_counts);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::_unique2(const at::Tensor & self, bool sorted, bool return_inverse, bool return_counts) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_unique2", 1);
  TF_VLOG(3) << "XLA _unique2 :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_unique2(external_tensors[0], sorted, return_inverse, return_counts);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::_unsafe_view(const at::Tensor & self, at::IntArrayRef size) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_unsafe_view", 1);
  TF_VLOG(3) << "XLA _unsafe_view :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_unsafe_view(external_tensors[0], size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::unsqueeze(const at::Tensor & self, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::unsqueeze", 1);
  TF_VLOG(3) << "XLA unsqueeze :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::unsqueeze(external_tensors[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::unsqueeze_(at::Tensor & self, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::unsqueeze_", 1);
  TF_VLOG(3) << "XLA unsqueeze_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].unsqueeze_(dim);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::var(const at::Tensor & self, bool unbiased) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::var", 1);
  TF_VLOG(3) << "XLA var :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::var(external_tensors[0], unbiased);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::var(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::var", 1);
  TF_VLOG(3) << "XLA var :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::var(external_tensors[0], dim, unbiased, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::var(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::var", 1);
  TF_VLOG(3) << "XLA var :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::var(external_tensors[0], dim, correction, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::var_out(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::var_out", 1);
  TF_VLOG(3) << "XLA var_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::var_outf(external_tensors[0], dim, correction, keepdim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::var_mean(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::var_mean", 1);
  TF_VLOG(3) << "XLA var_mean :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::var_mean(external_tensors[0], dim, correction, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::_s_where(const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_s_where", 1);
  TF_VLOG(3) << "XLA _s_where :" << " condition=" << condition.toString() << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {condition, self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_s_where(external_tensors[0], external_tensors[1], external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(condition));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::_weight_norm_cuda_interface(const at::Tensor & v, const at::Tensor & g, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_weight_norm_cuda_interface", 1);
  TF_VLOG(3) << "XLA _weight_norm_cuda_interface :" << " v=" << v.toString() << " g=" << g.toString();
  std::vector<at::Tensor> external_tensors_tensors = {v, g};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_weight_norm_cuda_interface(external_tensors[0], external_tensors[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(v)), to_device_opt(std::get<1>(x_result), get_device_arg(v)));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::_weight_norm_cuda_interface_backward(const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_weight_norm_cuda_interface_backward", 1);
  TF_VLOG(3) << "XLA _weight_norm_cuda_interface_backward :" << " grad_w=" << grad_w.toString() << " saved_v=" << saved_v.toString() << " saved_g=" << saved_g.toString() << " saved_norms=" << saved_norms.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_w, saved_v, saved_g, saved_norms};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_weight_norm_cuda_interface_backward(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors[3], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_w)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_w)));
}

at::Tensor AtenXlaTypeDefault::_standard_gamma_grad(const at::Tensor & self, const at::Tensor & output) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_standard_gamma_grad", 1);
  TF_VLOG(3) << "XLA _standard_gamma_grad :" << " self=" << self.toString() << " output=" << output.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, output};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_standard_gamma_grad(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_standard_gamma(const at::Tensor & self, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_standard_gamma", 1);
  TF_VLOG(3) << "XLA _standard_gamma :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_standard_gamma(external_tensors[0], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_dirichlet_grad(const at::Tensor & x, const at::Tensor & alpha, const at::Tensor & total) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_dirichlet_grad", 1);
  TF_VLOG(3) << "XLA _dirichlet_grad :" << " x=" << x.toString() << " alpha=" << alpha.toString() << " total=" << total.toString();
  std::vector<at::Tensor> external_tensors_tensors = {x, alpha, total};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_dirichlet_grad(external_tensors[0], external_tensors[1], external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(x));
}

at::Tensor AtenXlaTypeDefault::_sample_dirichlet(const at::Tensor & self, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_sample_dirichlet", 1);
  TF_VLOG(3) << "XLA _sample_dirichlet :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_sample_dirichlet(external_tensors[0], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::poisson(const at::Tensor & self, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::poisson", 1);
  TF_VLOG(3) << "XLA poisson :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::poisson(external_tensors[0], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::binomial(const at::Tensor & count, const at::Tensor & prob, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::binomial", 1);
  TF_VLOG(3) << "XLA binomial :" << " count=" << count.toString() << " prob=" << prob.toString();
  std::vector<at::Tensor> external_tensors_tensors = {count, prob};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::binomial(external_tensors[0], external_tensors[1], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(count));
}

at::Tensor AtenXlaTypeDefault::native_norm(const at::Tensor & self, const at::Scalar & p) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::native_norm", 1);
  TF_VLOG(3) << "XLA native_norm :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::native_norm(external_tensors[0], p);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::native_norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::native_norm", 1);
  TF_VLOG(3) << "XLA native_norm :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::native_norm(external_tensors[0], p, dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_sparse_sum_backward(const at::Tensor & grad, const at::Tensor & self, at::IntArrayRef dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_sparse_sum_backward", 1);
  TF_VLOG(3) << "XLA _sparse_sum_backward :" << " grad=" << grad.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_sparse_sum_backward(external_tensors[0], external_tensors[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad));
}

at::Tensor AtenXlaTypeDefault::_sparse_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_sparse_softmax", 1);
  TF_VLOG(3) << "XLA _sparse_softmax :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_sparse_softmax(external_tensors[0], dim, half_to_float);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_sparse_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_sparse_softmax_backward_data", 1);
  TF_VLOG(3) << "XLA _sparse_softmax_backward_data :" << " grad_output=" << grad_output.toString() << " output=" << output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_sparse_softmax_backward_data(external_tensors[0], external_tensors[1], dim, external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::_sparse_log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_sparse_log_softmax", 1);
  TF_VLOG(3) << "XLA _sparse_log_softmax :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_sparse_log_softmax(external_tensors[0], dim, half_to_float);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_sparse_log_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_sparse_log_softmax_backward_data", 1);
  TF_VLOG(3) << "XLA _sparse_log_softmax_backward_data :" << " grad_output=" << grad_output.toString() << " output=" << output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_sparse_log_softmax_backward_data(external_tensors[0], external_tensors[1], dim, external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::ScalarType dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::norm", 1);
  TF_VLOG(3) << "XLA norm :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::norm(external_tensors[0], p, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::norm(const at::Tensor & self, const at::Scalar & p) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::norm", 1);
  TF_VLOG(3) << "XLA norm :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::norm(external_tensors[0], p);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::norm", 1);
  TF_VLOG(3) << "XLA norm :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::norm(external_tensors[0], p, dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::norm_out(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::norm_out", 1);
  TF_VLOG(3) << "XLA norm_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::norm_outf(external_tensors[0], p, dim, keepdim, dtype, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::norm", 1);
  TF_VLOG(3) << "XLA norm :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::norm(external_tensors[0], p, dim, keepdim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::norm_out(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::norm_out", 1);
  TF_VLOG(3) << "XLA norm_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::norm_outf(external_tensors[0], p, dim, keepdim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::frexp_out(const at::Tensor & self, at::Tensor & mantissa, at::Tensor & exponent) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::frexp_out", 1);
  TF_VLOG(3) << "XLA frexp_out :" << " self=" << self.toString() << " mantissa=" << mantissa.toString() << " exponent=" << exponent.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mantissa, exponent};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::frexp_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(mantissa, exponent);
}

at::Tensor AtenXlaTypeDefault::clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::clone", 1);
  TF_VLOG(3) << "XLA clone :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::clone(external_tensors[0], memory_format);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

const at::Tensor & AtenXlaTypeDefault::resize_as_sparse_(const at::Tensor & self, const at::Tensor & the_template) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::resize_as_sparse_", 1);
  TF_VLOG(3) << "XLA resize_as_sparse_ :" << " self=" << self.toString() << " the_template=" << the_template.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, the_template};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::resize_as_sparse_(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::zero_(at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::zero_", 1);
  TF_VLOG(3) << "XLA zero_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::zero_(external_tensors[0]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::sub(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sub", 1);
  TF_VLOG(3) << "XLA sub :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sub(external_tensors[0], external_tensors[1], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::sub_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sub_out", 1);
  TF_VLOG(3) << "XLA sub_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sub_outf(external_tensors[0], external_tensors[1], alpha, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::sub(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sub", 1);
  TF_VLOG(3) << "XLA sub :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sub(external_tensors[0], other, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::rsub(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::rsub", 1);
  TF_VLOG(3) << "XLA rsub :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::rsub(external_tensors[0], external_tensors[1], alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::heaviside_out(const at::Tensor & self, const at::Tensor & values, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::heaviside_out", 1);
  TF_VLOG(3) << "XLA heaviside_out :" << " self=" << self.toString() << " values=" << values.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, values, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::heaviside_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::rsub(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::rsub", 1);
  TF_VLOG(3) << "XLA rsub :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::rsub(external_tensors[0], other, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::addmm", 1);
  TF_VLOG(3) << "XLA addmm :" << " self=" << self.toString() << " mat1=" << mat1.toString() << " mat2=" << mat2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mat1, mat2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::addmm(external_tensors[0], external_tensors[1], external_tensors[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::addmm_out(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::addmm_out", 1);
  TF_VLOG(3) << "XLA addmm_out :" << " self=" << self.toString() << " mat1=" << mat1.toString() << " mat2=" << mat2.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mat1, mat2, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::addmm_outf(external_tensors[0], external_tensors[1], external_tensors[2], beta, alpha, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_sparse_coo_tensor_with_dims", 1);
  TF_VLOG(3) << "XLA _sparse_coo_tensor_with_dims :";
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_sparse_coo_tensor_with_dims(sparse_dim, dense_dim, size, dtype, layout, device, pin_memory);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(device));
}

at::Tensor AtenXlaTypeDefault::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_sparse_coo_tensor_with_dims_and_tensors", 1);
  TF_VLOG(3) << "XLA _sparse_coo_tensor_with_dims_and_tensors :" << " indices=" << indices.toString() << " values=" << values.toString();
  std::vector<at::Tensor> external_tensors_tensors = {indices, values};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_sparse_coo_tensor_with_dims_and_tensors(sparse_dim, dense_dim, size, external_tensors[0], external_tensors[1], dtype, layout, device, pin_memory);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(indices));
}

const at::Tensor & AtenXlaTypeDefault::sparse_resize_(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sparse_resize_", 1);
  TF_VLOG(3) << "XLA sparse_resize_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].sparse_resize_(size, sparse_dim, dense_dim);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

const at::Tensor & AtenXlaTypeDefault::sparse_resize_and_clear_(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sparse_resize_and_clear_", 1);
  TF_VLOG(3) << "XLA sparse_resize_and_clear_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].sparse_resize_and_clear_(size, sparse_dim, dense_dim);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::sparse_mask(const at::Tensor & self, const at::Tensor & mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sparse_mask", 1);
  TF_VLOG(3) << "XLA sparse_mask :" << " self=" << self.toString() << " mask=" << mask.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mask};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].sparse_mask(external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::vector<at::Tensor> AtenXlaTypeDefault::_to_cpu(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_to_cpu", 1);
  TF_VLOG(3) << "XLA _to_cpu :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_to_cpu(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

at::Tensor AtenXlaTypeDefault::to_dense(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::to_dense", 1);
  TF_VLOG(3) << "XLA to_dense :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].to_dense(dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

int64_t AtenXlaTypeDefault::sparse_dim(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sparse_dim", 1);
  TF_VLOG(3) << "XLA sparse_dim :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].sparse_dim();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

int64_t AtenXlaTypeDefault::_dimI(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_dimI", 1);
  TF_VLOG(3) << "XLA _dimI :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0]._dimI();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

int64_t AtenXlaTypeDefault::dense_dim(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::dense_dim", 1);
  TF_VLOG(3) << "XLA dense_dim :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].dense_dim();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

int64_t AtenXlaTypeDefault::_dimV(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_dimV", 1);
  TF_VLOG(3) << "XLA _dimV :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0]._dimV();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

int64_t AtenXlaTypeDefault::_nnz(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_nnz", 1);
  TF_VLOG(3) << "XLA _nnz :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0]._nnz();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor AtenXlaTypeDefault::_coalesce(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_coalesce", 1);
  TF_VLOG(3) << "XLA _coalesce :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_coalesce(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

bool AtenXlaTypeDefault::is_coalesced(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::is_coalesced", 1);
  TF_VLOG(3) << "XLA is_coalesced :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].is_coalesced();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor AtenXlaTypeDefault::_indices(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_indices", 1);
  TF_VLOG(3) << "XLA _indices :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0]._indices();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_values(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_values", 1);
  TF_VLOG(3) << "XLA _values :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0]._values();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::_coalesced_(at::Tensor & self, bool coalesced) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_coalesced_", 1);
  TF_VLOG(3) << "XLA _coalesced_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0]._coalesced_(coalesced);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::indices(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::indices", 1);
  TF_VLOG(3) << "XLA indices :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].indices();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::values(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::values", 1);
  TF_VLOG(3) << "XLA values :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].values();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::crow_indices(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::crow_indices", 1);
  TF_VLOG(3) << "XLA crow_indices :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].crow_indices();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::col_indices(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::col_indices", 1);
  TF_VLOG(3) << "XLA col_indices :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].col_indices();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::hspmm(const at::Tensor & mat1, const at::Tensor & mat2) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hspmm", 1);
  TF_VLOG(3) << "XLA hspmm :" << " mat1=" << mat1.toString() << " mat2=" << mat2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {mat1, mat2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hspmm(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(mat1));
}

at::Tensor & AtenXlaTypeDefault::hspmm_out(const at::Tensor & mat1, const at::Tensor & mat2, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hspmm_out", 1);
  TF_VLOG(3) << "XLA hspmm_out :" << " mat1=" << mat1.toString() << " mat2=" << mat2.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {mat1, mat2, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hspmm_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::copy_sparse_to_sparse_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::copy_sparse_to_sparse_", 1);
  TF_VLOG(3) << "XLA copy_sparse_to_sparse_ :" << " self=" << self.toString() << " src=" << src.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, src};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::copy_sparse_to_sparse_(external_tensors[0], external_tensors[1], non_blocking);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

std::vector<at::Tensor> AtenXlaTypeDefault::unbind(const at::Tensor & self, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::unbind", 1);
  TF_VLOG(3) << "XLA unbind :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::unbind(external_tensors[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::to_sparse(const at::Tensor & self, int64_t sparse_dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::to_sparse", 1);
  TF_VLOG(3) << "XLA to_sparse :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].to_sparse(sparse_dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::to_sparse(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::to_sparse", 1);
  TF_VLOG(3) << "XLA to_sparse :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].to_sparse();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::to_mkldnn(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::to_mkldnn", 1);
  TF_VLOG(3) << "XLA to_mkldnn :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].to_mkldnn(dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::mkldnn_reorder_conv2d_weight(const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mkldnn_reorder_conv2d_weight", 1);
  TF_VLOG(3) << "XLA mkldnn_reorder_conv2d_weight :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mkldnn_reorder_conv2d_weight(external_tensors[0], padding, stride, dilation, groups);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::mkldnn_reorder_conv3d_weight(const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mkldnn_reorder_conv3d_weight", 1);
  TF_VLOG(3) << "XLA mkldnn_reorder_conv3d_weight :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mkldnn_reorder_conv3d_weight(external_tensors[0], padding, stride, dilation, groups);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::quantize_per_tensor(const at::Tensor & self, double scale, int64_t zero_point, at::ScalarType dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::quantize_per_tensor", 1);
  TF_VLOG(3) << "XLA quantize_per_tensor :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::quantize_per_tensor(external_tensors[0], scale, zero_point, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::vector<at::Tensor> AtenXlaTypeDefault::quantize_per_tensor(at::TensorList tensors, const at::Tensor & scales, const at::Tensor & zero_points, at::ScalarType dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::quantize_per_tensor", 1);
  TF_VLOG(3) << "XLA quantize_per_tensor :" << " scales=" << scales.toString() << " zero_points=" << zero_points.toString();
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {scales, zero_points};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::quantize_per_tensor(l_tensors, external_tensors[0], external_tensors[1], dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(scales));
}

at::Tensor AtenXlaTypeDefault::quantize_per_channel(const at::Tensor & self, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, at::ScalarType dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::quantize_per_channel", 1);
  TF_VLOG(3) << "XLA quantize_per_channel :" << " self=" << self.toString() << " scales=" << scales.toString() << " zero_points=" << zero_points.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, scales, zero_points};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::quantize_per_channel(external_tensors[0], external_tensors[1], external_tensors[2], axis, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::dequantize(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::dequantize", 1);
  TF_VLOG(3) << "XLA dequantize :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::dequantize(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::vector<at::Tensor> AtenXlaTypeDefault::dequantize(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::dequantize", 1);
  TF_VLOG(3) << "XLA dequantize :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::dequantize(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

double AtenXlaTypeDefault::q_scale(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::q_scale", 1);
  TF_VLOG(3) << "XLA q_scale :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::q_scale(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

int64_t AtenXlaTypeDefault::q_zero_point(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::q_zero_point", 1);
  TF_VLOG(3) << "XLA q_zero_point :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::q_zero_point(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor AtenXlaTypeDefault::q_per_channel_scales(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::q_per_channel_scales", 1);
  TF_VLOG(3) << "XLA q_per_channel_scales :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::q_per_channel_scales(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::q_per_channel_zero_points(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::q_per_channel_zero_points", 1);
  TF_VLOG(3) << "XLA q_per_channel_zero_points :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::q_per_channel_zero_points(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

int64_t AtenXlaTypeDefault::q_per_channel_axis(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::q_per_channel_axis", 1);
  TF_VLOG(3) << "XLA q_per_channel_axis :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::q_per_channel_axis(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor AtenXlaTypeDefault::int_repr(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::int_repr", 1);
  TF_VLOG(3) << "XLA int_repr :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::int_repr(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_make_per_tensor_quantized_tensor(const at::Tensor & self, double scale, int64_t zero_point) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_make_per_tensor_quantized_tensor", 1);
  TF_VLOG(3) << "XLA _make_per_tensor_quantized_tensor :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_make_per_tensor_quantized_tensor(external_tensors[0], scale, zero_point);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_make_per_channel_quantized_tensor(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_make_per_channel_quantized_tensor", 1);
  TF_VLOG(3) << "XLA _make_per_channel_quantized_tensor :" << " self=" << self.toString() << " scale=" << scale.toString() << " zero_point=" << zero_point.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, scale, zero_point};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_make_per_channel_quantized_tensor(external_tensors[0], external_tensors[1], external_tensors[2], axis);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::QScheme AtenXlaTypeDefault::qscheme(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::qscheme", 1);
  TF_VLOG(3) << "XLA qscheme :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].qscheme();
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::fake_quantize_per_tensor_affine_cachemask(const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fake_quantize_per_tensor_affine_cachemask", 1);
  TF_VLOG(3) << "XLA fake_quantize_per_tensor_affine_cachemask :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fake_quantize_per_tensor_affine_cachemask(external_tensors[0], scale, zero_point, quant_min, quant_max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::_fake_quantize_learnable_per_tensor_affine(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_fake_quantize_learnable_per_tensor_affine", 1);
  TF_VLOG(3) << "XLA _fake_quantize_learnable_per_tensor_affine :" << " self=" << self.toString() << " scale=" << scale.toString() << " zero_point=" << zero_point.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, scale, zero_point};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_fake_quantize_learnable_per_tensor_affine(external_tensors[0], external_tensors[1], external_tensors[2], quant_min, quant_max, grad_factor);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::fake_quantize_per_channel_affine_cachemask(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fake_quantize_per_channel_affine_cachemask", 1);
  TF_VLOG(3) << "XLA fake_quantize_per_channel_affine_cachemask :" << " self=" << self.toString() << " scale=" << scale.toString() << " zero_point=" << zero_point.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, scale, zero_point};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fake_quantize_per_channel_affine_cachemask(external_tensors[0], external_tensors[1], external_tensors[2], axis, quant_min, quant_max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::_fake_quantize_learnable_per_channel_affine(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_fake_quantize_learnable_per_channel_affine", 1);
  TF_VLOG(3) << "XLA _fake_quantize_learnable_per_channel_affine :" << " self=" << self.toString() << " scale=" << scale.toString() << " zero_point=" << zero_point.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, scale, zero_point};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_fake_quantize_learnable_per_channel_affine(external_tensors[0], external_tensors[1], external_tensors[2], axis, quant_min, quant_max, grad_factor);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Scalar AtenXlaTypeDefault::_local_scalar_dense(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_local_scalar_dense", 1);
  TF_VLOG(3) << "XLA _local_scalar_dense :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_local_scalar_dense(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::_thnn_fused_lstm_cell(const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & cx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_thnn_fused_lstm_cell", 1);
  TF_VLOG(3) << "XLA _thnn_fused_lstm_cell :" << " input_gates=" << input_gates.toString() << " hidden_gates=" << hidden_gates.toString() << " cx=" << cx.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input_gates, hidden_gates, cx};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {input_bias, hidden_bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::_thnn_fused_lstm_cell(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors_opt[0], external_tensors_opt[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(input_gates)), to_device_opt(std::get<1>(x_result), get_device_arg(input_gates)), to_device_opt(std::get<2>(x_result), get_device_arg(input_gates)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::_thnn_fused_lstm_cell_backward(const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & cx, const at::Tensor & cy, const at::Tensor & workspace, bool has_bias) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_thnn_fused_lstm_cell_backward", 1);
  TF_VLOG(3) << "XLA _thnn_fused_lstm_cell_backward :" << " cx=" << cx.toString() << " cy=" << cy.toString() << " workspace=" << workspace.toString();
  std::vector<at::Tensor> external_tensors_tensors = {cx, cy, workspace};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {grad_hy, grad_cy};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::_thnn_fused_lstm_cell_backward(external_tensors_opt[0], external_tensors_opt[1], external_tensors[0], external_tensors[1], external_tensors[2], has_bias);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_hy)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_hy)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_hy)), to_device_opt(std::get<3>(x_result), get_device_arg(grad_hy)), to_device_opt(std::get<4>(x_result), get_device_arg(grad_hy)));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::_thnn_fused_gru_cell(const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_thnn_fused_gru_cell", 1);
  TF_VLOG(3) << "XLA _thnn_fused_gru_cell :" << " input_gates=" << input_gates.toString() << " hidden_gates=" << hidden_gates.toString() << " hx=" << hx.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input_gates, hidden_gates, hx};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {input_bias, hidden_bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::_thnn_fused_gru_cell(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors_opt[0], external_tensors_opt[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(input_gates)), to_device_opt(std::get<1>(x_result), get_device_arg(input_gates)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::_thnn_fused_gru_cell_backward(const at::Tensor & grad_hy, const at::Tensor & workspace, bool has_bias) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_thnn_fused_gru_cell_backward", 1);
  TF_VLOG(3) << "XLA _thnn_fused_gru_cell_backward :" << " grad_hy=" << grad_hy.toString() << " workspace=" << workspace.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_hy, workspace};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_thnn_fused_gru_cell_backward(external_tensors[0], external_tensors[1], has_bias);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_hy)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_hy)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_hy)), to_device_opt(std::get<3>(x_result), get_device_arg(grad_hy)), to_device_opt(std::get<4>(x_result), get_device_arg(grad_hy)));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::_pack_padded_sequence(const at::Tensor & input, const at::Tensor & lengths, bool batch_first) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_pack_padded_sequence", 1);
  TF_VLOG(3) << "XLA _pack_padded_sequence :" << " input=" << input.toString() << " lengths=" << lengths.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, lengths};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_pack_padded_sequence(external_tensors[0], external_tensors[1], batch_first);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(input)), to_device_opt(std::get<1>(x_result), get_device_arg(input)));
}

at::Tensor & AtenXlaTypeDefault::set_(at::Tensor & self, at::Storage source) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::set_", 1);
  TF_VLOG(3) << "XLA set_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].set_(source);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::set_(at::Tensor & self, at::Storage source, int64_t storage_offset, at::IntArrayRef size, at::IntArrayRef stride) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::set_", 1);
  TF_VLOG(3) << "XLA set_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].set_(source, storage_offset, size, stride);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::set_(at::Tensor & self, const at::Tensor & source) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::set_", 1);
  TF_VLOG(3) << "XLA set_ :" << " self=" << self.toString() << " source=" << source.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, source};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].set_(external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::set_(at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::set_", 1);
  TF_VLOG(3) << "XLA set_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].set_();
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

bool AtenXlaTypeDefault::is_set_to(const at::Tensor & self, const at::Tensor & tensor) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::is_set_to", 1);
  TF_VLOG(3) << "XLA is_set_to :" << " self=" << self.toString() << " tensor=" << tensor.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, tensor};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].is_set_to(external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor & AtenXlaTypeDefault::masked_fill_(at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::masked_fill_", 1);
  TF_VLOG(3) << "XLA masked_fill_ :" << " self=" << self.toString() << " mask=" << mask.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mask};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].masked_fill_(external_tensors[1], value);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::masked_fill_(at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::masked_fill_", 1);
  TF_VLOG(3) << "XLA masked_fill_ :" << " self=" << self.toString() << " mask=" << mask.toString() << " value=" << value.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mask, value};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].masked_fill_(external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::masked_scatter_(at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::masked_scatter_", 1);
  TF_VLOG(3) << "XLA masked_scatter_ :" << " self=" << self.toString() << " mask=" << mask.toString() << " source=" << source.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mask, source};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].masked_scatter_(external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::view(const at::Tensor & self, at::IntArrayRef size) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::view", 1);
  TF_VLOG(3) << "XLA view :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].view(size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::put_(at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::put_", 1);
  TF_VLOG(3) << "XLA put_ :" << " self=" << self.toString() << " index=" << index.toString() << " source=" << source.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index, source};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].put_(external_tensors[1], external_tensors[2], accumulate);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::index_add_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::index_add_", 1);
  TF_VLOG(3) << "XLA index_add_ :" << " self=" << self.toString() << " index=" << index.toString() << " source=" << source.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index, source};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].index_add_(dim, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::index_add_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::index_add_", 1);
  TF_VLOG(3) << "XLA index_add_ :" << " self=" << self.toString() << " index=" << index.toString() << " source=" << source.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index, source};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].index_add_(dim, external_tensors[1], external_tensors[2], alpha);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::index_fill_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::index_fill_", 1);
  TF_VLOG(3) << "XLA index_fill_ :" << " self=" << self.toString() << " index=" << index.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].index_fill_(dim, external_tensors[1], value);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::index_fill_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::index_fill_", 1);
  TF_VLOG(3) << "XLA index_fill_ :" << " self=" << self.toString() << " index=" << index.toString() << " value=" << value.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index, value};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].index_fill_(dim, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::scatter_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::scatter_", 1);
  TF_VLOG(3) << "XLA scatter_ :" << " self=" << self.toString() << " index=" << index.toString() << " src=" << src.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index, src};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].scatter_(dim, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::scatter_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::scatter_", 1);
  TF_VLOG(3) << "XLA scatter_ :" << " self=" << self.toString() << " index=" << index.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].scatter_(dim, external_tensors[1], value);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::scatter_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::scatter_", 1);
  TF_VLOG(3) << "XLA scatter_ :" << " self=" << self.toString() << " index=" << index.toString() << " src=" << src.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index, src};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].scatter_(dim, external_tensors[1], external_tensors[2], reduce);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::scatter_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::scatter_", 1);
  TF_VLOG(3) << "XLA scatter_ :" << " self=" << self.toString() << " index=" << index.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].scatter_(dim, external_tensors[1], value, reduce);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::scatter_add_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::scatter_add_", 1);
  TF_VLOG(3) << "XLA scatter_add_ :" << " self=" << self.toString() << " index=" << index.toString() << " src=" << src.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index, src};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].scatter_add_(dim, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::eq(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::eq", 1);
  TF_VLOG(3) << "XLA eq :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::eq(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::eq_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::eq_out", 1);
  TF_VLOG(3) << "XLA eq_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::eq_outf(external_tensors[0], other, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::eq(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::eq", 1);
  TF_VLOG(3) << "XLA eq :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::eq(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::eq_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::eq_out", 1);
  TF_VLOG(3) << "XLA eq_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::eq_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::bitwise_and_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bitwise_and_out", 1);
  TF_VLOG(3) << "XLA bitwise_and_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bitwise_and_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::bitwise_and_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bitwise_and_out", 1);
  TF_VLOG(3) << "XLA bitwise_and_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bitwise_and_outf(external_tensors[0], other, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::bitwise_or_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bitwise_or_out", 1);
  TF_VLOG(3) << "XLA bitwise_or_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bitwise_or_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::bitwise_or_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bitwise_or_out", 1);
  TF_VLOG(3) << "XLA bitwise_or_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bitwise_or_outf(external_tensors[0], other, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::bitwise_xor_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bitwise_xor_out", 1);
  TF_VLOG(3) << "XLA bitwise_xor_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bitwise_xor_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::bitwise_xor_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bitwise_xor_out", 1);
  TF_VLOG(3) << "XLA bitwise_xor_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bitwise_xor_outf(external_tensors[0], other, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::__lshift__(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::__lshift__", 1);
  TF_VLOG(3) << "XLA __lshift__ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::__lshift__(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::__ilshift__(at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::__ilshift__", 1);
  TF_VLOG(3) << "XLA __ilshift__ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].__ilshift__(other);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::__lshift__(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::__lshift__", 1);
  TF_VLOG(3) << "XLA __lshift__ :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::__lshift__(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::__ilshift__(at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::__ilshift__", 1);
  TF_VLOG(3) << "XLA __ilshift__ :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].__ilshift__(external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::__rshift__(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::__rshift__", 1);
  TF_VLOG(3) << "XLA __rshift__ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::__rshift__(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::__irshift__(at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::__irshift__", 1);
  TF_VLOG(3) << "XLA __irshift__ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].__irshift__(other);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::__rshift__(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::__rshift__", 1);
  TF_VLOG(3) << "XLA __rshift__ :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::__rshift__(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::__irshift__(at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::__irshift__", 1);
  TF_VLOG(3) << "XLA __irshift__ :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].__irshift__(external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::tril(const at::Tensor & self, int64_t diagonal) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::tril", 1);
  TF_VLOG(3) << "XLA tril :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::tril(external_tensors[0], diagonal);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::tril_out(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::tril_out", 1);
  TF_VLOG(3) << "XLA tril_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::tril_outf(external_tensors[0], diagonal, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::tril_(at::Tensor & self, int64_t diagonal) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::tril_", 1);
  TF_VLOG(3) << "XLA tril_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].tril_(diagonal);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::triu(const at::Tensor & self, int64_t diagonal) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::triu", 1);
  TF_VLOG(3) << "XLA triu :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::triu(external_tensors[0], diagonal);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::triu_out(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::triu_out", 1);
  TF_VLOG(3) << "XLA triu_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::triu_outf(external_tensors[0], diagonal, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::triu_(at::Tensor & self, int64_t diagonal) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::triu_", 1);
  TF_VLOG(3) << "XLA triu_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].triu_(diagonal);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::digamma_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::digamma_out", 1);
  TF_VLOG(3) << "XLA digamma_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::digamma_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::renorm(const at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::renorm", 1);
  TF_VLOG(3) << "XLA renorm :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::renorm(external_tensors[0], p, dim, maxnorm);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::renorm_out(const at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::renorm_out", 1);
  TF_VLOG(3) << "XLA renorm_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::renorm_outf(external_tensors[0], p, dim, maxnorm, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::renorm_(at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::renorm_", 1);
  TF_VLOG(3) << "XLA renorm_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].renorm_(p, dim, maxnorm);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::lerp(const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lerp", 1);
  TF_VLOG(3) << "XLA lerp :" << " self=" << self.toString() << " end=" << end.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, end};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lerp(external_tensors[0], external_tensors[1], weight);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::lerp_out(const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lerp_out", 1);
  TF_VLOG(3) << "XLA lerp_out :" << " self=" << self.toString() << " end=" << end.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, end, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lerp_outf(external_tensors[0], external_tensors[1], weight, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::lerp_(at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lerp_", 1);
  TF_VLOG(3) << "XLA lerp_ :" << " self=" << self.toString() << " end=" << end.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, end};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].lerp_(external_tensors[1], weight);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::lerp(const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lerp", 1);
  TF_VLOG(3) << "XLA lerp :" << " self=" << self.toString() << " end=" << end.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, end, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lerp(external_tensors[0], external_tensors[1], external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::lerp_out(const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lerp_out", 1);
  TF_VLOG(3) << "XLA lerp_out :" << " self=" << self.toString() << " end=" << end.toString() << " weight=" << weight.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, end, weight, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lerp_outf(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::lerp_(at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lerp_", 1);
  TF_VLOG(3) << "XLA lerp_ :" << " self=" << self.toString() << " end=" << end.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, end, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].lerp_(external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::fmod(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fmod", 1);
  TF_VLOG(3) << "XLA fmod :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fmod(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::fmod_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fmod_out", 1);
  TF_VLOG(3) << "XLA fmod_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fmod_outf(external_tensors[0], other, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::fmod_(at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fmod_", 1);
  TF_VLOG(3) << "XLA fmod_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].fmod_(other);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::fmod(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fmod", 1);
  TF_VLOG(3) << "XLA fmod :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fmod(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::fmod_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fmod_out", 1);
  TF_VLOG(3) << "XLA fmod_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fmod_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::fmod_(at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fmod_", 1);
  TF_VLOG(3) << "XLA fmod_ :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].fmod_(external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::remainder(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::remainder", 1);
  TF_VLOG(3) << "XLA remainder :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::remainder(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::remainder_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::remainder_out", 1);
  TF_VLOG(3) << "XLA remainder_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::remainder_outf(external_tensors[0], other, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::remainder_(at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::remainder_", 1);
  TF_VLOG(3) << "XLA remainder_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].remainder_(other);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::remainder(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::remainder", 1);
  TF_VLOG(3) << "XLA remainder :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::remainder(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::remainder_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::remainder_out", 1);
  TF_VLOG(3) << "XLA remainder_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::remainder_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::remainder_(at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::remainder_", 1);
  TF_VLOG(3) << "XLA remainder_ :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].remainder_(external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::addbmm(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::addbmm", 1);
  TF_VLOG(3) << "XLA addbmm :" << " self=" << self.toString() << " batch1=" << batch1.toString() << " batch2=" << batch2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, batch1, batch2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::addbmm(external_tensors[0], external_tensors[1], external_tensors[2], beta, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::addbmm_out(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::addbmm_out", 1);
  TF_VLOG(3) << "XLA addbmm_out :" << " self=" << self.toString() << " batch1=" << batch1.toString() << " batch2=" << batch2.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, batch1, batch2, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::addbmm_outf(external_tensors[0], external_tensors[1], external_tensors[2], beta, alpha, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::addbmm_(at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::addbmm_", 1);
  TF_VLOG(3) << "XLA addbmm_ :" << " self=" << self.toString() << " batch1=" << batch1.toString() << " batch2=" << batch2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, batch1, batch2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].addbmm_(external_tensors[1], external_tensors[2], beta, alpha);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::addcdiv(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::addcdiv", 1);
  TF_VLOG(3) << "XLA addcdiv :" << " self=" << self.toString() << " tensor1=" << tensor1.toString() << " tensor2=" << tensor2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, tensor1, tensor2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::addcdiv(external_tensors[0], external_tensors[1], external_tensors[2], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::addcdiv_out(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::addcdiv_out", 1);
  TF_VLOG(3) << "XLA addcdiv_out :" << " self=" << self.toString() << " tensor1=" << tensor1.toString() << " tensor2=" << tensor2.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, tensor1, tensor2, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::addcdiv_outf(external_tensors[0], external_tensors[1], external_tensors[2], value, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::addcdiv_(at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::addcdiv_", 1);
  TF_VLOG(3) << "XLA addcdiv_ :" << " self=" << self.toString() << " tensor1=" << tensor1.toString() << " tensor2=" << tensor2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, tensor1, tensor2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].addcdiv_(external_tensors[1], external_tensors[2], value);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::random_(at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::random_", 1);
  TF_VLOG(3) << "XLA random_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].random_(from, to, generator);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::random_(at::Tensor & self, int64_t to, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::random_", 1);
  TF_VLOG(3) << "XLA random_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].random_(to, generator);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::random_(at::Tensor & self, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::random_", 1);
  TF_VLOG(3) << "XLA random_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].random_(generator);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::uniform_(at::Tensor & self, double from, double to, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::uniform_", 1);
  TF_VLOG(3) << "XLA uniform_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].uniform_(from, to, generator);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::cauchy_(at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cauchy_", 1);
  TF_VLOG(3) << "XLA cauchy_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].cauchy_(median, sigma, generator);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::log_normal_(at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log_normal_", 1);
  TF_VLOG(3) << "XLA log_normal_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].log_normal_(mean, std, generator);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::exponential_(at::Tensor & self, double lambd, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::exponential_", 1);
  TF_VLOG(3) << "XLA exponential_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].exponential_(lambd, generator);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenXlaTypeDefault::geometric_(at::Tensor & self, double p, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::geometric_", 1);
  TF_VLOG(3) << "XLA geometric_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].geometric_(p, generator);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::diag(const at::Tensor & self, int64_t diagonal) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::diag", 1);
  TF_VLOG(3) << "XLA diag :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::diag(external_tensors[0], diagonal);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::diag_out(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::diag_out", 1);
  TF_VLOG(3) << "XLA diag_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::diag_outf(external_tensors[0], diagonal, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::cross(const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cross", 1);
  TF_VLOG(3) << "XLA cross :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cross(external_tensors[0], external_tensors[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::cross_out(const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cross_out", 1);
  TF_VLOG(3) << "XLA cross_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cross_outf(external_tensors[0], external_tensors[1], dim, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::tril_indices(int64_t row, int64_t col, int64_t offset, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::tril_indices", 1);
  TF_VLOG(3) << "XLA tril_indices :";
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::tril_indices(row, col, offset, dtype, layout, device, pin_memory);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(device));
}

at::Tensor AtenXlaTypeDefault::triu_indices(int64_t row, int64_t col, int64_t offset, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::triu_indices", 1);
  TF_VLOG(3) << "XLA triu_indices :";
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::triu_indices(row, col, offset, dtype, layout, device, pin_memory);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(device));
}

at::Tensor AtenXlaTypeDefault::trace(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::trace", 1);
  TF_VLOG(3) << "XLA trace :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::trace(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::ne(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ne", 1);
  TF_VLOG(3) << "XLA ne :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ne(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::ne_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ne_out", 1);
  TF_VLOG(3) << "XLA ne_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ne_outf(external_tensors[0], other, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::ne(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ne", 1);
  TF_VLOG(3) << "XLA ne :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ne(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::ne_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ne_out", 1);
  TF_VLOG(3) << "XLA ne_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ne_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::ge(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ge", 1);
  TF_VLOG(3) << "XLA ge :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ge(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::ge_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ge_out", 1);
  TF_VLOG(3) << "XLA ge_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ge_outf(external_tensors[0], other, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::ge(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ge", 1);
  TF_VLOG(3) << "XLA ge :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ge(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::ge_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ge_out", 1);
  TF_VLOG(3) << "XLA ge_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ge_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::le(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::le", 1);
  TF_VLOG(3) << "XLA le :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::le(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::le_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::le_out", 1);
  TF_VLOG(3) << "XLA le_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::le_outf(external_tensors[0], other, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::le(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::le", 1);
  TF_VLOG(3) << "XLA le :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::le(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::le_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::le_out", 1);
  TF_VLOG(3) << "XLA le_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::le_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::gt(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::gt", 1);
  TF_VLOG(3) << "XLA gt :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::gt(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::gt_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::gt_out", 1);
  TF_VLOG(3) << "XLA gt_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::gt_outf(external_tensors[0], other, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::gt(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::gt", 1);
  TF_VLOG(3) << "XLA gt :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::gt(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::gt_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::gt_out", 1);
  TF_VLOG(3) << "XLA gt_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::gt_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::lt(const at::Tensor & self, const at::Scalar & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lt", 1);
  TF_VLOG(3) << "XLA lt :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lt(external_tensors[0], other);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::lt_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lt_out", 1);
  TF_VLOG(3) << "XLA lt_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lt_outf(external_tensors[0], other, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::lt(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lt", 1);
  TF_VLOG(3) << "XLA lt :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lt(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::lt_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lt_out", 1);
  TF_VLOG(3) << "XLA lt_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lt_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::take(const at::Tensor & self, const at::Tensor & index) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::take", 1);
  TF_VLOG(3) << "XLA take :" << " self=" << self.toString() << " index=" << index.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::take(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::take_out(const at::Tensor & self, const at::Tensor & index, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::take_out", 1);
  TF_VLOG(3) << "XLA take_out :" << " self=" << self.toString() << " index=" << index.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::take_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::index_select(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::index_select", 1);
  TF_VLOG(3) << "XLA index_select :" << " self=" << self.toString() << " index=" << index.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::index_select(external_tensors[0], dim, external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::index_select_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::index_select_out", 1);
  TF_VLOG(3) << "XLA index_select_out :" << " self=" << self.toString() << " index=" << index.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::index_select_outf(external_tensors[0], dim, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::masked_select(const at::Tensor & self, const at::Tensor & mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::masked_select", 1);
  TF_VLOG(3) << "XLA masked_select :" << " self=" << self.toString() << " mask=" << mask.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mask};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::masked_select(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::masked_select_out(const at::Tensor & self, const at::Tensor & mask, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::masked_select_out", 1);
  TF_VLOG(3) << "XLA masked_select_out :" << " self=" << self.toString() << " mask=" << mask.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, mask, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::masked_select_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::nonzero(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nonzero", 1);
  TF_VLOG(3) << "XLA nonzero :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::nonzero(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::nonzero_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nonzero_out", 1);
  TF_VLOG(3) << "XLA nonzero_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::nonzero_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::gather(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::gather", 1);
  TF_VLOG(3) << "XLA gather :" << " self=" << self.toString() << " index=" << index.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::gather(external_tensors[0], dim, external_tensors[1], sparse_grad);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::gather_out(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::gather_out", 1);
  TF_VLOG(3) << "XLA gather_out :" << " self=" << self.toString() << " index=" << index.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::gather_outf(external_tensors[0], dim, external_tensors[1], sparse_grad, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::addcmul(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::addcmul", 1);
  TF_VLOG(3) << "XLA addcmul :" << " self=" << self.toString() << " tensor1=" << tensor1.toString() << " tensor2=" << tensor2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, tensor1, tensor2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::addcmul(external_tensors[0], external_tensors[1], external_tensors[2], value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::addcmul_out(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::addcmul_out", 1);
  TF_VLOG(3) << "XLA addcmul_out :" << " self=" << self.toString() << " tensor1=" << tensor1.toString() << " tensor2=" << tensor2.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, tensor1, tensor2, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::addcmul_outf(external_tensors[0], external_tensors[1], external_tensors[2], value, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::lstsq(const at::Tensor & self, const at::Tensor & A) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lstsq", 1);
  TF_VLOG(3) << "XLA lstsq :" << " self=" << self.toString() << " A=" << A.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, A};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lstsq(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::lstsq_out(const at::Tensor & self, const at::Tensor & A, at::Tensor & X, at::Tensor & qr) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lstsq_out", 1);
  TF_VLOG(3) << "XLA lstsq_out :" << " self=" << self.toString() << " A=" << A.toString() << " X=" << X.toString() << " qr=" << qr.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, A, X, qr};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lstsq_outf(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {2, 3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(X, qr);
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::triangular_solve(const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::triangular_solve", 1);
  TF_VLOG(3) << "XLA triangular_solve :" << " self=" << self.toString() << " A=" << A.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, A};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::triangular_solve(external_tensors[0], external_tensors[1], upper, transpose, unitriangular);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::triangular_solve_out(const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular, at::Tensor & X, at::Tensor & M) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::triangular_solve_out", 1);
  TF_VLOG(3) << "XLA triangular_solve_out :" << " self=" << self.toString() << " A=" << A.toString() << " X=" << X.toString() << " M=" << M.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, A, X, M};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::triangular_solve_outf(external_tensors[0], external_tensors[1], upper, transpose, unitriangular, external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {2, 3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(X, M);
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::symeig(const at::Tensor & self, bool eigenvectors, bool upper) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::symeig", 1);
  TF_VLOG(3) << "XLA symeig :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::symeig(external_tensors[0], eigenvectors, upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::_symeig_helper(const at::Tensor & self, bool eigenvectors, bool upper) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_symeig_helper", 1);
  TF_VLOG(3) << "XLA _symeig_helper :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_symeig_helper(external_tensors[0], eigenvectors, upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::svd(const at::Tensor & self, bool some, bool compute_uv) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::svd", 1);
  TF_VLOG(3) << "XLA svd :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::svd(external_tensors[0], some, compute_uv);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::_svd_helper(const at::Tensor & self, bool some, bool compute_uv) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_svd_helper", 1);
  TF_VLOG(3) << "XLA _svd_helper :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_svd_helper(external_tensors[0], some, compute_uv);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::cholesky(const at::Tensor & self, bool upper) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cholesky", 1);
  TF_VLOG(3) << "XLA cholesky :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cholesky(external_tensors[0], upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::cholesky_out(const at::Tensor & self, bool upper, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cholesky_out", 1);
  TF_VLOG(3) << "XLA cholesky_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cholesky_outf(external_tensors[0], upper, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::_cholesky_solve_helper(const at::Tensor & self, const at::Tensor & A, bool upper) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_cholesky_solve_helper", 1);
  TF_VLOG(3) << "XLA _cholesky_solve_helper :" << " self=" << self.toString() << " A=" << A.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, A};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_cholesky_solve_helper(external_tensors[0], external_tensors[1], upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::_solve_helper(const at::Tensor & self, const at::Tensor & A) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_solve_helper", 1);
  TF_VLOG(3) << "XLA _solve_helper :" << " self=" << self.toString() << " A=" << A.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, A};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_solve_helper(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::cholesky_inverse(const at::Tensor & self, bool upper) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cholesky_inverse", 1);
  TF_VLOG(3) << "XLA cholesky_inverse :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cholesky_inverse(external_tensors[0], upper);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::cholesky_inverse_out(const at::Tensor & self, bool upper, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::cholesky_inverse_out", 1);
  TF_VLOG(3) << "XLA cholesky_inverse_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::cholesky_inverse_outf(external_tensors[0], upper, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::qr(const at::Tensor & self, bool some) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::qr", 1);
  TF_VLOG(3) << "XLA qr :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::qr(external_tensors[0], some);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::geqrf(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::geqrf", 1);
  TF_VLOG(3) << "XLA geqrf :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::geqrf(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::geqrf_out(const at::Tensor & self, at::Tensor & a, at::Tensor & tau) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::geqrf_out", 1);
  TF_VLOG(3) << "XLA geqrf_out :" << " self=" << self.toString() << " a=" << a.toString() << " tau=" << tau.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, a, tau};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::geqrf_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(a, tau);
}

at::Tensor AtenXlaTypeDefault::ormqr(const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ormqr", 1);
  TF_VLOG(3) << "XLA ormqr :" << " self=" << self.toString() << " input2=" << input2.toString() << " input3=" << input3.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, input2, input3};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ormqr(external_tensors[0], external_tensors[1], external_tensors[2], left, transpose);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::ormqr_out(const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ormqr_out", 1);
  TF_VLOG(3) << "XLA ormqr_out :" << " self=" << self.toString() << " input2=" << input2.toString() << " input3=" << input3.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, input2, input3, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ormqr_outf(external_tensors[0], external_tensors[1], external_tensors[2], left, transpose, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::_lu_with_info(const at::Tensor & self, bool pivot, bool check_errors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_lu_with_info", 1);
  TF_VLOG(3) << "XLA _lu_with_info :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_lu_with_info(external_tensors[0], pivot, check_errors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::lu_unpack(const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lu_unpack", 1);
  TF_VLOG(3) << "XLA lu_unpack :" << " LU_data=" << LU_data.toString() << " LU_pivots=" << LU_pivots.toString();
  std::vector<at::Tensor> external_tensors_tensors = {LU_data, LU_pivots};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lu_unpack(external_tensors[0], external_tensors[1], unpack_data, unpack_pivots);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(LU_data)), to_device_opt(std::get<1>(x_result), get_device_arg(LU_data)), to_device_opt(std::get<2>(x_result), get_device_arg(LU_data)));
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> AtenXlaTypeDefault::lu_unpack_out(const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots, at::Tensor & P, at::Tensor & L, at::Tensor & U) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lu_unpack_out", 1);
  TF_VLOG(3) << "XLA lu_unpack_out :" << " LU_data=" << LU_data.toString() << " LU_pivots=" << LU_pivots.toString() << " P=" << P.toString() << " L=" << L.toString() << " U=" << U.toString();
  std::vector<at::Tensor> external_tensors_tensors = {LU_data, LU_pivots, P, L, U};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lu_unpack_outf(external_tensors[0], external_tensors[1], unpack_data, unpack_pivots, external_tensors[2], external_tensors[3], external_tensors[4]);
  std::vector<size_t> external_tensors_update_indices = {2, 3, 4};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(P, L, U);
}

at::Tensor AtenXlaTypeDefault::multinomial(const at::Tensor & self, int64_t num_samples, bool replacement, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::multinomial", 1);
  TF_VLOG(3) << "XLA multinomial :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::multinomial(external_tensors[0], num_samples, replacement, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::multinomial_out(const at::Tensor & self, int64_t num_samples, bool replacement, c10::optional<at::Generator> generator, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::multinomial_out", 1);
  TF_VLOG(3) << "XLA multinomial_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::multinomial_outf(external_tensors[0], num_samples, replacement, generator, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::lgamma_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::lgamma_out", 1);
  TF_VLOG(3) << "XLA lgamma_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::lgamma_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::polygamma_out(int64_t n, const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::polygamma_out", 1);
  TF_VLOG(3) << "XLA polygamma_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::polygamma_outf(n, external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::erfinv(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::erfinv", 1);
  TF_VLOG(3) << "XLA erfinv :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::erfinv(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::erfinv_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::erfinv_out", 1);
  TF_VLOG(3) << "XLA erfinv_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::erfinv_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::i0_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::i0_out", 1);
  TF_VLOG(3) << "XLA i0_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::i0_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::sign(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sign", 1);
  TF_VLOG(3) << "XLA sign :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sign(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::sign_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sign_out", 1);
  TF_VLOG(3) << "XLA sign_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sign_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::signbit_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::signbit_out", 1);
  TF_VLOG(3) << "XLA signbit_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::signbit_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::atan2(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::atan2", 1);
  TF_VLOG(3) << "XLA atan2 :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::atan2(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::atan2_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::atan2_out", 1);
  TF_VLOG(3) << "XLA atan2_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::atan2_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::histc(const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::histc", 1);
  TF_VLOG(3) << "XLA histc :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::histc(external_tensors[0], bins, min, max);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::histc_out(const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::histc_out", 1);
  TF_VLOG(3) << "XLA histc_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::histc_outf(external_tensors[0], bins, min, max, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::hypot_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hypot_out", 1);
  TF_VLOG(3) << "XLA hypot_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hypot_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::igamma_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::igamma_out", 1);
  TF_VLOG(3) << "XLA igamma_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::igamma_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::igammac_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::igammac_out", 1);
  TF_VLOG(3) << "XLA igammac_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::igammac_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::nextafter_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nextafter_out", 1);
  TF_VLOG(3) << "XLA nextafter_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::nextafter_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::min(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::min", 1);
  TF_VLOG(3) << "XLA min :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::min(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::fmin_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fmin_out", 1);
  TF_VLOG(3) << "XLA fmin_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fmin_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::max(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max", 1);
  TF_VLOG(3) << "XLA max :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::fmax_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fmax_out", 1);
  TF_VLOG(3) << "XLA fmax_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fmax_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::maximum(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::maximum", 1);
  TF_VLOG(3) << "XLA maximum :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::maximum(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::maximum_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::maximum_out", 1);
  TF_VLOG(3) << "XLA maximum_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::maximum_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::minimum(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::minimum", 1);
  TF_VLOG(3) << "XLA minimum :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::minimum(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::minimum_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::minimum_out", 1);
  TF_VLOG(3) << "XLA minimum_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::minimum_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::sort(const at::Tensor & self, int64_t dim, bool descending) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sort", 1);
  TF_VLOG(3) << "XLA sort :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sort(external_tensors[0], dim, descending);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::sort_out(const at::Tensor & self, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sort_out", 1);
  TF_VLOG(3) << "XLA sort_out :" << " self=" << self.toString() << " values=" << values.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, values, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sort_outf(external_tensors[0], dim, descending, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::sort(const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sort", 1);
  TF_VLOG(3) << "XLA sort :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sort(external_tensors[0], stable, dim, descending);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::sort_out(const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sort_out", 1);
  TF_VLOG(3) << "XLA sort_out :" << " self=" << self.toString() << " values=" << values.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, values, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sort_outf(external_tensors[0], stable, dim, descending, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::topk(const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::topk", 1);
  TF_VLOG(3) << "XLA topk :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::topk(external_tensors[0], k, dim, largest, sorted);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::topk_out(const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted, at::Tensor & values, at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::topk_out", 1);
  TF_VLOG(3) << "XLA topk_out :" << " self=" << self.toString() << " values=" << values.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, values, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::topk_outf(external_tensors[0], k, dim, largest, sorted, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(values, indices);
}

at::Tensor AtenXlaTypeDefault::all(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::all", 1);
  TF_VLOG(3) << "XLA all :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::all(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::any(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::any", 1);
  TF_VLOG(3) << "XLA any :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::any(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::unfold(const at::Tensor & self, int64_t dimension, int64_t size, int64_t step) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::unfold", 1);
  TF_VLOG(3) << "XLA unfold :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].unfold(dimension, size, step);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::unfold_backward(const at::Tensor & grad_in, at::IntArrayRef input_sizes, int64_t dim, int64_t size, int64_t step) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::unfold_backward", 1);
  TF_VLOG(3) << "XLA unfold_backward :" << " grad_in=" << grad_in.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_in};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::unfold_backward(external_tensors[0], input_sizes, dim, size, step);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_in));
}

bool AtenXlaTypeDefault::equal(const at::Tensor & self, const at::Tensor & other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::equal", 1);
  TF_VLOG(3) << "XLA equal :" << " self=" << self.toString() << " other=" << other.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::equal(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return x_result;
}

at::Tensor AtenXlaTypeDefault::pow(const at::Tensor & self, const at::Tensor & exponent) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::pow", 1);
  TF_VLOG(3) << "XLA pow :" << " self=" << self.toString() << " exponent=" << exponent.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, exponent};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::pow(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::pow_out(const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::pow_out", 1);
  TF_VLOG(3) << "XLA pow_out :" << " self=" << self.toString() << " exponent=" << exponent.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, exponent, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::pow_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::pow(const at::Scalar & self, const at::Tensor & exponent) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::pow", 1);
  TF_VLOG(3) << "XLA pow :" << " exponent=" << exponent.toString();
  std::vector<at::Tensor> external_tensors_tensors = {exponent};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::pow(self, external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(exponent));
}

at::Tensor & AtenXlaTypeDefault::pow_out(const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::pow_out", 1);
  TF_VLOG(3) << "XLA pow_out :" << " exponent=" << exponent.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {exponent, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::pow_outf(self, external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::pow(const at::Tensor & self, const at::Scalar & exponent) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::pow", 1);
  TF_VLOG(3) << "XLA pow :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::pow(external_tensors[0], exponent);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::pow_out(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::pow_out", 1);
  TF_VLOG(3) << "XLA pow_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::pow_outf(external_tensors[0], exponent, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::normal_(at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::normal_", 1);
  TF_VLOG(3) << "XLA normal_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = external_tensors[0].normal_(mean, std, generator);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::normal(const at::Tensor & mean, double std, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::normal", 1);
  TF_VLOG(3) << "XLA normal :" << " mean=" << mean.toString();
  std::vector<at::Tensor> external_tensors_tensors = {mean};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::normal(external_tensors[0], std, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(mean));
}

at::Tensor & AtenXlaTypeDefault::normal_out(const at::Tensor & mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::normal_out", 1);
  TF_VLOG(3) << "XLA normal_out :" << " mean=" << mean.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {mean, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::normal_outf(external_tensors[0], std, generator, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::normal(double mean, const at::Tensor & std, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::normal", 1);
  TF_VLOG(3) << "XLA normal :" << " std=" << std.toString();
  std::vector<at::Tensor> external_tensors_tensors = {std};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::normal(mean, external_tensors[0], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(std));
}

at::Tensor & AtenXlaTypeDefault::normal_out(double mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::normal_out", 1);
  TF_VLOG(3) << "XLA normal_out :" << " std=" << std.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {std, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::normal_outf(mean, external_tensors[0], generator, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::normal(const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::normal", 1);
  TF_VLOG(3) << "XLA normal :" << " mean=" << mean.toString() << " std=" << std.toString();
  std::vector<at::Tensor> external_tensors_tensors = {mean, std};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::normal(external_tensors[0], external_tensors[1], generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(mean));
}

at::Tensor & AtenXlaTypeDefault::normal_out(const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::normal_out", 1);
  TF_VLOG(3) << "XLA normal_out :" << " mean=" << mean.toString() << " std=" << std.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {mean, std, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::normal_outf(external_tensors[0], external_tensors[1], generator, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::alias(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::alias", 1);
  TF_VLOG(3) << "XLA alias :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::alias(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::_index_copy_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_index_copy_", 1);
  TF_VLOG(3) << "XLA _index_copy_ :" << " self=" << self.toString() << " index=" << index.toString() << " source=" << source.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, index, source};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_index_copy_(external_tensors[0], dim, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::_cumsum(const at::Tensor & self, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_cumsum", 1);
  TF_VLOG(3) << "XLA _cumsum :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_cumsum(external_tensors[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::_cumsum_out(const at::Tensor & self, int64_t dim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_cumsum_out", 1);
  TF_VLOG(3) << "XLA _cumsum_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_cumsum_outf(external_tensors[0], dim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::_cumprod(const at::Tensor & self, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_cumprod", 1);
  TF_VLOG(3) << "XLA _cumprod :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_cumprod(external_tensors[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::_cumprod_out(const at::Tensor & self, int64_t dim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_cumprod_out", 1);
  TF_VLOG(3) << "XLA _cumprod_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_cumprod_outf(external_tensors[0], dim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

void AtenXlaTypeDefault::_amp_foreach_non_finite_check_and_unscale_(at::TensorList self, at::Tensor & found_inf, const at::Tensor & inv_scale) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_amp_foreach_non_finite_check_and_unscale_", 1);
  TF_VLOG(3) << "XLA _amp_foreach_non_finite_check_and_unscale_ :" << " found_inf=" << found_inf.toString() << " inv_scale=" << inv_scale.toString();
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {found_inf, inv_scale};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_amp_foreach_non_finite_check_and_unscale_(l_self, external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

}

at::Tensor & AtenXlaTypeDefault::_amp_update_scale_(at::Tensor & self, at::Tensor & growth_tracker, const at::Tensor & found_inf, double scale_growth_factor, double scale_backoff_factor, int64_t growth_interval) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_amp_update_scale_", 1);
  TF_VLOG(3) << "XLA _amp_update_scale_ :" << " self=" << self.toString() << " growth_tracker=" << growth_tracker.toString() << " found_inf=" << found_inf.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, growth_tracker, found_inf};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_amp_update_scale_(external_tensors[0], external_tensors[1], external_tensors[2], scale_growth_factor, scale_backoff_factor, growth_interval);
  std::vector<size_t> external_tensors_update_indices = {0, 1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::_cat(at::TensorList tensors, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_cat", 1);
  TF_VLOG(3) << "XLA _cat :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_cat(l_tensors, dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

at::Tensor & AtenXlaTypeDefault::_cat_out(at::TensorList tensors, int64_t dim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_cat_out", 1);
  TF_VLOG(3) << "XLA _cat_out :" << " out=" << out.toString();
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_cat_outf(l_tensors, dim, external_tensors[0]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_add(at::TensorList tensors, const at::Scalar & scalar) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_add", 1);
  TF_VLOG(3) << "XLA _foreach_add :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_add(l_tensors, scalar);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_add_(at::TensorList self, const at::Scalar & scalar) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_add_", 1);
  TF_VLOG(3) << "XLA _foreach_add_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_add_(l_self, scalar);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_sub(at::TensorList tensors, const at::Scalar & scalar) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sub", 1);
  TF_VLOG(3) << "XLA _foreach_sub :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_sub(l_tensors, scalar);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_sub_(at::TensorList self, const at::Scalar & scalar) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sub_", 1);
  TF_VLOG(3) << "XLA _foreach_sub_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_sub_(l_self, scalar);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_mul(at::TensorList tensors, const at::Scalar & scalar) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_mul", 1);
  TF_VLOG(3) << "XLA _foreach_mul :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_mul(l_tensors, scalar);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_mul_(at::TensorList self, const at::Scalar & scalar) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_mul_", 1);
  TF_VLOG(3) << "XLA _foreach_mul_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_mul_(l_self, scalar);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_div(at::TensorList tensors, const at::Scalar & scalar) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_div", 1);
  TF_VLOG(3) << "XLA _foreach_div :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_div(l_tensors, scalar);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_div_(at::TensorList self, const at::Scalar & scalar) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_div_", 1);
  TF_VLOG(3) << "XLA _foreach_div_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_div_(l_self, scalar);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_add(at::TensorList tensors1, at::TensorList tensors2, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_add", 1);
  TF_VLOG(3) << "XLA _foreach_add :";
  auto l_tensors1 = to_cpu(tensors1);
  auto l_tensors2 = to_cpu(tensors2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_add(l_tensors1, l_tensors2, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors1));
}

void AtenXlaTypeDefault::_foreach_add_(at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_add_", 1);
  TF_VLOG(3) << "XLA _foreach_add_ :";
  auto l_self = to_cpu(self);
  auto l_other = to_cpu(other);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_add_(l_self, l_other, alpha);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_sub(at::TensorList tensors1, at::TensorList tensors2, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sub", 1);
  TF_VLOG(3) << "XLA _foreach_sub :";
  auto l_tensors1 = to_cpu(tensors1);
  auto l_tensors2 = to_cpu(tensors2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_sub(l_tensors1, l_tensors2, alpha);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors1));
}

void AtenXlaTypeDefault::_foreach_sub_(at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sub_", 1);
  TF_VLOG(3) << "XLA _foreach_sub_ :";
  auto l_self = to_cpu(self);
  auto l_other = to_cpu(other);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_sub_(l_self, l_other, alpha);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_mul(at::TensorList tensors1, at::TensorList tensors2) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_mul", 1);
  TF_VLOG(3) << "XLA _foreach_mul :";
  auto l_tensors1 = to_cpu(tensors1);
  auto l_tensors2 = to_cpu(tensors2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_mul(l_tensors1, l_tensors2);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors1));
}

void AtenXlaTypeDefault::_foreach_mul_(at::TensorList self, at::TensorList other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_mul_", 1);
  TF_VLOG(3) << "XLA _foreach_mul_ :";
  auto l_self = to_cpu(self);
  auto l_other = to_cpu(other);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_mul_(l_self, l_other);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_div(at::TensorList tensors1, at::TensorList tensors2) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_div", 1);
  TF_VLOG(3) << "XLA _foreach_div :";
  auto l_tensors1 = to_cpu(tensors1);
  auto l_tensors2 = to_cpu(tensors2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_div(l_tensors1, l_tensors2);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors1));
}

void AtenXlaTypeDefault::_foreach_div_(at::TensorList self, at::TensorList other) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_div_", 1);
  TF_VLOG(3) << "XLA _foreach_div_ :";
  auto l_self = to_cpu(self);
  auto l_other = to_cpu(other);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_div_(l_self, l_other);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_add(at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_add", 1);
  TF_VLOG(3) << "XLA _foreach_add :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_add(l_tensors, scalars);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_add_(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_add_", 1);
  TF_VLOG(3) << "XLA _foreach_add_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_add_(l_self, scalars);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_sub(at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sub", 1);
  TF_VLOG(3) << "XLA _foreach_sub :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_sub(l_tensors, scalars);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_sub_(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sub_", 1);
  TF_VLOG(3) << "XLA _foreach_sub_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_sub_(l_self, scalars);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_div(at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_div", 1);
  TF_VLOG(3) << "XLA _foreach_div :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_div(l_tensors, scalars);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_div_(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_div_", 1);
  TF_VLOG(3) << "XLA _foreach_div_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_div_(l_self, scalars);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_mul(at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_mul", 1);
  TF_VLOG(3) << "XLA _foreach_mul :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_mul(l_tensors, scalars);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_mul_(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_mul_", 1);
  TF_VLOG(3) << "XLA _foreach_mul_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_mul_(l_self, scalars);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_exp(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_exp", 1);
  TF_VLOG(3) << "XLA _foreach_exp :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_exp(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_zero_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_zero_", 1);
  TF_VLOG(3) << "XLA _foreach_zero_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_zero_(l_self);
}

void AtenXlaTypeDefault::_foreach_exp_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_exp_", 1);
  TF_VLOG(3) << "XLA _foreach_exp_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_exp_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_sqrt(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sqrt", 1);
  TF_VLOG(3) << "XLA _foreach_sqrt :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_sqrt(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_sqrt_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sqrt_", 1);
  TF_VLOG(3) << "XLA _foreach_sqrt_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_sqrt_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_abs(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_abs", 1);
  TF_VLOG(3) << "XLA _foreach_abs :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_abs(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_abs_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_abs_", 1);
  TF_VLOG(3) << "XLA _foreach_abs_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_abs_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_acos(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_acos", 1);
  TF_VLOG(3) << "XLA _foreach_acos :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_acos(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_acos_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_acos_", 1);
  TF_VLOG(3) << "XLA _foreach_acos_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_acos_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_asin(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_asin", 1);
  TF_VLOG(3) << "XLA _foreach_asin :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_asin(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_asin_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_asin_", 1);
  TF_VLOG(3) << "XLA _foreach_asin_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_asin_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_atan(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_atan", 1);
  TF_VLOG(3) << "XLA _foreach_atan :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_atan(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_atan_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_atan_", 1);
  TF_VLOG(3) << "XLA _foreach_atan_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_atan_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_ceil(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_ceil", 1);
  TF_VLOG(3) << "XLA _foreach_ceil :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_ceil(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_ceil_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_ceil_", 1);
  TF_VLOG(3) << "XLA _foreach_ceil_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_ceil_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_cos(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_cos", 1);
  TF_VLOG(3) << "XLA _foreach_cos :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_cos(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_cos_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_cos_", 1);
  TF_VLOG(3) << "XLA _foreach_cos_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_cos_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_cosh(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_cosh", 1);
  TF_VLOG(3) << "XLA _foreach_cosh :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_cosh(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_cosh_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_cosh_", 1);
  TF_VLOG(3) << "XLA _foreach_cosh_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_cosh_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_erf(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_erf", 1);
  TF_VLOG(3) << "XLA _foreach_erf :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_erf(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_erf_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_erf_", 1);
  TF_VLOG(3) << "XLA _foreach_erf_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_erf_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_erfc(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_erfc", 1);
  TF_VLOG(3) << "XLA _foreach_erfc :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_erfc(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_erfc_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_erfc_", 1);
  TF_VLOG(3) << "XLA _foreach_erfc_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_erfc_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_expm1(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_expm1", 1);
  TF_VLOG(3) << "XLA _foreach_expm1 :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_expm1(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_expm1_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_expm1_", 1);
  TF_VLOG(3) << "XLA _foreach_expm1_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_expm1_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_floor(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_floor", 1);
  TF_VLOG(3) << "XLA _foreach_floor :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_floor(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_floor_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_floor_", 1);
  TF_VLOG(3) << "XLA _foreach_floor_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_floor_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_log(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_log", 1);
  TF_VLOG(3) << "XLA _foreach_log :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_log(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_log_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_log_", 1);
  TF_VLOG(3) << "XLA _foreach_log_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_log_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_log10(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_log10", 1);
  TF_VLOG(3) << "XLA _foreach_log10 :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_log10(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_log10_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_log10_", 1);
  TF_VLOG(3) << "XLA _foreach_log10_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_log10_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_log1p(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_log1p", 1);
  TF_VLOG(3) << "XLA _foreach_log1p :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_log1p(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_log1p_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_log1p_", 1);
  TF_VLOG(3) << "XLA _foreach_log1p_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_log1p_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_log2(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_log2", 1);
  TF_VLOG(3) << "XLA _foreach_log2 :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_log2(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_log2_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_log2_", 1);
  TF_VLOG(3) << "XLA _foreach_log2_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_log2_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_neg(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_neg", 1);
  TF_VLOG(3) << "XLA _foreach_neg :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_neg(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_neg_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_neg_", 1);
  TF_VLOG(3) << "XLA _foreach_neg_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_neg_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_tan(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_tan", 1);
  TF_VLOG(3) << "XLA _foreach_tan :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_tan(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_tan_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_tan_", 1);
  TF_VLOG(3) << "XLA _foreach_tan_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_tan_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_tanh(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_tanh", 1);
  TF_VLOG(3) << "XLA _foreach_tanh :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_tanh(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_tanh_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_tanh_", 1);
  TF_VLOG(3) << "XLA _foreach_tanh_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_tanh_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_sin(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sin", 1);
  TF_VLOG(3) << "XLA _foreach_sin :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_sin(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_sin_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sin_", 1);
  TF_VLOG(3) << "XLA _foreach_sin_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_sin_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_sinh(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sinh", 1);
  TF_VLOG(3) << "XLA _foreach_sinh :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_sinh(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_sinh_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sinh_", 1);
  TF_VLOG(3) << "XLA _foreach_sinh_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_sinh_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_round(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_round", 1);
  TF_VLOG(3) << "XLA _foreach_round :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_round(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_round_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_round_", 1);
  TF_VLOG(3) << "XLA _foreach_round_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_round_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_lgamma(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_lgamma", 1);
  TF_VLOG(3) << "XLA _foreach_lgamma :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_lgamma(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_lgamma_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_lgamma_", 1);
  TF_VLOG(3) << "XLA _foreach_lgamma_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_lgamma_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_frac(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_frac", 1);
  TF_VLOG(3) << "XLA _foreach_frac :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_frac(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_frac_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_frac_", 1);
  TF_VLOG(3) << "XLA _foreach_frac_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_frac_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_reciprocal(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_reciprocal", 1);
  TF_VLOG(3) << "XLA _foreach_reciprocal :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_reciprocal(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_reciprocal_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_reciprocal_", 1);
  TF_VLOG(3) << "XLA _foreach_reciprocal_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_reciprocal_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_sigmoid(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sigmoid", 1);
  TF_VLOG(3) << "XLA _foreach_sigmoid :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_sigmoid(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_sigmoid_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_sigmoid_", 1);
  TF_VLOG(3) << "XLA _foreach_sigmoid_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_sigmoid_(l_self);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_trunc(at::TensorList tensors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_trunc", 1);
  TF_VLOG(3) << "XLA _foreach_trunc :";
  auto l_tensors = to_cpu(tensors);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_trunc(l_tensors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors));
}

void AtenXlaTypeDefault::_foreach_trunc_(at::TensorList self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_trunc_", 1);
  TF_VLOG(3) << "XLA _foreach_trunc_ :";
  auto l_self = to_cpu(self);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_trunc_(l_self);
}

void AtenXlaTypeDefault::_foreach_addcdiv_(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_addcdiv_", 1);
  TF_VLOG(3) << "XLA _foreach_addcdiv_ :";
  auto l_self = to_cpu(self);
  auto l_tensor1 = to_cpu(tensor1);
  auto l_tensor2 = to_cpu(tensor2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_addcdiv_(l_self, l_tensor1, l_tensor2, value);
}

void AtenXlaTypeDefault::_foreach_addcmul_(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_addcmul_", 1);
  TF_VLOG(3) << "XLA _foreach_addcmul_ :";
  auto l_self = to_cpu(self);
  auto l_tensor1 = to_cpu(tensor1);
  auto l_tensor2 = to_cpu(tensor2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_addcmul_(l_self, l_tensor1, l_tensor2, value);
}

void AtenXlaTypeDefault::_foreach_addcdiv_(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_addcdiv_", 1);
  TF_VLOG(3) << "XLA _foreach_addcdiv_ :";
  auto l_self = to_cpu(self);
  auto l_tensor1 = to_cpu(tensor1);
  auto l_tensor2 = to_cpu(tensor2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_addcdiv_(l_self, l_tensor1, l_tensor2, scalars);
}

void AtenXlaTypeDefault::_foreach_addcmul_(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_addcmul_", 1);
  TF_VLOG(3) << "XLA _foreach_addcmul_ :";
  auto l_self = to_cpu(self);
  auto l_tensor1 = to_cpu(tensor1);
  auto l_tensor2 = to_cpu(tensor2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  at::_foreach_addcmul_(l_self, l_tensor1, l_tensor2, scalars);
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_addcdiv(at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_addcdiv", 1);
  TF_VLOG(3) << "XLA _foreach_addcdiv :";
  auto l_input = to_cpu(input);
  auto l_tensor1 = to_cpu(tensor1);
  auto l_tensor2 = to_cpu(tensor2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_addcdiv(l_input, l_tensor1, l_tensor2, value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_addcmul(at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_addcmul", 1);
  TF_VLOG(3) << "XLA _foreach_addcmul :";
  auto l_input = to_cpu(input);
  auto l_tensor1 = to_cpu(tensor1);
  auto l_tensor2 = to_cpu(tensor2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_addcmul(l_input, l_tensor1, l_tensor2, value);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_addcdiv(at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_addcdiv", 1);
  TF_VLOG(3) << "XLA _foreach_addcdiv :";
  auto l_input = to_cpu(input);
  auto l_tensor1 = to_cpu(tensor1);
  auto l_tensor2 = to_cpu(tensor2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_addcdiv(l_input, l_tensor1, l_tensor2, scalars);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_addcmul(at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_addcmul", 1);
  TF_VLOG(3) << "XLA _foreach_addcmul :";
  auto l_input = to_cpu(input);
  auto l_tensor1 = to_cpu(tensor1);
  auto l_tensor2 = to_cpu(tensor2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_addcmul(l_input, l_tensor1, l_tensor2, scalars);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_maximum(at::TensorList tensors1, at::TensorList tensors2) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_maximum", 1);
  TF_VLOG(3) << "XLA _foreach_maximum :";
  auto l_tensors1 = to_cpu(tensors1);
  auto l_tensors2 = to_cpu(tensors2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_maximum(l_tensors1, l_tensors2);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors1));
}

std::vector<at::Tensor> AtenXlaTypeDefault::_foreach_minimum(at::TensorList tensors1, at::TensorList tensors2) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_foreach_minimum", 1);
  TF_VLOG(3) << "XLA _foreach_minimum :";
  auto l_tensors1 = to_cpu(tensors1);
  auto l_tensors2 = to_cpu(tensors2);
  std::vector<at::Tensor> external_tensors_tensors = {};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_foreach_minimum(l_tensors1, l_tensors2);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(tensors1));
}

at::Tensor AtenXlaTypeDefault::bucketize(const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bucketize", 1);
  TF_VLOG(3) << "XLA bucketize :" << " self=" << self.toString() << " boundaries=" << boundaries.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, boundaries};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bucketize(external_tensors[0], external_tensors[1], out_int32, right);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::bucketize_out(const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bucketize_out", 1);
  TF_VLOG(3) << "XLA bucketize_out :" << " self=" << self.toString() << " boundaries=" << boundaries.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, boundaries, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bucketize_outf(external_tensors[0], external_tensors[1], out_int32, right, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::bucketize(const at::Scalar & self, const at::Tensor & boundaries, bool out_int32, bool right) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::bucketize", 1);
  TF_VLOG(3) << "XLA bucketize :" << " boundaries=" << boundaries.toString();
  std::vector<at::Tensor> external_tensors_tensors = {boundaries};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::bucketize(self, external_tensors[0], out_int32, right);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(boundaries));
}

at::Tensor AtenXlaTypeDefault::searchsorted(const at::Tensor & sorted_sequence, const at::Tensor & self, bool out_int32, bool right) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::searchsorted", 1);
  TF_VLOG(3) << "XLA searchsorted :" << " sorted_sequence=" << sorted_sequence.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {sorted_sequence, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::searchsorted(external_tensors[0], external_tensors[1], out_int32, right);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(sorted_sequence));
}

at::Tensor & AtenXlaTypeDefault::searchsorted_out(const at::Tensor & sorted_sequence, const at::Tensor & self, bool out_int32, bool right, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::searchsorted_out", 1);
  TF_VLOG(3) << "XLA searchsorted_out :" << " sorted_sequence=" << sorted_sequence.toString() << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {sorted_sequence, self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::searchsorted_outf(external_tensors[0], external_tensors[1], out_int32, right, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::searchsorted(const at::Tensor & sorted_sequence, const at::Scalar & self, bool out_int32, bool right) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::searchsorted", 1);
  TF_VLOG(3) << "XLA searchsorted :" << " sorted_sequence=" << sorted_sequence.toString();
  std::vector<at::Tensor> external_tensors_tensors = {sorted_sequence};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::searchsorted(external_tensors[0], self, out_int32, right);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(sorted_sequence));
}

at::Tensor AtenXlaTypeDefault::mse_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mse_loss", 1);
  TF_VLOG(3) << "XLA mse_loss :" << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mse_loss(external_tensors[0], external_tensors[1], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::mse_loss_out(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mse_loss_out", 1);
  TF_VLOG(3) << "XLA mse_loss_out :" << " self=" << self.toString() << " target=" << target.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mse_loss_outf(external_tensors[0], external_tensors[1], reduction, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::mse_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mse_loss_backward", 1);
  TF_VLOG(3) << "XLA mse_loss_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mse_loss_backward(external_tensors[0], external_tensors[1], external_tensors[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::mse_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mse_loss_backward_out", 1);
  TF_VLOG(3) << "XLA mse_loss_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mse_loss_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], reduction, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::l1_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::l1_loss", 1);
  TF_VLOG(3) << "XLA l1_loss :" << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::l1_loss(external_tensors[0], external_tensors[1], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::l1_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::l1_loss_backward", 1);
  TF_VLOG(3) << "XLA l1_loss_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::l1_loss_backward(external_tensors[0], external_tensors[1], external_tensors[2], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::l1_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::l1_loss_backward_out", 1);
  TF_VLOG(3) << "XLA l1_loss_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::l1_loss_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], reduction, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::multi_margin_loss(const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::multi_margin_loss", 1);
  TF_VLOG(3) << "XLA multi_margin_loss :" << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::multi_margin_loss(external_tensors[0], external_tensors[1], p, margin, external_tensors_opt[0], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::multi_margin_loss_out(const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::multi_margin_loss_out", 1);
  TF_VLOG(3) << "XLA multi_margin_loss_out :" << " self=" << self.toString() << " target=" << target.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::multi_margin_loss_outf(external_tensors[0], external_tensors[1], p, margin, external_tensors_opt[0], reduction, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::multi_margin_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::multi_margin_loss_backward", 1);
  TF_VLOG(3) << "XLA multi_margin_loss_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::multi_margin_loss_backward(external_tensors[0], external_tensors[1], external_tensors[2], p, margin, external_tensors_opt[0], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::multi_margin_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::multi_margin_loss_backward_out", 1);
  TF_VLOG(3) << "XLA multi_margin_loss_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::multi_margin_loss_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], p, margin, external_tensors_opt[0], reduction, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::multilabel_margin_loss_forward(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::multilabel_margin_loss_forward", 1);
  TF_VLOG(3) << "XLA multilabel_margin_loss_forward :" << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::multilabel_margin_loss_forward(external_tensors[0], external_tensors[1], reduction);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::multilabel_margin_loss_forward_out(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & output, at::Tensor & is_target) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::multilabel_margin_loss_forward_out", 1);
  TF_VLOG(3) << "XLA multilabel_margin_loss_forward_out :" << " self=" << self.toString() << " target=" << target.toString() << " output=" << output.toString() << " is_target=" << is_target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target, output, is_target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::multilabel_margin_loss_forward_outf(external_tensors[0], external_tensors[1], reduction, external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {2, 3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(output, is_target);
}

at::Tensor AtenXlaTypeDefault::multilabel_margin_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, const at::Tensor & is_target) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::multilabel_margin_loss_backward", 1);
  TF_VLOG(3) << "XLA multilabel_margin_loss_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString() << " is_target=" << is_target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target, is_target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::multilabel_margin_loss_backward(external_tensors[0], external_tensors[1], external_tensors[2], reduction, external_tensors[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::multilabel_margin_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, const at::Tensor & is_target, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::multilabel_margin_loss_backward_out", 1);
  TF_VLOG(3) << "XLA multilabel_margin_loss_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString() << " is_target=" << is_target.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target, is_target, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::multilabel_margin_loss_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], reduction, external_tensors[3], external_tensors[4]);
  std::vector<size_t> external_tensors_update_indices = {4};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::nll_loss_forward(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nll_loss_forward", 1);
  TF_VLOG(3) << "XLA nll_loss_forward :" << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::nll_loss_forward(external_tensors[0], external_tensors[1], external_tensors_opt[0], reduction, ignore_index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::nll_loss_forward_out(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & output, at::Tensor & total_weight) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nll_loss_forward_out", 1);
  TF_VLOG(3) << "XLA nll_loss_forward_out :" << " self=" << self.toString() << " target=" << target.toString() << " output=" << output.toString() << " total_weight=" << total_weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target, output, total_weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::nll_loss_forward_outf(external_tensors[0], external_tensors[1], external_tensors_opt[0], reduction, ignore_index, external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {2, 3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(output, total_weight);
}

at::Tensor AtenXlaTypeDefault::nll_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nll_loss_backward", 1);
  TF_VLOG(3) << "XLA nll_loss_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString() << " total_weight=" << total_weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target, total_weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::nll_loss_backward(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors_opt[0], reduction, ignore_index, external_tensors[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::nll_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nll_loss_backward_out", 1);
  TF_VLOG(3) << "XLA nll_loss_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString() << " total_weight=" << total_weight.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target, total_weight, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::nll_loss_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors_opt[0], reduction, ignore_index, external_tensors[3], external_tensors[4]);
  std::vector<size_t> external_tensors_update_indices = {4};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::nll_loss2d_forward(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nll_loss2d_forward", 1);
  TF_VLOG(3) << "XLA nll_loss2d_forward :" << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::nll_loss2d_forward(external_tensors[0], external_tensors[1], external_tensors_opt[0], reduction, ignore_index);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::nll_loss2d_forward_out(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & output, at::Tensor & total_weight) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nll_loss2d_forward_out", 1);
  TF_VLOG(3) << "XLA nll_loss2d_forward_out :" << " self=" << self.toString() << " target=" << target.toString() << " output=" << output.toString() << " total_weight=" << total_weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target, output, total_weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::nll_loss2d_forward_outf(external_tensors[0], external_tensors[1], external_tensors_opt[0], reduction, ignore_index, external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {2, 3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(output, total_weight);
}

at::Tensor AtenXlaTypeDefault::nll_loss2d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nll_loss2d_backward", 1);
  TF_VLOG(3) << "XLA nll_loss2d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString() << " total_weight=" << total_weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target, total_weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::nll_loss2d_backward(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors_opt[0], reduction, ignore_index, external_tensors[3]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::nll_loss2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::nll_loss2d_backward_out", 1);
  TF_VLOG(3) << "XLA nll_loss2d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString() << " total_weight=" << total_weight.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target, total_weight, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {weight};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::nll_loss2d_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors_opt[0], reduction, ignore_index, external_tensors[3], external_tensors[4]);
  std::vector<size_t> external_tensors_update_indices = {4};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::smooth_l1_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::smooth_l1_loss", 1);
  TF_VLOG(3) << "XLA smooth_l1_loss :" << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::smooth_l1_loss(external_tensors[0], external_tensors[1], reduction, beta);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::smooth_l1_loss_out(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::smooth_l1_loss_out", 1);
  TF_VLOG(3) << "XLA smooth_l1_loss_out :" << " self=" << self.toString() << " target=" << target.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::smooth_l1_loss_outf(external_tensors[0], external_tensors[1], reduction, beta, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::smooth_l1_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::smooth_l1_loss_backward", 1);
  TF_VLOG(3) << "XLA smooth_l1_loss_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::smooth_l1_loss_backward(external_tensors[0], external_tensors[1], external_tensors[2], reduction, beta);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::smooth_l1_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::smooth_l1_loss_backward_out", 1);
  TF_VLOG(3) << "XLA smooth_l1_loss_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::smooth_l1_loss_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], reduction, beta, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::huber_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::huber_loss", 1);
  TF_VLOG(3) << "XLA huber_loss :" << " self=" << self.toString() << " target=" << target.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::huber_loss(external_tensors[0], external_tensors[1], reduction, delta);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::huber_loss_out(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::huber_loss_out", 1);
  TF_VLOG(3) << "XLA huber_loss_out :" << " self=" << self.toString() << " target=" << target.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, target, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::huber_loss_outf(external_tensors[0], external_tensors[1], reduction, delta, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::huber_loss_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::huber_loss_backward_out", 1);
  TF_VLOG(3) << "XLA huber_loss_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " target=" << target.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, target, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::huber_loss_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], reduction, delta, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::elu(const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::elu", 1);
  TF_VLOG(3) << "XLA elu :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::elu(external_tensors[0], alpha, scale, input_scale);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::elu_out(const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::elu_out", 1);
  TF_VLOG(3) << "XLA elu_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::elu_outf(external_tensors[0], alpha, scale, input_scale, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::elu_(at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::elu_", 1);
  TF_VLOG(3) << "XLA elu_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::elu_(external_tensors[0], alpha, scale, input_scale);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::elu_backward(const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::elu_backward", 1);
  TF_VLOG(3) << "XLA elu_backward :" << " grad_output=" << grad_output.toString() << " self_or_result=" << self_or_result.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self_or_result};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::elu_backward(external_tensors[0], alpha, scale, input_scale, is_result, external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::elu_backward_out(const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::elu_backward_out", 1);
  TF_VLOG(3) << "XLA elu_backward_out :" << " grad_output=" << grad_output.toString() << " self_or_result=" << self_or_result.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self_or_result, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::elu_backward_outf(external_tensors[0], alpha, scale, input_scale, is_result, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::glu(const at::Tensor & self, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::glu", 1);
  TF_VLOG(3) << "XLA glu :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::glu(external_tensors[0], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::glu_out(const at::Tensor & self, int64_t dim, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::glu_out", 1);
  TF_VLOG(3) << "XLA glu_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::glu_outf(external_tensors[0], dim, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::glu_backward(const at::Tensor & grad_output, const at::Tensor & self, int64_t dim) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::glu_backward", 1);
  TF_VLOG(3) << "XLA glu_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::glu_backward(external_tensors[0], external_tensors[1], dim);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::glu_backward_out(const at::Tensor & grad_output, const at::Tensor & self, int64_t dim, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::glu_backward_out", 1);
  TF_VLOG(3) << "XLA glu_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::glu_backward_outf(external_tensors[0], external_tensors[1], dim, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::hardsigmoid(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardsigmoid", 1);
  TF_VLOG(3) << "XLA hardsigmoid :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardsigmoid(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::hardsigmoid_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardsigmoid_out", 1);
  TF_VLOG(3) << "XLA hardsigmoid_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardsigmoid_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::hardsigmoid_backward(const at::Tensor & grad_output, const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardsigmoid_backward", 1);
  TF_VLOG(3) << "XLA hardsigmoid_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardsigmoid_backward(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::hardsigmoid_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardsigmoid_backward_out", 1);
  TF_VLOG(3) << "XLA hardsigmoid_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardsigmoid_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::hardtanh(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardtanh", 1);
  TF_VLOG(3) << "XLA hardtanh :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardtanh(external_tensors[0], min_val, max_val);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::hardtanh_out(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardtanh_out", 1);
  TF_VLOG(3) << "XLA hardtanh_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardtanh_outf(external_tensors[0], min_val, max_val, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::hardtanh_(at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardtanh_", 1);
  TF_VLOG(3) << "XLA hardtanh_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardtanh_(external_tensors[0], min_val, max_val);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::hardtanh_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardtanh_backward", 1);
  TF_VLOG(3) << "XLA hardtanh_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardtanh_backward(external_tensors[0], external_tensors[1], min_val, max_val);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::hardtanh_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardtanh_backward_out", 1);
  TF_VLOG(3) << "XLA hardtanh_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardtanh_backward_outf(external_tensors[0], external_tensors[1], min_val, max_val, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::hardswish(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardswish", 1);
  TF_VLOG(3) << "XLA hardswish :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardswish(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::hardswish_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardswish_out", 1);
  TF_VLOG(3) << "XLA hardswish_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardswish_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::hardswish_(at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardswish_", 1);
  TF_VLOG(3) << "XLA hardswish_ :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardswish_(external_tensors[0]);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::hardswish_backward(const at::Tensor & grad_output, const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::hardswish_backward", 1);
  TF_VLOG(3) << "XLA hardswish_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::hardswish_backward(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::leaky_relu(const at::Tensor & self, const at::Scalar & negative_slope) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::leaky_relu", 1);
  TF_VLOG(3) << "XLA leaky_relu :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::leaky_relu(external_tensors[0], negative_slope);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::leaky_relu_out(const at::Tensor & self, const at::Scalar & negative_slope, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::leaky_relu_out", 1);
  TF_VLOG(3) << "XLA leaky_relu_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::leaky_relu_outf(external_tensors[0], negative_slope, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::leaky_relu_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::leaky_relu_backward", 1);
  TF_VLOG(3) << "XLA leaky_relu_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::leaky_relu_backward(external_tensors[0], external_tensors[1], negative_slope, self_is_result);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::leaky_relu_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::leaky_relu_backward_out", 1);
  TF_VLOG(3) << "XLA leaky_relu_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::leaky_relu_backward_outf(external_tensors[0], external_tensors[1], negative_slope, self_is_result, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::log_sigmoid_forward(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log_sigmoid_forward", 1);
  TF_VLOG(3) << "XLA log_sigmoid_forward :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::log_sigmoid_forward(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::log_sigmoid_forward_out(const at::Tensor & self, at::Tensor & output, at::Tensor & buffer) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log_sigmoid_forward_out", 1);
  TF_VLOG(3) << "XLA log_sigmoid_forward_out :" << " self=" << self.toString() << " output=" << output.toString() << " buffer=" << buffer.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, output, buffer};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::log_sigmoid_forward_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(output, buffer);
}

at::Tensor AtenXlaTypeDefault::log_sigmoid_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log_sigmoid_backward", 1);
  TF_VLOG(3) << "XLA log_sigmoid_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " buffer=" << buffer.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, buffer};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::log_sigmoid_backward(external_tensors[0], external_tensors[1], external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::log_sigmoid_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::log_sigmoid_backward_out", 1);
  TF_VLOG(3) << "XLA log_sigmoid_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " buffer=" << buffer.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, buffer, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::log_sigmoid_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::rrelu_with_noise(const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::rrelu_with_noise", 1);
  TF_VLOG(3) << "XLA rrelu_with_noise :" << " self=" << self.toString() << " noise=" << noise.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, noise};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::rrelu_with_noise(external_tensors[0], external_tensors[1], lower, upper, training, generator);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::rrelu_with_noise_out(const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::rrelu_with_noise_out", 1);
  TF_VLOG(3) << "XLA rrelu_with_noise_out :" << " self=" << self.toString() << " noise=" << noise.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, noise, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::rrelu_with_noise_outf(external_tensors[0], external_tensors[1], lower, upper, training, generator, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::rrelu_with_noise_(at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::rrelu_with_noise_", 1);
  TF_VLOG(3) << "XLA rrelu_with_noise_ :" << " self=" << self.toString() << " noise=" << noise.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, noise};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::rrelu_with_noise_(external_tensors[0], external_tensors[1], lower, upper, training, generator);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::rrelu_with_noise_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, bool self_is_result) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::rrelu_with_noise_backward", 1);
  TF_VLOG(3) << "XLA rrelu_with_noise_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " noise=" << noise.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, noise};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::rrelu_with_noise_backward(external_tensors[0], external_tensors[1], external_tensors[2], lower, upper, training, self_is_result);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::softplus(const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::softplus", 1);
  TF_VLOG(3) << "XLA softplus :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::softplus(external_tensors[0], beta, threshold);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::softplus_out(const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::softplus_out", 1);
  TF_VLOG(3) << "XLA softplus_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::softplus_outf(external_tensors[0], beta, threshold, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::softplus_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, const at::Tensor & output) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::softplus_backward", 1);
  TF_VLOG(3) << "XLA softplus_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " output=" << output.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, output};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::softplus_backward(external_tensors[0], external_tensors[1], beta, threshold, external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::softplus_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, const at::Tensor & output, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::softplus_backward_out", 1);
  TF_VLOG(3) << "XLA softplus_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " output=" << output.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, output, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::softplus_backward_outf(external_tensors[0], external_tensors[1], beta, threshold, external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::softshrink(const at::Tensor & self, const at::Scalar & lambd) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::softshrink", 1);
  TF_VLOG(3) << "XLA softshrink :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::softshrink(external_tensors[0], lambd);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::softshrink_out(const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::softshrink_out", 1);
  TF_VLOG(3) << "XLA softshrink_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::softshrink_outf(external_tensors[0], lambd, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::softshrink_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::softshrink_backward", 1);
  TF_VLOG(3) << "XLA softshrink_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::softshrink_backward(external_tensors[0], external_tensors[1], lambd);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::softshrink_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::softshrink_backward_out", 1);
  TF_VLOG(3) << "XLA softshrink_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::softshrink_backward_outf(external_tensors[0], external_tensors[1], lambd, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor & AtenXlaTypeDefault::adaptive_avg_pool2d_out(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::adaptive_avg_pool2d_out", 1);
  TF_VLOG(3) << "XLA adaptive_avg_pool2d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::adaptive_avg_pool2d_outf(external_tensors[0], output_size, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::mkldnn_adaptive_avg_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mkldnn_adaptive_avg_pool2d", 1);
  TF_VLOG(3) << "XLA mkldnn_adaptive_avg_pool2d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mkldnn_adaptive_avg_pool2d(external_tensors[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::mkldnn_adaptive_avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::mkldnn_adaptive_avg_pool2d_backward", 1);
  TF_VLOG(3) << "XLA mkldnn_adaptive_avg_pool2d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::mkldnn_adaptive_avg_pool2d_backward(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::_adaptive_avg_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_adaptive_avg_pool2d", 1);
  TF_VLOG(3) << "XLA _adaptive_avg_pool2d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_adaptive_avg_pool2d(external_tensors[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::_adaptive_avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_adaptive_avg_pool2d_backward", 1);
  TF_VLOG(3) << "XLA _adaptive_avg_pool2d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_adaptive_avg_pool2d_backward(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::adaptive_avg_pool3d_out(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::adaptive_avg_pool3d_out", 1);
  TF_VLOG(3) << "XLA adaptive_avg_pool3d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::adaptive_avg_pool3d_outf(external_tensors[0], output_size, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::_adaptive_avg_pool3d(const at::Tensor & self, at::IntArrayRef output_size) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_adaptive_avg_pool3d", 1);
  TF_VLOG(3) << "XLA _adaptive_avg_pool3d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_adaptive_avg_pool3d(external_tensors[0], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::adaptive_avg_pool3d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::adaptive_avg_pool3d_backward_out", 1);
  TF_VLOG(3) << "XLA adaptive_avg_pool3d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::adaptive_avg_pool3d_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::_adaptive_avg_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_adaptive_avg_pool3d_backward", 1);
  TF_VLOG(3) << "XLA _adaptive_avg_pool3d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_adaptive_avg_pool3d_backward(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::adaptive_max_pool2d_out(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::adaptive_max_pool2d_out", 1);
  TF_VLOG(3) << "XLA adaptive_max_pool2d_out :" << " self=" << self.toString() << " out=" << out.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::adaptive_max_pool2d_outf(external_tensors[0], output_size, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(out, indices);
}

at::Tensor & AtenXlaTypeDefault::adaptive_max_pool2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::adaptive_max_pool2d_backward_out", 1);
  TF_VLOG(3) << "XLA adaptive_max_pool2d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::adaptive_max_pool2d_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::adaptive_max_pool3d_out(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::adaptive_max_pool3d_out", 1);
  TF_VLOG(3) << "XLA adaptive_max_pool3d_out :" << " self=" << self.toString() << " out=" << out.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::adaptive_max_pool3d_outf(external_tensors[0], output_size, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(out, indices);
}

at::Tensor & AtenXlaTypeDefault::adaptive_max_pool3d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::adaptive_max_pool3d_backward_out", 1);
  TF_VLOG(3) << "XLA adaptive_max_pool3d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::adaptive_max_pool3d_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::avg_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::avg_pool2d", 1);
  TF_VLOG(3) << "XLA avg_pool2d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::avg_pool2d(external_tensors[0], kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::avg_pool2d_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::avg_pool2d_out", 1);
  TF_VLOG(3) << "XLA avg_pool2d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::avg_pool2d_outf(external_tensors[0], kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::avg_pool2d_backward", 1);
  TF_VLOG(3) << "XLA avg_pool2d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::avg_pool2d_backward(external_tensors[0], external_tensors[1], kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::avg_pool2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::avg_pool2d_backward_out", 1);
  TF_VLOG(3) << "XLA avg_pool2d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::avg_pool2d_backward_outf(external_tensors[0], external_tensors[1], kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::avg_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::avg_pool3d", 1);
  TF_VLOG(3) << "XLA avg_pool3d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::avg_pool3d(external_tensors[0], kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::avg_pool3d_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::avg_pool3d_out", 1);
  TF_VLOG(3) << "XLA avg_pool3d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::avg_pool3d_outf(external_tensors[0], kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::avg_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::avg_pool3d_backward", 1);
  TF_VLOG(3) << "XLA avg_pool3d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::avg_pool3d_backward(external_tensors[0], external_tensors[1], kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::avg_pool3d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::avg_pool3d_backward_out", 1);
  TF_VLOG(3) << "XLA avg_pool3d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::avg_pool3d_backward_outf(external_tensors[0], external_tensors[1], kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::fractional_max_pool2d_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fractional_max_pool2d_out", 1);
  TF_VLOG(3) << "XLA fractional_max_pool2d_out :" << " self=" << self.toString() << " random_samples=" << random_samples.toString() << " output=" << output.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, random_samples, output, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fractional_max_pool2d_outf(external_tensors[0], kernel_size, output_size, external_tensors[1], external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {2, 3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(output, indices);
}

at::Tensor AtenXlaTypeDefault::fractional_max_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fractional_max_pool2d_backward", 1);
  TF_VLOG(3) << "XLA fractional_max_pool2d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fractional_max_pool2d_backward(external_tensors[0], external_tensors[1], kernel_size, output_size, external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::fractional_max_pool2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fractional_max_pool2d_backward_out", 1);
  TF_VLOG(3) << "XLA fractional_max_pool2d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fractional_max_pool2d_backward_outf(external_tensors[0], external_tensors[1], kernel_size, output_size, external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::fractional_max_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fractional_max_pool3d", 1);
  TF_VLOG(3) << "XLA fractional_max_pool3d :" << " self=" << self.toString() << " random_samples=" << random_samples.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, random_samples};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fractional_max_pool3d(external_tensors[0], kernel_size, output_size, external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::fractional_max_pool3d_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fractional_max_pool3d_out", 1);
  TF_VLOG(3) << "XLA fractional_max_pool3d_out :" << " self=" << self.toString() << " random_samples=" << random_samples.toString() << " output=" << output.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, random_samples, output, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fractional_max_pool3d_outf(external_tensors[0], kernel_size, output_size, external_tensors[1], external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {2, 3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(output, indices);
}

at::Tensor AtenXlaTypeDefault::fractional_max_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fractional_max_pool3d_backward", 1);
  TF_VLOG(3) << "XLA fractional_max_pool3d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fractional_max_pool3d_backward(external_tensors[0], external_tensors[1], kernel_size, output_size, external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::fractional_max_pool3d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::fractional_max_pool3d_backward_out", 1);
  TF_VLOG(3) << "XLA fractional_max_pool3d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::fractional_max_pool3d_backward_outf(external_tensors[0], external_tensors[1], kernel_size, output_size, external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::max_pool2d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_pool2d_with_indices", 1);
  TF_VLOG(3) << "XLA max_pool2d_with_indices :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_pool2d_with_indices(external_tensors[0], kernel_size, stride, padding, dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::max_pool2d_with_indices_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_pool2d_with_indices_out", 1);
  TF_VLOG(3) << "XLA max_pool2d_with_indices_out :" << " self=" << self.toString() << " out=" << out.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_pool2d_with_indices_outf(external_tensors[0], kernel_size, stride, padding, dilation, ceil_mode, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(out, indices);
}

at::Tensor AtenXlaTypeDefault::max_pool2d_with_indices_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_pool2d_with_indices_backward", 1);
  TF_VLOG(3) << "XLA max_pool2d_with_indices_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_pool2d_with_indices_backward(external_tensors[0], external_tensors[1], kernel_size, stride, padding, dilation, ceil_mode, external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::max_pool2d_with_indices_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_pool2d_with_indices_backward_out", 1);
  TF_VLOG(3) << "XLA max_pool2d_with_indices_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_pool2d_with_indices_backward_outf(external_tensors[0], external_tensors[1], kernel_size, stride, padding, dilation, ceil_mode, external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::max_pool3d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_pool3d_with_indices", 1);
  TF_VLOG(3) << "XLA max_pool3d_with_indices :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_pool3d_with_indices(external_tensors[0], kernel_size, stride, padding, dilation, ceil_mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::max_pool3d_with_indices_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_pool3d_with_indices_out", 1);
  TF_VLOG(3) << "XLA max_pool3d_with_indices_out :" << " self=" << self.toString() << " out=" << out.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_pool3d_with_indices_outf(external_tensors[0], kernel_size, stride, padding, dilation, ceil_mode, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(out, indices);
}

at::Tensor AtenXlaTypeDefault::max_pool3d_with_indices_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_pool3d_with_indices_backward", 1);
  TF_VLOG(3) << "XLA max_pool3d_with_indices_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_pool3d_with_indices_backward(external_tensors[0], external_tensors[1], kernel_size, stride, padding, dilation, ceil_mode, external_tensors[2]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::max_pool3d_with_indices_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_pool3d_with_indices_backward_out", 1);
  TF_VLOG(3) << "XLA max_pool3d_with_indices_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_pool3d_with_indices_backward_outf(external_tensors[0], external_tensors[1], kernel_size, stride, padding, dilation, ceil_mode, external_tensors[2], external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::max_unpool2d(const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_unpool2d", 1);
  TF_VLOG(3) << "XLA max_unpool2d :" << " self=" << self.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_unpool2d(external_tensors[0], external_tensors[1], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::max_unpool2d_out(const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_unpool2d_out", 1);
  TF_VLOG(3) << "XLA max_unpool2d_out :" << " self=" << self.toString() << " indices=" << indices.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, indices, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_unpool2d_outf(external_tensors[0], external_tensors[1], output_size, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::max_unpool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_unpool2d_backward", 1);
  TF_VLOG(3) << "XLA max_unpool2d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_unpool2d_backward(external_tensors[0], external_tensors[1], external_tensors[2], output_size);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::max_unpool2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_unpool2d_backward_out", 1);
  TF_VLOG(3) << "XLA max_unpool2d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_unpool2d_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], output_size, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::max_unpool3d(const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_unpool3d", 1);
  TF_VLOG(3) << "XLA max_unpool3d :" << " self=" << self.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_unpool3d(external_tensors[0], external_tensors[1], output_size, stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::max_unpool3d_out(const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_unpool3d_out", 1);
  TF_VLOG(3) << "XLA max_unpool3d_out :" << " self=" << self.toString() << " indices=" << indices.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, indices, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_unpool3d_outf(external_tensors[0], external_tensors[1], output_size, stride, padding, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::max_unpool3d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_unpool3d_backward", 1);
  TF_VLOG(3) << "XLA max_unpool3d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_unpool3d_backward(external_tensors[0], external_tensors[1], external_tensors[2], output_size, stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::max_unpool3d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::max_unpool3d_backward_out", 1);
  TF_VLOG(3) << "XLA max_unpool3d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " indices=" << indices.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, indices, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::max_unpool3d_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], output_size, stride, padding, external_tensors[3]);
  std::vector<size_t> external_tensors_update_indices = {3};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor & AtenXlaTypeDefault::reflection_pad1d_out(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::reflection_pad1d_out", 1);
  TF_VLOG(3) << "XLA reflection_pad1d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::reflection_pad1d_outf(external_tensors[0], padding, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::reflection_pad1d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::reflection_pad1d_backward", 1);
  TF_VLOG(3) << "XLA reflection_pad1d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::reflection_pad1d_backward(external_tensors[0], external_tensors[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::reflection_pad1d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::reflection_pad1d_backward_out", 1);
  TF_VLOG(3) << "XLA reflection_pad1d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::reflection_pad1d_backward_outf(external_tensors[0], external_tensors[1], padding, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::reflection_pad2d(const at::Tensor & self, at::IntArrayRef padding) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::reflection_pad2d", 1);
  TF_VLOG(3) << "XLA reflection_pad2d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::reflection_pad2d(external_tensors[0], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::reflection_pad2d_out(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::reflection_pad2d_out", 1);
  TF_VLOG(3) << "XLA reflection_pad2d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::reflection_pad2d_outf(external_tensors[0], padding, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::reflection_pad2d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::reflection_pad2d_backward", 1);
  TF_VLOG(3) << "XLA reflection_pad2d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::reflection_pad2d_backward(external_tensors[0], external_tensors[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::reflection_pad2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::reflection_pad2d_backward_out", 1);
  TF_VLOG(3) << "XLA reflection_pad2d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::reflection_pad2d_backward_outf(external_tensors[0], external_tensors[1], padding, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::replication_pad1d(const at::Tensor & self, at::IntArrayRef padding) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::replication_pad1d", 1);
  TF_VLOG(3) << "XLA replication_pad1d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::replication_pad1d(external_tensors[0], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::replication_pad1d_out(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::replication_pad1d_out", 1);
  TF_VLOG(3) << "XLA replication_pad1d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::replication_pad1d_outf(external_tensors[0], padding, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::replication_pad1d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::replication_pad1d_backward", 1);
  TF_VLOG(3) << "XLA replication_pad1d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::replication_pad1d_backward(external_tensors[0], external_tensors[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::replication_pad1d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::replication_pad1d_backward_out", 1);
  TF_VLOG(3) << "XLA replication_pad1d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::replication_pad1d_backward_outf(external_tensors[0], external_tensors[1], padding, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::replication_pad2d(const at::Tensor & self, at::IntArrayRef padding) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::replication_pad2d", 1);
  TF_VLOG(3) << "XLA replication_pad2d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::replication_pad2d(external_tensors[0], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::replication_pad2d_out(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::replication_pad2d_out", 1);
  TF_VLOG(3) << "XLA replication_pad2d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::replication_pad2d_outf(external_tensors[0], padding, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::replication_pad2d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::replication_pad2d_backward", 1);
  TF_VLOG(3) << "XLA replication_pad2d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::replication_pad2d_backward(external_tensors[0], external_tensors[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::replication_pad2d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::replication_pad2d_backward_out", 1);
  TF_VLOG(3) << "XLA replication_pad2d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::replication_pad2d_backward_outf(external_tensors[0], external_tensors[1], padding, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor & AtenXlaTypeDefault::replication_pad3d_out(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::replication_pad3d_out", 1);
  TF_VLOG(3) << "XLA replication_pad3d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::replication_pad3d_outf(external_tensors[0], padding, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::replication_pad3d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::replication_pad3d_backward", 1);
  TF_VLOG(3) << "XLA replication_pad3d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::replication_pad3d_backward(external_tensors[0], external_tensors[1], padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::replication_pad3d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::replication_pad3d_backward_out", 1);
  TF_VLOG(3) << "XLA replication_pad3d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::replication_pad3d_backward_outf(external_tensors[0], external_tensors[1], padding, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::upsample_nearest2d(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_nearest2d", 1);
  TF_VLOG(3) << "XLA upsample_nearest2d :" << " input=" << input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_nearest2d(external_tensors[0], output_size, scale_factors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

at::Tensor AtenXlaTypeDefault::upsample_nearest2d_backward(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_nearest2d_backward", 1);
  TF_VLOG(3) << "XLA upsample_nearest2d_backward :" << " grad_output=" << grad_output.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_nearest2d_backward(external_tensors[0], output_size, input_size, scale_factors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor AtenXlaTypeDefault::upsample_nearest3d(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_nearest3d", 1);
  TF_VLOG(3) << "XLA upsample_nearest3d :" << " input=" << input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_nearest3d(external_tensors[0], output_size, scale_factors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

at::Tensor AtenXlaTypeDefault::upsample_nearest3d_backward(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_nearest3d_backward", 1);
  TF_VLOG(3) << "XLA upsample_nearest3d_backward :" << " grad_output=" << grad_output.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_nearest3d_backward(external_tensors[0], output_size, input_size, scale_factors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::upsample_linear1d_out(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_linear1d_out", 1);
  TF_VLOG(3) << "XLA upsample_linear1d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_linear1d_outf(external_tensors[0], output_size, align_corners, scales, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::upsample_linear1d_backward_out(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_linear1d_backward_out", 1);
  TF_VLOG(3) << "XLA upsample_linear1d_backward_out :" << " grad_output=" << grad_output.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_linear1d_backward_outf(external_tensors[0], output_size, input_size, align_corners, scales, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::upsample_bilinear2d(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_bilinear2d", 1);
  TF_VLOG(3) << "XLA upsample_bilinear2d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_bilinear2d(external_tensors[0], output_size, align_corners, scales_h, scales_w);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::upsample_bilinear2d_out(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_bilinear2d_out", 1);
  TF_VLOG(3) << "XLA upsample_bilinear2d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_bilinear2d_outf(external_tensors[0], output_size, align_corners, scales_h, scales_w, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::upsample_bilinear2d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_bilinear2d_backward", 1);
  TF_VLOG(3) << "XLA upsample_bilinear2d_backward :" << " grad_output=" << grad_output.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_bilinear2d_backward(external_tensors[0], output_size, input_size, align_corners, scales_h, scales_w);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::upsample_bilinear2d_backward_out(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_bilinear2d_backward_out", 1);
  TF_VLOG(3) << "XLA upsample_bilinear2d_backward_out :" << " grad_output=" << grad_output.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_bilinear2d_backward_outf(external_tensors[0], output_size, input_size, align_corners, scales_h, scales_w, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor & AtenXlaTypeDefault::upsample_bicubic2d_out(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_bicubic2d_out", 1);
  TF_VLOG(3) << "XLA upsample_bicubic2d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_bicubic2d_outf(external_tensors[0], output_size, align_corners, scales_h, scales_w, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::upsample_bicubic2d_backward_out(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_bicubic2d_backward_out", 1);
  TF_VLOG(3) << "XLA upsample_bicubic2d_backward_out :" << " grad_output=" << grad_output.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_bicubic2d_backward_outf(external_tensors[0], output_size, input_size, align_corners, scales_h, scales_w, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor & AtenXlaTypeDefault::upsample_trilinear3d_out(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_trilinear3d_out", 1);
  TF_VLOG(3) << "XLA upsample_trilinear3d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_trilinear3d_outf(external_tensors[0], output_size, align_corners, scales_d, scales_h, scales_w, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::upsample_trilinear3d_backward_out(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_trilinear3d_backward_out", 1);
  TF_VLOG(3) << "XLA upsample_trilinear3d_backward_out :" << " grad_output=" << grad_output.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_trilinear3d_backward_outf(external_tensors[0], output_size, input_size, align_corners, scales_d, scales_h, scales_w, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor & AtenXlaTypeDefault::upsample_nearest1d_out(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_nearest1d_out", 1);
  TF_VLOG(3) << "XLA upsample_nearest1d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_nearest1d_outf(external_tensors[0], output_size, scales, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::upsample_nearest1d_backward_out(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_nearest1d_backward_out", 1);
  TF_VLOG(3) << "XLA upsample_nearest1d_backward_out :" << " grad_output=" << grad_output.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_nearest1d_backward_outf(external_tensors[0], output_size, input_size, scales, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::upsample_nearest2d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_nearest2d", 1);
  TF_VLOG(3) << "XLA upsample_nearest2d :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_nearest2d(external_tensors[0], output_size, scales_h, scales_w);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::upsample_nearest2d_out(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_nearest2d_out", 1);
  TF_VLOG(3) << "XLA upsample_nearest2d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_nearest2d_outf(external_tensors[0], output_size, scales_h, scales_w, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::upsample_nearest2d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_nearest2d_backward", 1);
  TF_VLOG(3) << "XLA upsample_nearest2d_backward :" << " grad_output=" << grad_output.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_nearest2d_backward(external_tensors[0], output_size, input_size, scales_h, scales_w);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::upsample_nearest2d_backward_out(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_nearest2d_backward_out", 1);
  TF_VLOG(3) << "XLA upsample_nearest2d_backward_out :" << " grad_output=" << grad_output.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_nearest2d_backward_outf(external_tensors[0], output_size, input_size, scales_h, scales_w, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor & AtenXlaTypeDefault::upsample_nearest3d_out(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_nearest3d_out", 1);
  TF_VLOG(3) << "XLA upsample_nearest3d_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_nearest3d_outf(external_tensors[0], output_size, scales_d, scales_h, scales_w, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::upsample_nearest3d_backward_out(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::upsample_nearest3d_backward_out", 1);
  TF_VLOG(3) << "XLA upsample_nearest3d_backward_out :" << " grad_output=" << grad_output.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::upsample_nearest3d_backward_outf(external_tensors[0], output_size, input_size, scales_d, scales_h, scales_w, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::sigmoid_backward(const at::Tensor & grad_output, const at::Tensor & output) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sigmoid_backward", 1);
  TF_VLOG(3) << "XLA sigmoid_backward :" << " grad_output=" << grad_output.toString() << " output=" << output.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, output};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sigmoid_backward(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::sigmoid_backward_out(const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::sigmoid_backward_out", 1);
  TF_VLOG(3) << "XLA sigmoid_backward_out :" << " grad_output=" << grad_output.toString() << " output=" << output.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, output, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::sigmoid_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::logit_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logit_backward", 1);
  TF_VLOG(3) << "XLA logit_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logit_backward(external_tensors[0], external_tensors[1], eps);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::logit_backward_out(const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::logit_backward_out", 1);
  TF_VLOG(3) << "XLA logit_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::logit_backward_outf(external_tensors[0], external_tensors[1], eps, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::tanh_backward(const at::Tensor & grad_output, const at::Tensor & output) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::tanh_backward", 1);
  TF_VLOG(3) << "XLA tanh_backward :" << " grad_output=" << grad_output.toString() << " output=" << output.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, output};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::tanh_backward(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::tanh_backward_out(const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::tanh_backward_out", 1);
  TF_VLOG(3) << "XLA tanh_backward_out :" << " grad_output=" << grad_output.toString() << " output=" << output.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, output, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::tanh_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::slow_conv_transpose2d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv_transpose2d", 1);
  TF_VLOG(3) << "XLA slow_conv_transpose2d :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::slow_conv_transpose2d(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding, output_padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::slow_conv_transpose2d_out(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv_transpose2d_out", 1);
  TF_VLOG(3) << "XLA slow_conv_transpose2d_out :" << " self=" << self.toString() << " weight=" << weight.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::slow_conv_transpose2d_outf(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding, output_padding, dilation, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::slow_conv_transpose2d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & columns, const at::Tensor & ones, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv_transpose2d_backward", 1);
  TF_VLOG(3) << "XLA slow_conv_transpose2d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " weight=" << weight.toString() << " columns=" << columns.toString() << " ones=" << ones.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, weight, columns, ones};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::slow_conv_transpose2d_backward(external_tensors[0], external_tensors[1], external_tensors[2], kernel_size, stride, padding, output_padding, dilation, external_tensors[3], external_tensors[4], output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_output)));
}

at::Tensor AtenXlaTypeDefault::slow_conv_transpose3d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv_transpose3d", 1);
  TF_VLOG(3) << "XLA slow_conv_transpose3d :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::slow_conv_transpose3d(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding, output_padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::slow_conv_transpose3d_out(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv_transpose3d_out", 1);
  TF_VLOG(3) << "XLA slow_conv_transpose3d_out :" << " self=" << self.toString() << " weight=" << weight.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::slow_conv_transpose3d_outf(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding, output_padding, dilation, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::slow_conv_transpose3d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & finput, const at::Tensor & fgrad_input, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv_transpose3d_backward", 1);
  TF_VLOG(3) << "XLA slow_conv_transpose3d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " weight=" << weight.toString() << " finput=" << finput.toString() << " fgrad_input=" << fgrad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, weight, finput, fgrad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::slow_conv_transpose3d_backward(external_tensors[0], external_tensors[1], external_tensors[2], kernel_size, stride, padding, output_padding, dilation, external_tensors[3], external_tensors[4], output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_output)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::thnn_conv2d_forward(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::thnn_conv2d_forward", 1);
  TF_VLOG(3) << "XLA thnn_conv2d_forward :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::thnn_conv2d_forward(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> AtenXlaTypeDefault::thnn_conv2d_forward_out(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & output, at::Tensor & finput, at::Tensor & fgrad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::thnn_conv2d_forward_out", 1);
  TF_VLOG(3) << "XLA thnn_conv2d_forward_out :" << " self=" << self.toString() << " weight=" << weight.toString() << " output=" << output.toString() << " finput=" << finput.toString() << " fgrad_input=" << fgrad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight, output, finput, fgrad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::thnn_conv2d_forward_outf(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding, external_tensors[2], external_tensors[3], external_tensors[4]);
  std::vector<size_t> external_tensors_update_indices = {2, 3, 4};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(output, finput, fgrad_input);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::thnn_conv2d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::thnn_conv2d_backward", 1);
  TF_VLOG(3) << "XLA thnn_conv2d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " weight=" << weight.toString() << " finput=" << finput.toString() << " fgrad_input=" << fgrad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, weight, finput, fgrad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::thnn_conv2d_backward(external_tensors[0], external_tensors[1], external_tensors[2], kernel_size, stride, padding, external_tensors[3], external_tensors[4], output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_output)));
}

at::Tensor AtenXlaTypeDefault::thnn_conv_depthwise2d_forward(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::thnn_conv_depthwise2d_forward", 1);
  TF_VLOG(3) << "XLA thnn_conv_depthwise2d_forward :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::thnn_conv_depthwise2d_forward(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::thnn_conv_depthwise2d_forward_out(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::thnn_conv_depthwise2d_forward_out", 1);
  TF_VLOG(3) << "XLA thnn_conv_depthwise2d_forward_out :" << " self=" << self.toString() << " weight=" << weight.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::thnn_conv_depthwise2d_forward_outf(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding, dilation, external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::thnn_conv_depthwise2d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, std::array<bool,2> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::thnn_conv_depthwise2d_backward", 1);
  TF_VLOG(3) << "XLA thnn_conv_depthwise2d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::thnn_conv_depthwise2d_backward(external_tensors[0], external_tensors[1], external_tensors[2], kernel_size, stride, padding, dilation, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)));
}

at::Tensor AtenXlaTypeDefault::conv_depthwise3d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::conv_depthwise3d", 1);
  TF_VLOG(3) << "XLA conv_depthwise3d :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::conv_depthwise3d(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> AtenXlaTypeDefault::conv_depthwise3d_backward_out(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::conv_depthwise3d_backward_out", 1);
  TF_VLOG(3) << "XLA conv_depthwise3d_backward_out :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " weight=" << weight.toString() << " grad_input=" << grad_input.toString() << " grad_weight=" << grad_weight.toString() << " grad_bias=" << grad_bias.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, weight, grad_input, grad_weight, grad_bias};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::conv_depthwise3d_backward_outf(external_tensors[0], external_tensors[1], external_tensors[2], kernel_size, stride, padding, dilation, external_tensors[3], external_tensors[4], external_tensors[5]);
  std::vector<size_t> external_tensors_update_indices = {3, 4, 5};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(grad_input, grad_weight, grad_bias);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::conv_depthwise3d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::conv_depthwise3d_backward", 1);
  TF_VLOG(3) << "XLA conv_depthwise3d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::conv_depthwise3d_backward(external_tensors[0], external_tensors[1], external_tensors[2], kernel_size, stride, padding, dilation, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_output)));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::slow_conv3d_forward(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv3d_forward", 1);
  TF_VLOG(3) << "XLA slow_conv3d_forward :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::slow_conv3d_forward(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)), to_device_opt(std::get<2>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> AtenXlaTypeDefault::slow_conv3d_forward_out(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & output, at::Tensor & finput, at::Tensor & fgrad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv3d_forward_out", 1);
  TF_VLOG(3) << "XLA slow_conv3d_forward_out :" << " self=" << self.toString() << " weight=" << weight.toString() << " output=" << output.toString() << " finput=" << finput.toString() << " fgrad_input=" << fgrad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight, output, finput, fgrad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::slow_conv3d_forward_outf(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding, external_tensors[2], external_tensors[3], external_tensors[4]);
  std::vector<size_t> external_tensors_update_indices = {2, 3, 4};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &,at::Tensor &>(output, finput, fgrad_input);
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::slow_conv3d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv3d_backward", 1);
  TF_VLOG(3) << "XLA slow_conv3d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " weight=" << weight.toString() << " finput=" << finput.toString() << " fgrad_input=" << fgrad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, weight, finput, fgrad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::slow_conv3d_backward(external_tensors[0], external_tensors[1], external_tensors[2], kernel_size, stride, padding, external_tensors[3], external_tensors[4], output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_output)));
}

at::Tensor AtenXlaTypeDefault::slow_conv_dilated2d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv_dilated2d", 1);
  TF_VLOG(3) << "XLA slow_conv_dilated2d :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::slow_conv_dilated2d(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::slow_conv_dilated2d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv_dilated2d_backward", 1);
  TF_VLOG(3) << "XLA slow_conv_dilated2d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::slow_conv_dilated2d_backward(external_tensors[0], external_tensors[1], external_tensors[2], kernel_size, stride, padding, dilation, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_output)));
}

at::Tensor AtenXlaTypeDefault::slow_conv_dilated3d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv_dilated3d", 1);
  TF_VLOG(3) << "XLA slow_conv_dilated3d :" << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {bias};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::slow_conv_dilated3d(external_tensors[0], external_tensors[1], kernel_size, external_tensors_opt[0], stride, padding, dilation);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

std::tuple<at::Tensor,at::Tensor,at::Tensor> AtenXlaTypeDefault::slow_conv_dilated3d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, std::array<bool,3> output_mask) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::slow_conv_dilated3d_backward", 1);
  TF_VLOG(3) << "XLA slow_conv_dilated3d_backward :" << " grad_output=" << grad_output.toString() << " self=" << self.toString() << " weight=" << weight.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, self, weight};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::slow_conv_dilated3d_backward(external_tensors[0], external_tensors[1], external_tensors[2], kernel_size, stride, padding, dilation, output_mask);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<1>(x_result), get_device_arg(grad_output)), to_device_opt(std::get<2>(x_result), get_device_arg(grad_output)));
}

at::Tensor AtenXlaTypeDefault::col2im(const at::Tensor & self, at::IntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::col2im", 1);
  TF_VLOG(3) << "XLA col2im :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::col2im(external_tensors[0], output_size, kernel_size, dilation, padding, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::col2im_out(const at::Tensor & self, at::IntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::col2im_out", 1);
  TF_VLOG(3) << "XLA col2im_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::col2im_outf(external_tensors[0], output_size, kernel_size, dilation, padding, stride, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::col2im_backward(const at::Tensor & grad_output, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::col2im_backward", 1);
  TF_VLOG(3) << "XLA col2im_backward :" << " grad_output=" << grad_output.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::col2im_backward(external_tensors[0], kernel_size, dilation, padding, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::col2im_backward_out(const at::Tensor & grad_output, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::col2im_backward_out", 1);
  TF_VLOG(3) << "XLA col2im_backward_out :" << " grad_output=" << grad_output.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::col2im_backward_outf(external_tensors[0], kernel_size, dilation, padding, stride, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

at::Tensor AtenXlaTypeDefault::im2col(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::im2col", 1);
  TF_VLOG(3) << "XLA im2col :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::im2col(external_tensors[0], kernel_size, dilation, padding, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::im2col_out(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::im2col_out", 1);
  TF_VLOG(3) << "XLA im2col_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::im2col_outf(external_tensors[0], kernel_size, dilation, padding, stride, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenXlaTypeDefault::im2col_backward(const at::Tensor & grad_output, at::IntArrayRef input_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::im2col_backward", 1);
  TF_VLOG(3) << "XLA im2col_backward :" << " grad_output=" << grad_output.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::im2col_backward(external_tensors[0], input_size, kernel_size, dilation, padding, stride);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad_output));
}

at::Tensor & AtenXlaTypeDefault::im2col_backward_out(const at::Tensor & grad_output, at::IntArrayRef input_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & grad_input) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::im2col_backward_out", 1);
  TF_VLOG(3) << "XLA im2col_backward_out :" << " grad_output=" << grad_output.toString() << " grad_input=" << grad_input.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad_output, grad_input};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::im2col_backward_outf(external_tensors[0], input_size, kernel_size, dilation, padding, stride, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return grad_input;
}

void AtenXlaTypeDefault::record_stream(at::Tensor & self, at::Stream s) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::record_stream", 1);
  TF_VLOG(3) << "XLA record_stream :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  external_tensors[0].record_stream(s);
  std::vector<size_t> external_tensors_update_indices = {0};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

}

at::Tensor & AtenXlaTypeDefault::isposinf_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::isposinf_out", 1);
  TF_VLOG(3) << "XLA isposinf_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::isposinf_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::isneginf_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::isneginf_out", 1);
  TF_VLOG(3) << "XLA isneginf_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::isneginf_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::special_entr_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::special_entr_out", 1);
  TF_VLOG(3) << "XLA special_entr_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::special_entr_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::special_xlog1py_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::special_xlog1py_out", 1);
  TF_VLOG(3) << "XLA special_xlog1py_out :" << " self=" << self.toString() << " other=" << other.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::special_xlog1py_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::special_i0e_out(const at::Tensor & self, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::special_i0e_out", 1);
  TF_VLOG(3) << "XLA special_i0e_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::special_i0e_outf(external_tensors[0], external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::linalg_cholesky_ex(const at::Tensor & self, bool check_errors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::linalg_cholesky_ex", 1);
  TF_VLOG(3) << "XLA linalg_cholesky_ex :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::linalg_cholesky_ex(external_tensors[0], check_errors);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::linalg_cholesky_ex_out(const at::Tensor & self, bool check_errors, at::Tensor & L, at::Tensor & info) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::linalg_cholesky_ex_out", 1);
  TF_VLOG(3) << "XLA linalg_cholesky_ex_out :" << " self=" << self.toString() << " L=" << L.toString() << " info=" << info.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, L, info};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::linalg_cholesky_ex_outf(external_tensors[0], check_errors, external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(L, info);
}

std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> AtenXlaTypeDefault::linalg_lstsq_out(const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver, at::Tensor & solution, at::Tensor & residuals, at::Tensor & rank, at::Tensor & singular_values) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::linalg_lstsq_out", 1);
  TF_VLOG(3) << "XLA linalg_lstsq_out :" << " self=" << self.toString() << " b=" << b.toString() << " solution=" << solution.toString() << " residuals=" << residuals.toString() << " rank=" << rank.toString() << " singular_values=" << singular_values.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, b, solution, residuals, rank, singular_values};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::linalg_lstsq_outf(external_tensors[0], external_tensors[1], rcond, driver, external_tensors[2], external_tensors[3], external_tensors[4], external_tensors[5]);
  std::vector<size_t> external_tensors_update_indices = {2, 3, 4, 5};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &>(solution, residuals, rank, singular_values);
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::linalg_slogdet(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::linalg_slogdet", 1);
  TF_VLOG(3) << "XLA linalg_slogdet :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::linalg_slogdet(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::linalg_slogdet_out(const at::Tensor & self, at::Tensor & sign, at::Tensor & logabsdet) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::linalg_slogdet_out", 1);
  TF_VLOG(3) << "XLA linalg_slogdet_out :" << " self=" << self.toString() << " sign=" << sign.toString() << " logabsdet=" << logabsdet.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, sign, logabsdet};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::linalg_slogdet_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(sign, logabsdet);
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::linalg_eig(const at::Tensor & self) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::linalg_eig", 1);
  TF_VLOG(3) << "XLA linalg_eig :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::linalg_eig(external_tensors[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

std::tuple<at::Tensor &,at::Tensor &> AtenXlaTypeDefault::linalg_eig_out(const at::Tensor & self, at::Tensor & eigenvalues, at::Tensor & eigenvectors) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::linalg_eig_out", 1);
  TF_VLOG(3) << "XLA linalg_eig_out :" << " self=" << self.toString() << " eigenvalues=" << eigenvalues.toString() << " eigenvectors=" << eigenvectors.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, eigenvalues, eigenvectors};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::linalg_eig_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor &,at::Tensor &>(eigenvalues, eigenvectors);
}

at::Tensor AtenXlaTypeDefault::linalg_householder_product(const at::Tensor & input, const at::Tensor & tau) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::linalg_householder_product", 1);
  TF_VLOG(3) << "XLA linalg_householder_product :" << " input=" << input.toString() << " tau=" << tau.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, tau};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::linalg_householder_product(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(input));
}

at::Tensor & AtenXlaTypeDefault::linalg_householder_product_out(const at::Tensor & input, const at::Tensor & tau, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::linalg_householder_product_out", 1);
  TF_VLOG(3) << "XLA linalg_householder_product_out :" << " input=" << input.toString() << " tau=" << tau.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {input, tau, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::linalg_householder_product_outf(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::_linalg_inv_out_helper_(at::Tensor & self, at::Tensor & infos_lu, at::Tensor & infos_getri) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_linalg_inv_out_helper_", 1);
  TF_VLOG(3) << "XLA _linalg_inv_out_helper_ :" << " self=" << self.toString() << " infos_lu=" << infos_lu.toString() << " infos_getri=" << infos_getri.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, infos_lu, infos_getri};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_linalg_inv_out_helper_(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {0, 1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenXlaTypeDefault::ger(const at::Tensor & self, const at::Tensor & vec2) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::ger", 1);
  TF_VLOG(3) << "XLA ger :" << " self=" << self.toString() << " vec2=" << vec2.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, vec2};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::ger(external_tensors[0], external_tensors[1]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor AtenXlaTypeDefault::linalg_vector_norm(const at::Tensor & self, const at::Scalar & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::linalg_vector_norm", 1);
  TF_VLOG(3) << "XLA linalg_vector_norm :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::linalg_vector_norm(external_tensors[0], ord, dim, keepdim, dtype);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(self));
}

at::Tensor & AtenXlaTypeDefault::linalg_vector_norm_out(const at::Tensor & self, const at::Scalar & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::linalg_vector_norm_out", 1);
  TF_VLOG(3) << "XLA linalg_vector_norm_out :" << " self=" << self.toString() << " out=" << out.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, out};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::linalg_vector_norm_outf(external_tensors[0], ord, dim, keepdim, dtype, external_tensors[1]);
  std::vector<size_t> external_tensors_update_indices = {1};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return out;
}

at::Tensor & AtenXlaTypeDefault::_linalg_solve_out_helper_(at::Tensor & self, at::Tensor & other, at::Tensor & infos) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_linalg_solve_out_helper_", 1);
  TF_VLOG(3) << "XLA _linalg_solve_out_helper_ :" << " self=" << self.toString() << " other=" << other.toString() << " infos=" << infos.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self, other, infos};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_linalg_solve_out_helper_(external_tensors[0], external_tensors[1], external_tensors[2]);
  std::vector<size_t> external_tensors_update_indices = {0, 1, 2};
  for (int i : external_tensors_update_indices) {
    at::_copy_from_and_resize(external_tensors[i], external_tensors_tensors[i]);
  }

  static_cast<void>(x_result); // Avoid warnings in case not used
  return self;
}

std::tuple<at::Tensor,at::Tensor> AtenXlaTypeDefault::_linalg_qr_helper(const at::Tensor & self, c10::string_view mode) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_linalg_qr_helper", 1);
  TF_VLOG(3) << "XLA _linalg_qr_helper :" << " self=" << self.toString();
  std::vector<at::Tensor> external_tensors_tensors = {self};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_linalg_qr_helper(external_tensors[0], mode);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return std::tuple<at::Tensor,at::Tensor>(to_device_opt(std::get<0>(x_result), get_device_arg(self)), to_device_opt(std::get<1>(x_result), get_device_arg(self)));
}

at::Tensor AtenXlaTypeDefault::_test_optional_intlist(const at::Tensor & values, c10::optional<at::IntArrayRef> addends) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_test_optional_intlist", 1);
  TF_VLOG(3) << "XLA _test_optional_intlist :" << " values=" << values.toString();
  std::vector<at::Tensor> external_tensors_tensors = {values};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_test_optional_intlist(external_tensors[0], addends);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(values));
}

at::Tensor AtenXlaTypeDefault::_test_optional_filled_intlist(const at::Tensor & values, c10::optional<at::IntArrayRef> addends) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_test_optional_filled_intlist", 1);
  TF_VLOG(3) << "XLA _test_optional_filled_intlist :" << " values=" << values.toString();
  std::vector<at::Tensor> external_tensors_tensors = {values};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_test_optional_filled_intlist(external_tensors[0], addends);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(values));
}

at::Tensor AtenXlaTypeDefault::_test_optional_floatlist(const at::Tensor & values, c10::optional<at::ArrayRef<double>> addends) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::_test_optional_floatlist", 1);
  TF_VLOG(3) << "XLA _test_optional_floatlist :" << " values=" << values.toString();
  std::vector<at::Tensor> external_tensors_tensors = {values};
  auto external_tensors = to_cpu(external_tensors_tensors);
  auto&& x_result = at::_test_optional_floatlist(external_tensors[0], addends);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(values));
}

at::Tensor AtenXlaTypeDefault::segment_reduce(const at::Tensor & data, c10::string_view reduce, const c10::optional<at::Tensor> & lengths, const c10::optional<at::Tensor> & indices, int64_t axis, bool unsafe, const c10::optional<at::Scalar> & initial) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::segment_reduce", 1);
  TF_VLOG(3) << "XLA segment_reduce :" << " data=" << data.toString();
  std::vector<at::Tensor> external_tensors_tensors = {data};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {lengths, indices};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::segment_reduce(external_tensors[0], reduce, external_tensors_opt[0], external_tensors_opt[1], axis, unsafe, initial);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(data));
}

at::Tensor AtenXlaTypeDefault::segment_reduce_backward(const at::Tensor & grad, const at::Tensor & output, const at::Tensor & data, const c10::optional<at::Tensor> & lengths) {
  XLA_FN_TRACK(3);
  XLA_COUNTER("aten::segment_reduce_backward", 1);
  TF_VLOG(3) << "XLA segment_reduce_backward :" << " grad=" << grad.toString() << " output=" << output.toString() << " data=" << data.toString();
  std::vector<at::Tensor> external_tensors_tensors = {grad, output, data};
  auto external_tensors = to_cpu(external_tensors_tensors);
  std::vector<c10::optional<at::Tensor>> external_tensors_opt_tensors = {lengths};
  auto external_tensors_opt = to_cpu(external_tensors_opt_tensors);
  auto&& x_result = at::segment_reduce_backward(external_tensors[0], external_tensors[1], external_tensors[2], external_tensors_opt[0]);
  static_cast<void>(x_result); // Avoid warnings in case not used
  return to_device_opt(x_result, get_device_arg(grad));
}



TORCH_LIBRARY_IMPL(aten, XLA, m) {
  m.impl("_assert_async", static_cast<void (*)(const at::Tensor &)>(&AtenXlaTypeDefault::_assert_async));
  m.impl("_fused_dropout", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, double, c10::optional<at::Generator>)>(&AtenXlaTypeDefault::_fused_dropout));
  m.impl("_masked_scale", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, double)>(&AtenXlaTypeDefault::_masked_scale));
  m.impl("angle", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::angle));
  m.impl("angle.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::angle_out));
  m.impl("view_as_real", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::view_as_real));
  m.impl("view_as_complex", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::view_as_complex));
  m.impl("sgn.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::sgn_out));
  m.impl("conj.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::conj_out));
  m.impl("_add_relu.Tensor", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Scalar &)>(&AtenXlaTypeDefault::_add_relu));
  m.impl("_add_relu.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Scalar &, at::Tensor &)>(&AtenXlaTypeDefault::_add_relu_out));
  m.impl("_add_relu_.Tensor", static_cast<at::Tensor & (*)(at::Tensor &, const at::Tensor &, const at::Scalar &)>(&AtenXlaTypeDefault::_add_relu_));
  m.impl("addmv.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Scalar &, const at::Scalar &, at::Tensor &)>(&AtenXlaTypeDefault::addmv_out));
  m.impl("quantized_batch_norm", static_cast<at::Tensor (*)(const at::Tensor &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, const at::Tensor &, const at::Tensor &, double, double, int64_t)>(&AtenXlaTypeDefault::quantized_batch_norm));
  m.impl("bincount", static_cast<at::Tensor (*)(const at::Tensor &, const c10::optional<at::Tensor> &, int64_t)>(&AtenXlaTypeDefault::bincount));
  m.impl("copysign.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::copysign_out));
  m.impl("logical_not.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::logical_not_out));
  m.impl("logical_xor.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::logical_xor_out));
  m.impl("logical_and.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::logical_and_out));
  m.impl("logical_or.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::logical_or_out));
  m.impl("_bmm", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, bool)>(&AtenXlaTypeDefault::_bmm));
  m.impl("_bmm.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, bool, at::Tensor &)>(&AtenXlaTypeDefault::_bmm_out));
  m.impl("complex.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::complex_out));
  m.impl("polar.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::polar_out));
  m.impl("count_nonzero.dim_IntList", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef)>(&AtenXlaTypeDefault::count_nonzero));
  m.impl("_cummax_helper", static_cast<void (*)(const at::Tensor &, at::Tensor &, at::Tensor &, int64_t)>(&AtenXlaTypeDefault::_cummax_helper));
  m.impl("_cummin_helper", static_cast<void (*)(const at::Tensor &, at::Tensor &, at::Tensor &, int64_t)>(&AtenXlaTypeDefault::_cummin_helper));
  m.impl("_ctc_loss", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, int64_t, bool)>(&AtenXlaTypeDefault::_ctc_loss));
  m.impl("_ctc_loss_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, const at::Tensor &, const at::Tensor &, int64_t, bool)>(&AtenXlaTypeDefault::_ctc_loss_backward));
  m.impl("vdot", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::vdot));
  m.impl("embedding_renorm_", static_cast<at::Tensor & (*)(at::Tensor &, const at::Tensor &, double, double)>(&AtenXlaTypeDefault::embedding_renorm_));
  m.impl("_embedding_bag_forward_only", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, bool, int64_t, bool, const c10::optional<at::Tensor> &, bool, int64_t)>(&AtenXlaTypeDefault::_embedding_bag_forward_only));
  m.impl("_embedding_bag", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, bool, int64_t, bool, const c10::optional<at::Tensor> &, bool, int64_t)>(&AtenXlaTypeDefault::_embedding_bag));
  m.impl("_embedding_bag_dense_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, bool, int64_t, const c10::optional<at::Tensor> &, int64_t)>(&AtenXlaTypeDefault::_embedding_bag_dense_backward));
  m.impl("_embedding_bag_per_sample_weights_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, int64_t)>(&AtenXlaTypeDefault::_embedding_bag_per_sample_weights_backward));
  m.impl("_empty_affine_quantized", static_cast<at::Tensor (*)(at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>, double, int64_t, c10::optional<at::MemoryFormat>)>(&AtenXlaTypeDefault::_empty_affine_quantized));
  m.impl("_empty_per_channel_affine_quantized", static_cast<at::Tensor (*)(at::IntArrayRef, const at::Tensor &, const at::Tensor &, int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>, c10::optional<at::MemoryFormat>)>(&AtenXlaTypeDefault::_empty_per_channel_affine_quantized));
  m.impl("empty_quantized", static_cast<at::Tensor (*)(at::IntArrayRef, const at::Tensor &)>(&AtenXlaTypeDefault::empty_quantized));
  m.impl("exp2.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::exp2_out));
  m.impl("floor_divide", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::floor_divide));
  m.impl("floor_divide.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::floor_divide_out));
  m.impl("floor_divide_.Tensor", static_cast<at::Tensor & (*)(at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::floor_divide_));
  m.impl("from_file", static_cast<at::Tensor (*)(c10::string_view, c10::optional<bool>, c10::optional<int64_t>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>(&AtenXlaTypeDefault::from_file));
  m.impl("gcd.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::gcd_out));
  m.impl("lcm.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::lcm_out));
  m.impl("grid_sampler_2d", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, int64_t, int64_t, bool)>(&AtenXlaTypeDefault::grid_sampler_2d));
  m.impl("grid_sampler_2d_backward", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, int64_t, bool)>(&AtenXlaTypeDefault::grid_sampler_2d_backward));
  m.impl("grid_sampler_3d", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, int64_t, int64_t, bool)>(&AtenXlaTypeDefault::grid_sampler_3d));
  m.impl("grid_sampler_3d_backward", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, int64_t, bool)>(&AtenXlaTypeDefault::grid_sampler_3d_backward));
  m.impl("native_group_norm_backward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, int64_t, int64_t, int64_t, int64_t, std::array<bool,3>)>(&AtenXlaTypeDefault::native_group_norm_backward));
  m.impl("_fft_r2c", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, int64_t, bool)>(&AtenXlaTypeDefault::_fft_r2c));
  m.impl("_fft_r2c.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, int64_t, bool, at::Tensor &)>(&AtenXlaTypeDefault::_fft_r2c_out));
  m.impl("_fft_c2r", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, int64_t, int64_t)>(&AtenXlaTypeDefault::_fft_c2r));
  m.impl("_fft_c2r.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, int64_t, int64_t, at::Tensor &)>(&AtenXlaTypeDefault::_fft_c2r_out));
  m.impl("_fft_c2c", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, int64_t, bool)>(&AtenXlaTypeDefault::_fft_c2c));
  m.impl("_fft_c2c.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, int64_t, bool, at::Tensor &)>(&AtenXlaTypeDefault::_fft_c2c_out));
  m.impl("_inverse_helper", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::_inverse_helper));
  m.impl("isnan", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::isnan));
  m.impl("native_layer_norm_backward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, std::array<bool,3>)>(&AtenXlaTypeDefault::native_layer_norm_backward));
  m.impl("nan_to_num.out", static_cast<at::Tensor & (*)(const at::Tensor &, c10::optional<double>, c10::optional<double>, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::nan_to_num_out));
  m.impl("mkldnn_linear", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &)>(&AtenXlaTypeDefault::mkldnn_linear));
  m.impl("mkldnn_linear_backward_input", static_cast<at::Tensor (*)(at::IntArrayRef, const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::mkldnn_linear_backward_input));
  m.impl("mkldnn_linear_backward_weights", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, bool)>(&AtenXlaTypeDefault::mkldnn_linear_backward_weights));
  m.impl("mkldnn_linear_backward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, std::array<bool,3>)>(&AtenXlaTypeDefault::mkldnn_linear_backward));
  m.impl("linspace.out", static_cast<at::Tensor & (*)(const at::Scalar &, const at::Scalar &, c10::optional<int64_t>, at::Tensor &)>(&AtenXlaTypeDefault::linspace_out));
  m.impl("logaddexp.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::logaddexp_out));
  m.impl("logaddexp2.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::logaddexp2_out));
  m.impl("xlogy.Tensor", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::xlogy));
  m.impl("xlogy.OutTensor", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::xlogy_out));
  m.impl("xlogy_.Tensor", static_cast<at::Tensor & (*)(at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::xlogy_));
  m.impl("xlogy.Scalar_Self", static_cast<at::Tensor (*)(const at::Scalar &, const at::Tensor &)>(&AtenXlaTypeDefault::xlogy));
  m.impl("xlogy.OutScalar_Self", static_cast<at::Tensor & (*)(const at::Scalar &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::xlogy_out));
  m.impl("xlogy.Scalar_Other", static_cast<at::Tensor (*)(const at::Tensor &, const at::Scalar &)>(&AtenXlaTypeDefault::xlogy));
  m.impl("xlogy.OutScalar_Other", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Scalar &, at::Tensor &)>(&AtenXlaTypeDefault::xlogy_out));
  m.impl("xlogy_.Scalar_Other", static_cast<at::Tensor & (*)(at::Tensor &, const at::Scalar &)>(&AtenXlaTypeDefault::xlogy_));
  m.impl("logspace.out", static_cast<at::Tensor & (*)(const at::Scalar &, const at::Scalar &, c10::optional<int64_t>, double, at::Tensor &)>(&AtenXlaTypeDefault::logspace_out));
  m.impl("_logcumsumexp", static_cast<at::Tensor (*)(const at::Tensor &, int64_t)>(&AtenXlaTypeDefault::_logcumsumexp));
  m.impl("_logcumsumexp.out", static_cast<at::Tensor & (*)(const at::Tensor &, int64_t, at::Tensor &)>(&AtenXlaTypeDefault::_logcumsumexp_out));
  m.impl("matrix_exp", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::matrix_exp));
  m.impl("_aminmax", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &)>(&AtenXlaTypeDefault::_aminmax));
  m.impl("_aminmax.dim", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, int64_t, bool)>(&AtenXlaTypeDefault::_aminmax));
  m.impl("_compute_linear_combination", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::_compute_linear_combination));
  m.impl("_compute_linear_combination.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::_compute_linear_combination_out));
  m.impl("amax.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, bool, at::Tensor &)>(&AtenXlaTypeDefault::amax_out));
  m.impl("mkldnn_max_pool2d", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, bool)>(&AtenXlaTypeDefault::mkldnn_max_pool2d));
  m.impl("mkldnn_max_pool2d_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, bool)>(&AtenXlaTypeDefault::mkldnn_max_pool2d_backward));
  m.impl("mkldnn_max_pool3d", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, bool)>(&AtenXlaTypeDefault::mkldnn_max_pool3d));
  m.impl("mkldnn_max_pool3d_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, bool)>(&AtenXlaTypeDefault::mkldnn_max_pool3d_backward));
  m.impl("quantized_max_pool1d", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, bool)>(&AtenXlaTypeDefault::quantized_max_pool1d));
  m.impl("quantized_max_pool2d", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, bool)>(&AtenXlaTypeDefault::quantized_max_pool2d));
  m.impl("median", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::median));
  m.impl("median.dim_values", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, int64_t, bool, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::median_out));
  m.impl("nanmedian", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::nanmedian));
  m.impl("nanmedian.dim_values", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, int64_t, bool, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::nanmedian_out));
  m.impl("amin.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, bool, at::Tensor &)>(&AtenXlaTypeDefault::amin_out));
  m.impl("miopen_batch_norm", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, bool, double, double)>(&AtenXlaTypeDefault::miopen_batch_norm));
  m.impl("miopen_batch_norm_backward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, double)>(&AtenXlaTypeDefault::miopen_batch_norm_backward));
  m.impl("miopen_convolution", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool)>(&AtenXlaTypeDefault::miopen_convolution));
  m.impl("miopen_convolution_backward_input", static_cast<at::Tensor (*)(at::IntArrayRef, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool)>(&AtenXlaTypeDefault::miopen_convolution_backward_input));
  m.impl("miopen_convolution_backward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool, std::array<bool,3>)>(&AtenXlaTypeDefault::miopen_convolution_backward));
  m.impl("miopen_convolution_backward_bias", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::miopen_convolution_backward_bias));
  m.impl("miopen_convolution_backward_weight", static_cast<at::Tensor (*)(at::IntArrayRef, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool)>(&AtenXlaTypeDefault::miopen_convolution_backward_weight));
  m.impl("miopen_convolution_transpose", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool)>(&AtenXlaTypeDefault::miopen_convolution_transpose));
  m.impl("miopen_convolution_transpose_backward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool, std::array<bool,3>)>(&AtenXlaTypeDefault::miopen_convolution_transpose_backward));
  m.impl("miopen_convolution_transpose_backward_input", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool)>(&AtenXlaTypeDefault::miopen_convolution_transpose_backward_input));
  m.impl("miopen_convolution_transpose_backward_weight", static_cast<at::Tensor (*)(at::IntArrayRef, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool)>(&AtenXlaTypeDefault::miopen_convolution_transpose_backward_weight));
  m.impl("miopen_depthwise_convolution", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool)>(&AtenXlaTypeDefault::miopen_depthwise_convolution));
  m.impl("miopen_depthwise_convolution_backward_input", static_cast<at::Tensor (*)(at::IntArrayRef, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool)>(&AtenXlaTypeDefault::miopen_depthwise_convolution_backward_input));
  m.impl("miopen_depthwise_convolution_backward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool, std::array<bool,3>)>(&AtenXlaTypeDefault::miopen_depthwise_convolution_backward));
  m.impl("miopen_depthwise_convolution_backward_weight", static_cast<at::Tensor (*)(at::IntArrayRef, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t, bool, bool)>(&AtenXlaTypeDefault::miopen_depthwise_convolution_backward_weight));
  m.impl("miopen_rnn", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, at::TensorList, int64_t, const at::Tensor &, const c10::optional<at::Tensor> &, int64_t, int64_t, int64_t, bool, double, bool, bool, at::IntArrayRef, const c10::optional<at::Tensor> &)>(&AtenXlaTypeDefault::miopen_rnn));
  m.impl("miopen_rnn_backward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor,std::vector<at::Tensor>> (*)(const at::Tensor &, at::TensorList, int64_t, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, const at::Tensor &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, int64_t, int64_t, int64_t, bool, double, bool, bool, at::IntArrayRef, const c10::optional<at::Tensor> &, const at::Tensor &, std::array<bool,4>)>(&AtenXlaTypeDefault::miopen_rnn_backward));
  m.impl("_sparse_sparse_matmul", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::_sparse_sparse_matmul));
  m.impl("_sparse_mask_helper", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::_sparse_mask_helper));
  m.impl("mode", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, int64_t, bool)>(&AtenXlaTypeDefault::mode));
  m.impl("narrow_copy.out", static_cast<at::Tensor & (*)(const at::Tensor &, int64_t, int64_t, int64_t, at::Tensor &)>(&AtenXlaTypeDefault::narrow_copy_out));
  m.impl("batch_norm_stats", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, double)>(&AtenXlaTypeDefault::batch_norm_stats));
  m.impl("batch_norm_elemt", static_cast<at::Tensor (*)(const at::Tensor &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, const at::Tensor &, const at::Tensor &, double)>(&AtenXlaTypeDefault::batch_norm_elemt));
  m.impl("batch_norm_elemt.out", static_cast<at::Tensor & (*)(const at::Tensor &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, const at::Tensor &, const at::Tensor &, double, at::Tensor &)>(&AtenXlaTypeDefault::batch_norm_elemt_out));
  m.impl("batch_norm_gather_stats", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, double, double, int64_t)>(&AtenXlaTypeDefault::batch_norm_gather_stats));
  m.impl("batch_norm_gather_stats_with_counts", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, double, double, const at::Tensor &)>(&AtenXlaTypeDefault::batch_norm_gather_stats_with_counts));
  m.impl("batch_norm_backward_reduce", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, bool, bool, bool)>(&AtenXlaTypeDefault::batch_norm_backward_reduce));
  m.impl("batch_norm_backward_elemt", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, const at::Tensor &, const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::batch_norm_backward_elemt));
  m.impl("batch_norm_update_stats", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, double)>(&AtenXlaTypeDefault::batch_norm_update_stats));
  m.impl("_cdist_forward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, double, c10::optional<int64_t>)>(&AtenXlaTypeDefault::_cdist_forward));
  m.impl("_cdist_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, double, const at::Tensor &)>(&AtenXlaTypeDefault::_cdist_backward));
  m.impl("_pdist_forward", static_cast<at::Tensor (*)(const at::Tensor &, double)>(&AtenXlaTypeDefault::_pdist_forward));
  m.impl("_pdist_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, double, const at::Tensor &)>(&AtenXlaTypeDefault::_pdist_backward));
  m.impl("channel_shuffle", static_cast<at::Tensor (*)(const at::Tensor &, int64_t)>(&AtenXlaTypeDefault::channel_shuffle));
  m.impl("randperm.generator_out", static_cast<at::Tensor & (*)(int64_t, c10::optional<at::Generator>, at::Tensor &)>(&AtenXlaTypeDefault::randperm_out));
  m.impl("range.out", static_cast<at::Tensor & (*)(const at::Scalar &, const at::Scalar &, const at::Scalar &, at::Tensor &)>(&AtenXlaTypeDefault::range_out));
  m.impl("repeat_interleave.Tensor", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<int64_t>)>(&AtenXlaTypeDefault::repeat_interleave));
  m.impl("_mkldnn_reshape", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef)>(&AtenXlaTypeDefault::_mkldnn_reshape));
  m.impl("prelu", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::prelu));
  m.impl("prelu_backward", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::prelu_backward));
  m.impl("logit", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<double>)>(&AtenXlaTypeDefault::logit));
  m.impl("logit.out", static_cast<at::Tensor & (*)(const at::Tensor &, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::logit_out));
  m.impl("logit_", static_cast<at::Tensor & (*)(at::Tensor &, c10::optional<double>)>(&AtenXlaTypeDefault::logit_));
  m.impl("sinc.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::sinc_out));
  m.impl("sspaddmm.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Scalar &, const at::Scalar &, at::Tensor &)>(&AtenXlaTypeDefault::sspaddmm_out));
  m.impl("nansum", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<at::ScalarType>)>(&AtenXlaTypeDefault::nansum));
  m.impl("nansum.dim_IntList", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, bool, c10::optional<at::ScalarType>)>(&AtenXlaTypeDefault::nansum));
  m.impl("nansum.IntList_out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, bool, c10::optional<at::ScalarType>, at::Tensor &)>(&AtenXlaTypeDefault::nansum_out));
  m.impl("square.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::square_out));
  m.impl("std_mean.correction", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, c10::optional<at::IntArrayRef>, c10::optional<int64_t>, bool)>(&AtenXlaTypeDefault::std_mean));
  m.impl("tensordot.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::tensordot_out));
  m.impl("_mkldnn_transpose", static_cast<at::Tensor (*)(const at::Tensor &, int64_t, int64_t)>(&AtenXlaTypeDefault::_mkldnn_transpose));
  m.impl("_mkldnn_transpose_", static_cast<at::Tensor & (*)(at::Tensor &, int64_t, int64_t)>(&AtenXlaTypeDefault::_mkldnn_transpose_));
  m.impl("roll", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::roll));
  m.impl("_unique", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, bool, bool)>(&AtenXlaTypeDefault::_unique));
  m.impl("unique_dim", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, int64_t, bool, bool, bool)>(&AtenXlaTypeDefault::unique_dim));
  m.impl("unique_consecutive", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, bool, bool, c10::optional<int64_t>)>(&AtenXlaTypeDefault::unique_consecutive));
  m.impl("unique_dim_consecutive", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, int64_t, bool, bool)>(&AtenXlaTypeDefault::unique_dim_consecutive));
  m.impl("_unique2", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, bool, bool, bool)>(&AtenXlaTypeDefault::_unique2));
  m.impl("var_mean.correction", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, c10::optional<at::IntArrayRef>, c10::optional<int64_t>, bool)>(&AtenXlaTypeDefault::var_mean));
  m.impl("_weight_norm_cuda_interface", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, int64_t)>(&AtenXlaTypeDefault::_weight_norm_cuda_interface));
  m.impl("_weight_norm_cuda_interface_backward", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t)>(&AtenXlaTypeDefault::_weight_norm_cuda_interface_backward));
  m.impl("_standard_gamma_grad", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::_standard_gamma_grad));
  m.impl("_standard_gamma", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<at::Generator>)>(&AtenXlaTypeDefault::_standard_gamma));
  m.impl("_dirichlet_grad", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::_dirichlet_grad));
  m.impl("_sample_dirichlet", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<at::Generator>)>(&AtenXlaTypeDefault::_sample_dirichlet));
  m.impl("poisson", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<at::Generator>)>(&AtenXlaTypeDefault::poisson));
  m.impl("binomial", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, c10::optional<at::Generator>)>(&AtenXlaTypeDefault::binomial));
  m.impl("native_norm", static_cast<at::Tensor (*)(const at::Tensor &, const at::Scalar &)>(&AtenXlaTypeDefault::native_norm));
  m.impl("native_norm.ScalarOpt_dim_dtype", static_cast<at::Tensor (*)(const at::Tensor &, const c10::optional<at::Scalar> &, at::IntArrayRef, bool, c10::optional<at::ScalarType>)>(&AtenXlaTypeDefault::native_norm));
  m.impl("_sparse_sum_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef)>(&AtenXlaTypeDefault::_sparse_sum_backward));
  m.impl("_sparse_softmax", static_cast<at::Tensor (*)(const at::Tensor &, int64_t, bool)>(&AtenXlaTypeDefault::_sparse_softmax));
  m.impl("_sparse_softmax_backward_data", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, int64_t, const at::Tensor &)>(&AtenXlaTypeDefault::_sparse_softmax_backward_data));
  m.impl("_sparse_log_softmax", static_cast<at::Tensor (*)(const at::Tensor &, int64_t, bool)>(&AtenXlaTypeDefault::_sparse_log_softmax));
  m.impl("_sparse_log_softmax_backward_data", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, int64_t, const at::Tensor &)>(&AtenXlaTypeDefault::_sparse_log_softmax_backward_data));
  m.impl("frexp.Tensor_out", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::frexp_out));
  m.impl("resize_as_sparse_", static_cast<const at::Tensor & (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::resize_as_sparse_));
  m.impl("heaviside.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::heaviside_out));
  m.impl("_sparse_coo_tensor_with_dims", static_cast<at::Tensor (*)(int64_t, int64_t, at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>(&AtenXlaTypeDefault::_sparse_coo_tensor_with_dims));
  m.impl("_sparse_coo_tensor_with_dims_and_tensors", static_cast<at::Tensor (*)(int64_t, int64_t, at::IntArrayRef, const at::Tensor &, const at::Tensor &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>(&AtenXlaTypeDefault::_sparse_coo_tensor_with_dims_and_tensors));
  m.impl("sparse_resize_", static_cast<const at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, int64_t, int64_t)>(&AtenXlaTypeDefault::sparse_resize_));
  m.impl("sparse_resize_and_clear_", static_cast<const at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, int64_t, int64_t)>(&AtenXlaTypeDefault::sparse_resize_and_clear_));
  m.impl("sparse_mask", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::sparse_mask));
  m.impl("to_dense", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<at::ScalarType>)>(&AtenXlaTypeDefault::to_dense));
  m.impl("sparse_dim", static_cast<int64_t (*)(const at::Tensor &)>(&AtenXlaTypeDefault::sparse_dim));
  m.impl("_dimI", static_cast<int64_t (*)(const at::Tensor &)>(&AtenXlaTypeDefault::_dimI));
  m.impl("dense_dim", static_cast<int64_t (*)(const at::Tensor &)>(&AtenXlaTypeDefault::dense_dim));
  m.impl("_dimV", static_cast<int64_t (*)(const at::Tensor &)>(&AtenXlaTypeDefault::_dimV));
  m.impl("_nnz", static_cast<int64_t (*)(const at::Tensor &)>(&AtenXlaTypeDefault::_nnz));
  m.impl("_coalesce", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::_coalesce));
  m.impl("is_coalesced", static_cast<bool (*)(const at::Tensor &)>(&AtenXlaTypeDefault::is_coalesced));
  m.impl("_indices", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::_indices));
  m.impl("_values", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::_values));
  m.impl("_coalesced_", static_cast<at::Tensor & (*)(at::Tensor &, bool)>(&AtenXlaTypeDefault::_coalesced_));
  m.impl("indices", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::indices));
  m.impl("values", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::values));
  m.impl("crow_indices", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::crow_indices));
  m.impl("col_indices", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::col_indices));
  m.impl("hspmm", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::hspmm));
  m.impl("hspmm.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::hspmm_out));
  m.impl("copy_sparse_to_sparse_", static_cast<at::Tensor & (*)(at::Tensor &, const at::Tensor &, bool)>(&AtenXlaTypeDefault::copy_sparse_to_sparse_));
  m.impl("to_sparse.sparse_dim", static_cast<at::Tensor (*)(const at::Tensor &, int64_t)>(&AtenXlaTypeDefault::to_sparse));
  m.impl("to_sparse", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::to_sparse));
  m.impl("to_mkldnn", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<at::ScalarType>)>(&AtenXlaTypeDefault::to_mkldnn));
  m.impl("mkldnn_reorder_conv2d_weight", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t)>(&AtenXlaTypeDefault::mkldnn_reorder_conv2d_weight));
  m.impl("mkldnn_reorder_conv3d_weight", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, int64_t)>(&AtenXlaTypeDefault::mkldnn_reorder_conv3d_weight));
  m.impl("quantize_per_tensor", static_cast<at::Tensor (*)(const at::Tensor &, double, int64_t, at::ScalarType)>(&AtenXlaTypeDefault::quantize_per_tensor));
  m.impl("quantize_per_tensor.tensors", static_cast<std::vector<at::Tensor> (*)(at::TensorList, const at::Tensor &, const at::Tensor &, at::ScalarType)>(&AtenXlaTypeDefault::quantize_per_tensor));
  m.impl("quantize_per_channel", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, at::ScalarType)>(&AtenXlaTypeDefault::quantize_per_channel));
  m.impl("dequantize.self", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::dequantize));
  m.impl("dequantize.tensors", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::dequantize));
  m.impl("q_scale", static_cast<double (*)(const at::Tensor &)>(&AtenXlaTypeDefault::q_scale));
  m.impl("q_zero_point", static_cast<int64_t (*)(const at::Tensor &)>(&AtenXlaTypeDefault::q_zero_point));
  m.impl("q_per_channel_scales", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::q_per_channel_scales));
  m.impl("q_per_channel_zero_points", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::q_per_channel_zero_points));
  m.impl("q_per_channel_axis", static_cast<int64_t (*)(const at::Tensor &)>(&AtenXlaTypeDefault::q_per_channel_axis));
  m.impl("int_repr", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::int_repr));
  m.impl("_make_per_tensor_quantized_tensor", static_cast<at::Tensor (*)(const at::Tensor &, double, int64_t)>(&AtenXlaTypeDefault::_make_per_tensor_quantized_tensor));
  m.impl("_make_per_channel_quantized_tensor", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t)>(&AtenXlaTypeDefault::_make_per_channel_quantized_tensor));
  m.impl("qscheme", static_cast<at::QScheme (*)(const at::Tensor &)>(&AtenXlaTypeDefault::qscheme));
  m.impl("fake_quantize_per_tensor_affine_cachemask", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, double, int64_t, int64_t, int64_t)>(&AtenXlaTypeDefault::fake_quantize_per_tensor_affine_cachemask));
  m.impl("_fake_quantize_learnable_per_tensor_affine", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, int64_t, double)>(&AtenXlaTypeDefault::_fake_quantize_learnable_per_tensor_affine));
  m.impl("fake_quantize_per_channel_affine_cachemask", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, int64_t, int64_t)>(&AtenXlaTypeDefault::fake_quantize_per_channel_affine_cachemask));
  m.impl("_fake_quantize_learnable_per_channel_affine", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, int64_t, int64_t, double)>(&AtenXlaTypeDefault::_fake_quantize_learnable_per_channel_affine));
  m.impl("_thnn_fused_lstm_cell", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &)>(&AtenXlaTypeDefault::_thnn_fused_lstm_cell));
  m.impl("_thnn_fused_lstm_cell_backward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> (*)(const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, const at::Tensor &, const at::Tensor &, const at::Tensor &, bool)>(&AtenXlaTypeDefault::_thnn_fused_lstm_cell_backward));
  m.impl("_thnn_fused_gru_cell", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &)>(&AtenXlaTypeDefault::_thnn_fused_gru_cell));
  m.impl("_thnn_fused_gru_cell_backward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, bool)>(&AtenXlaTypeDefault::_thnn_fused_gru_cell_backward));
  m.impl("set_.source_Storage", static_cast<at::Tensor & (*)(at::Tensor &, at::Storage)>(&AtenXlaTypeDefault::set_));
  m.impl("set_.source_Storage_storage_offset", static_cast<at::Tensor & (*)(at::Tensor &, at::Storage, int64_t, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::set_));
  m.impl("set_.source_Tensor", static_cast<at::Tensor & (*)(at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::set_));
  m.impl("set_", static_cast<at::Tensor & (*)(at::Tensor &)>(&AtenXlaTypeDefault::set_));
  m.impl("is_set_to", static_cast<bool (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::is_set_to));
  m.impl("index_add_.alpha", static_cast<at::Tensor & (*)(at::Tensor &, int64_t, const at::Tensor &, const at::Tensor &, const at::Scalar &)>(&AtenXlaTypeDefault::index_add_));
  m.impl("scatter_.reduce", static_cast<at::Tensor & (*)(at::Tensor &, int64_t, const at::Tensor &, const at::Tensor &, c10::string_view)>(&AtenXlaTypeDefault::scatter_));
  m.impl("scatter_.value_reduce", static_cast<at::Tensor & (*)(at::Tensor &, int64_t, const at::Tensor &, const at::Scalar &, c10::string_view)>(&AtenXlaTypeDefault::scatter_));
  m.impl("digamma.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::digamma_out));
  m.impl("renorm", static_cast<at::Tensor (*)(const at::Tensor &, const at::Scalar &, int64_t, const at::Scalar &)>(&AtenXlaTypeDefault::renorm));
  m.impl("renorm.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Scalar &, int64_t, const at::Scalar &, at::Tensor &)>(&AtenXlaTypeDefault::renorm_out));
  m.impl("renorm_", static_cast<at::Tensor & (*)(at::Tensor &, const at::Scalar &, int64_t, const at::Scalar &)>(&AtenXlaTypeDefault::renorm_));
  m.impl("lerp.Scalar", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Scalar &)>(&AtenXlaTypeDefault::lerp));
  m.impl("lerp.Scalar_out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Scalar &, at::Tensor &)>(&AtenXlaTypeDefault::lerp_out));
  m.impl("lerp_.Scalar", static_cast<at::Tensor & (*)(at::Tensor &, const at::Tensor &, const at::Scalar &)>(&AtenXlaTypeDefault::lerp_));
  m.impl("lerp.Tensor", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::lerp));
  m.impl("lerp.Tensor_out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::lerp_out));
  m.impl("lerp_.Tensor", static_cast<at::Tensor & (*)(at::Tensor &, const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::lerp_));
  m.impl("addbmm", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Scalar &, const at::Scalar &)>(&AtenXlaTypeDefault::addbmm));
  m.impl("addbmm.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Scalar &, const at::Scalar &, at::Tensor &)>(&AtenXlaTypeDefault::addbmm_out));
  m.impl("addbmm_", static_cast<at::Tensor & (*)(at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Scalar &, const at::Scalar &)>(&AtenXlaTypeDefault::addbmm_));
  m.impl("cauchy_", static_cast<at::Tensor & (*)(at::Tensor &, double, double, c10::optional<at::Generator>)>(&AtenXlaTypeDefault::cauchy_));
  m.impl("log_normal_", static_cast<at::Tensor & (*)(at::Tensor &, double, double, c10::optional<at::Generator>)>(&AtenXlaTypeDefault::log_normal_));
  m.impl("geometric_", static_cast<at::Tensor & (*)(at::Tensor &, double, c10::optional<at::Generator>)>(&AtenXlaTypeDefault::geometric_));
  m.impl("tril_indices", static_cast<at::Tensor (*)(int64_t, int64_t, int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>(&AtenXlaTypeDefault::tril_indices));
  m.impl("triu_indices", static_cast<at::Tensor (*)(int64_t, int64_t, int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>(&AtenXlaTypeDefault::triu_indices));
  m.impl("lstsq", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::lstsq));
  m.impl("lstsq.X", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, const at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::lstsq_out));
  m.impl("_symeig_helper", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, bool, bool)>(&AtenXlaTypeDefault::_symeig_helper));
  m.impl("_svd_helper", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, bool, bool)>(&AtenXlaTypeDefault::_svd_helper));
  m.impl("_cholesky_solve_helper", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, bool)>(&AtenXlaTypeDefault::_cholesky_solve_helper));
  m.impl("_solve_helper", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::_solve_helper));
  m.impl("cholesky_inverse", static_cast<at::Tensor (*)(const at::Tensor &, bool)>(&AtenXlaTypeDefault::cholesky_inverse));
  m.impl("cholesky_inverse.out", static_cast<at::Tensor & (*)(const at::Tensor &, bool, at::Tensor &)>(&AtenXlaTypeDefault::cholesky_inverse_out));
  m.impl("geqrf", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &)>(&AtenXlaTypeDefault::geqrf));
  m.impl("geqrf.a", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::geqrf_out));
  m.impl("ormqr", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, bool, bool)>(&AtenXlaTypeDefault::ormqr));
  m.impl("ormqr.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, bool, bool, at::Tensor &)>(&AtenXlaTypeDefault::ormqr_out));
  m.impl("_lu_with_info", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, bool, bool)>(&AtenXlaTypeDefault::_lu_with_info));
  m.impl("lu_unpack", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, bool, bool)>(&AtenXlaTypeDefault::lu_unpack));
  m.impl("lu_unpack.out", static_cast<std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> (*)(const at::Tensor &, const at::Tensor &, bool, bool, at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::lu_unpack_out));
  m.impl("multinomial", static_cast<at::Tensor (*)(const at::Tensor &, int64_t, bool, c10::optional<at::Generator>)>(&AtenXlaTypeDefault::multinomial));
  m.impl("multinomial.out", static_cast<at::Tensor & (*)(const at::Tensor &, int64_t, bool, c10::optional<at::Generator>, at::Tensor &)>(&AtenXlaTypeDefault::multinomial_out));
  m.impl("lgamma.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::lgamma_out));
  m.impl("polygamma.out", static_cast<at::Tensor & (*)(int64_t, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::polygamma_out));
  m.impl("i0.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::i0_out));
  m.impl("signbit.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::signbit_out));
  m.impl("histc", static_cast<at::Tensor (*)(const at::Tensor &, int64_t, const at::Scalar &, const at::Scalar &)>(&AtenXlaTypeDefault::histc));
  m.impl("histc.out", static_cast<at::Tensor & (*)(const at::Tensor &, int64_t, const at::Scalar &, const at::Scalar &, at::Tensor &)>(&AtenXlaTypeDefault::histc_out));
  m.impl("hypot.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::hypot_out));
  m.impl("igamma.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::igamma_out));
  m.impl("igammac.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::igammac_out));
  m.impl("nextafter.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::nextafter_out));
  m.impl("fmin.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::fmin_out));
  m.impl("fmax.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::fmax_out));
  m.impl("sort.stable", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, c10::optional<bool>, int64_t, bool)>(&AtenXlaTypeDefault::sort));
  m.impl("sort.values_stable", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, c10::optional<bool>, int64_t, bool, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::sort_out));
  m.impl("unfold", static_cast<at::Tensor (*)(const at::Tensor &, int64_t, int64_t, int64_t)>(&AtenXlaTypeDefault::unfold));
  m.impl("unfold_backward", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, int64_t, int64_t, int64_t)>(&AtenXlaTypeDefault::unfold_backward));
  m.impl("equal", static_cast<bool (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::equal));
  m.impl("_index_copy_", static_cast<at::Tensor & (*)(at::Tensor &, int64_t, const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::_index_copy_));
  m.impl("_cumsum", static_cast<at::Tensor (*)(const at::Tensor &, int64_t)>(&AtenXlaTypeDefault::_cumsum));
  m.impl("_cumsum.out", static_cast<at::Tensor & (*)(const at::Tensor &, int64_t, at::Tensor &)>(&AtenXlaTypeDefault::_cumsum_out));
  m.impl("_cumprod", static_cast<at::Tensor (*)(const at::Tensor &, int64_t)>(&AtenXlaTypeDefault::_cumprod));
  m.impl("_cumprod.out", static_cast<at::Tensor & (*)(const at::Tensor &, int64_t, at::Tensor &)>(&AtenXlaTypeDefault::_cumprod_out));
  m.impl("_cat", static_cast<at::Tensor (*)(at::TensorList, int64_t)>(&AtenXlaTypeDefault::_cat));
  m.impl("_cat.out", static_cast<at::Tensor & (*)(at::TensorList, int64_t, at::Tensor &)>(&AtenXlaTypeDefault::_cat_out));
  m.impl("_foreach_add.Scalar", static_cast<std::vector<at::Tensor> (*)(at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_add));
  m.impl("_foreach_add_.Scalar", static_cast<void (*)(at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_add_));
  m.impl("_foreach_sub.Scalar", static_cast<std::vector<at::Tensor> (*)(at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_sub));
  m.impl("_foreach_sub_.Scalar", static_cast<void (*)(at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_sub_));
  m.impl("_foreach_mul.Scalar", static_cast<std::vector<at::Tensor> (*)(at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_mul));
  m.impl("_foreach_mul_.Scalar", static_cast<void (*)(at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_mul_));
  m.impl("_foreach_div.Scalar", static_cast<std::vector<at::Tensor> (*)(at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_div));
  m.impl("_foreach_div_.Scalar", static_cast<void (*)(at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_div_));
  m.impl("_foreach_add.List", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_add));
  m.impl("_foreach_add_.List", static_cast<void (*)(at::TensorList, at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_add_));
  m.impl("_foreach_sub.List", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_sub));
  m.impl("_foreach_sub_.List", static_cast<void (*)(at::TensorList, at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_sub_));
  m.impl("_foreach_mul.List", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::TensorList)>(&AtenXlaTypeDefault::_foreach_mul));
  m.impl("_foreach_mul_.List", static_cast<void (*)(at::TensorList, at::TensorList)>(&AtenXlaTypeDefault::_foreach_mul_));
  m.impl("_foreach_div.List", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::TensorList)>(&AtenXlaTypeDefault::_foreach_div));
  m.impl("_foreach_div_.List", static_cast<void (*)(at::TensorList, at::TensorList)>(&AtenXlaTypeDefault::_foreach_div_));
  m.impl("_foreach_add.ScalarList", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::ArrayRef<at::Scalar>)>(&AtenXlaTypeDefault::_foreach_add));
  m.impl("_foreach_add_.ScalarList", static_cast<void (*)(at::TensorList, at::ArrayRef<at::Scalar>)>(&AtenXlaTypeDefault::_foreach_add_));
  m.impl("_foreach_sub.ScalarList", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::ArrayRef<at::Scalar>)>(&AtenXlaTypeDefault::_foreach_sub));
  m.impl("_foreach_sub_.ScalarList", static_cast<void (*)(at::TensorList, at::ArrayRef<at::Scalar>)>(&AtenXlaTypeDefault::_foreach_sub_));
  m.impl("_foreach_div.ScalarList", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::ArrayRef<at::Scalar>)>(&AtenXlaTypeDefault::_foreach_div));
  m.impl("_foreach_div_.ScalarList", static_cast<void (*)(at::TensorList, at::ArrayRef<at::Scalar>)>(&AtenXlaTypeDefault::_foreach_div_));
  m.impl("_foreach_mul.ScalarList", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::ArrayRef<at::Scalar>)>(&AtenXlaTypeDefault::_foreach_mul));
  m.impl("_foreach_mul_.ScalarList", static_cast<void (*)(at::TensorList, at::ArrayRef<at::Scalar>)>(&AtenXlaTypeDefault::_foreach_mul_));
  m.impl("_foreach_exp", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_exp));
  m.impl("_foreach_zero_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_zero_));
  m.impl("_foreach_exp_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_exp_));
  m.impl("_foreach_sqrt", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_sqrt));
  m.impl("_foreach_sqrt_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_sqrt_));
  m.impl("_foreach_abs", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_abs));
  m.impl("_foreach_abs_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_abs_));
  m.impl("_foreach_acos", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_acos));
  m.impl("_foreach_acos_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_acos_));
  m.impl("_foreach_asin", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_asin));
  m.impl("_foreach_asin_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_asin_));
  m.impl("_foreach_atan", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_atan));
  m.impl("_foreach_atan_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_atan_));
  m.impl("_foreach_ceil", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_ceil));
  m.impl("_foreach_ceil_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_ceil_));
  m.impl("_foreach_cos", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_cos));
  m.impl("_foreach_cos_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_cos_));
  m.impl("_foreach_cosh", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_cosh));
  m.impl("_foreach_cosh_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_cosh_));
  m.impl("_foreach_erf", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_erf));
  m.impl("_foreach_erf_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_erf_));
  m.impl("_foreach_erfc", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_erfc));
  m.impl("_foreach_erfc_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_erfc_));
  m.impl("_foreach_expm1", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_expm1));
  m.impl("_foreach_expm1_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_expm1_));
  m.impl("_foreach_floor", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_floor));
  m.impl("_foreach_floor_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_floor_));
  m.impl("_foreach_log", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_log));
  m.impl("_foreach_log_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_log_));
  m.impl("_foreach_log10", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_log10));
  m.impl("_foreach_log10_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_log10_));
  m.impl("_foreach_log1p", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_log1p));
  m.impl("_foreach_log1p_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_log1p_));
  m.impl("_foreach_log2", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_log2));
  m.impl("_foreach_log2_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_log2_));
  m.impl("_foreach_neg", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_neg));
  m.impl("_foreach_neg_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_neg_));
  m.impl("_foreach_tan", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_tan));
  m.impl("_foreach_tan_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_tan_));
  m.impl("_foreach_tanh", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_tanh));
  m.impl("_foreach_tanh_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_tanh_));
  m.impl("_foreach_sin", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_sin));
  m.impl("_foreach_sin_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_sin_));
  m.impl("_foreach_sinh", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_sinh));
  m.impl("_foreach_sinh_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_sinh_));
  m.impl("_foreach_round", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_round));
  m.impl("_foreach_round_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_round_));
  m.impl("_foreach_lgamma", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_lgamma));
  m.impl("_foreach_lgamma_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_lgamma_));
  m.impl("_foreach_frac", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_frac));
  m.impl("_foreach_frac_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_frac_));
  m.impl("_foreach_reciprocal", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_reciprocal));
  m.impl("_foreach_reciprocal_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_reciprocal_));
  m.impl("_foreach_sigmoid", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_sigmoid));
  m.impl("_foreach_sigmoid_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_sigmoid_));
  m.impl("_foreach_trunc", static_cast<std::vector<at::Tensor> (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_trunc));
  m.impl("_foreach_trunc_", static_cast<void (*)(at::TensorList)>(&AtenXlaTypeDefault::_foreach_trunc_));
  m.impl("_foreach_addcdiv_.Scalar", static_cast<void (*)(at::TensorList, at::TensorList, at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_addcdiv_));
  m.impl("_foreach_addcmul_.Scalar", static_cast<void (*)(at::TensorList, at::TensorList, at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_addcmul_));
  m.impl("_foreach_addcdiv_.ScalarList", static_cast<void (*)(at::TensorList, at::TensorList, at::TensorList, at::ArrayRef<at::Scalar>)>(&AtenXlaTypeDefault::_foreach_addcdiv_));
  m.impl("_foreach_addcmul_.ScalarList", static_cast<void (*)(at::TensorList, at::TensorList, at::TensorList, at::ArrayRef<at::Scalar>)>(&AtenXlaTypeDefault::_foreach_addcmul_));
  m.impl("_foreach_addcdiv.Scalar", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::TensorList, at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_addcdiv));
  m.impl("_foreach_addcmul.Scalar", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::TensorList, at::TensorList, const at::Scalar &)>(&AtenXlaTypeDefault::_foreach_addcmul));
  m.impl("_foreach_addcdiv.ScalarList", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::TensorList, at::TensorList, at::ArrayRef<at::Scalar>)>(&AtenXlaTypeDefault::_foreach_addcdiv));
  m.impl("_foreach_addcmul.ScalarList", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::TensorList, at::TensorList, at::ArrayRef<at::Scalar>)>(&AtenXlaTypeDefault::_foreach_addcmul));
  m.impl("_foreach_maximum.List", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::TensorList)>(&AtenXlaTypeDefault::_foreach_maximum));
  m.impl("_foreach_minimum.List", static_cast<std::vector<at::Tensor> (*)(at::TensorList, at::TensorList)>(&AtenXlaTypeDefault::_foreach_minimum));
  m.impl("bucketize.Tensor", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, bool, bool)>(&AtenXlaTypeDefault::bucketize));
  m.impl("bucketize.Tensor_out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, bool, bool, at::Tensor &)>(&AtenXlaTypeDefault::bucketize_out));
  m.impl("bucketize.Scalar", static_cast<at::Tensor (*)(const at::Scalar &, const at::Tensor &, bool, bool)>(&AtenXlaTypeDefault::bucketize));
  m.impl("searchsorted.Tensor", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, bool, bool)>(&AtenXlaTypeDefault::searchsorted));
  m.impl("searchsorted.Tensor_out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, bool, bool, at::Tensor &)>(&AtenXlaTypeDefault::searchsorted_out));
  m.impl("searchsorted.Scalar", static_cast<at::Tensor (*)(const at::Tensor &, const at::Scalar &, bool, bool)>(&AtenXlaTypeDefault::searchsorted));
  m.impl("multi_margin_loss", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Scalar &, const at::Scalar &, const c10::optional<at::Tensor> &, int64_t)>(&AtenXlaTypeDefault::multi_margin_loss));
  m.impl("multi_margin_loss.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Scalar &, const at::Scalar &, const c10::optional<at::Tensor> &, int64_t, at::Tensor &)>(&AtenXlaTypeDefault::multi_margin_loss_out));
  m.impl("multi_margin_loss_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Scalar &, const at::Scalar &, const c10::optional<at::Tensor> &, int64_t)>(&AtenXlaTypeDefault::multi_margin_loss_backward));
  m.impl("multi_margin_loss_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Scalar &, const at::Scalar &, const c10::optional<at::Tensor> &, int64_t, at::Tensor &)>(&AtenXlaTypeDefault::multi_margin_loss_backward_out));
  m.impl("multilabel_margin_loss_forward", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, int64_t)>(&AtenXlaTypeDefault::multilabel_margin_loss_forward));
  m.impl("multilabel_margin_loss_forward.output", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, const at::Tensor &, int64_t, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::multilabel_margin_loss_forward_out));
  m.impl("multilabel_margin_loss_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, const at::Tensor &)>(&AtenXlaTypeDefault::multilabel_margin_loss_backward));
  m.impl("multilabel_margin_loss_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::multilabel_margin_loss_backward_out));
  m.impl("huber_loss", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, int64_t, double)>(&AtenXlaTypeDefault::huber_loss));
  m.impl("huber_loss.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, int64_t, double, at::Tensor &)>(&AtenXlaTypeDefault::huber_loss_out));
  m.impl("huber_loss_backward.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, double, at::Tensor &)>(&AtenXlaTypeDefault::huber_loss_backward_out));
  m.impl("glu", static_cast<at::Tensor (*)(const at::Tensor &, int64_t)>(&AtenXlaTypeDefault::glu));
  m.impl("glu.out", static_cast<at::Tensor & (*)(const at::Tensor &, int64_t, at::Tensor &)>(&AtenXlaTypeDefault::glu_out));
  m.impl("glu_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, int64_t)>(&AtenXlaTypeDefault::glu_backward));
  m.impl("glu_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, int64_t, at::Tensor &)>(&AtenXlaTypeDefault::glu_backward_out));
  m.impl("hardswish", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenXlaTypeDefault::hardswish));
  m.impl("hardswish.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::hardswish_out));
  m.impl("hardswish_", static_cast<at::Tensor & (*)(at::Tensor &)>(&AtenXlaTypeDefault::hardswish_));
  m.impl("hardswish_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::hardswish_backward));
  m.impl("adaptive_avg_pool2d.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::adaptive_avg_pool2d_out));
  m.impl("mkldnn_adaptive_avg_pool2d", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef)>(&AtenXlaTypeDefault::mkldnn_adaptive_avg_pool2d));
  m.impl("mkldnn_adaptive_avg_pool2d_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::mkldnn_adaptive_avg_pool2d_backward));
  m.impl("adaptive_avg_pool3d.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::adaptive_avg_pool3d_out));
  m.impl("adaptive_avg_pool3d_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::adaptive_avg_pool3d_backward_out));
  m.impl("adaptive_max_pool2d.out", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, at::IntArrayRef, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::adaptive_max_pool2d_out));
  m.impl("adaptive_max_pool2d_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::adaptive_max_pool2d_backward_out));
  m.impl("adaptive_max_pool3d.out", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, at::IntArrayRef, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::adaptive_max_pool3d_out));
  m.impl("adaptive_max_pool3d_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::adaptive_max_pool3d_backward_out));
  m.impl("fractional_max_pool2d.output", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, const at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::fractional_max_pool2d_out));
  m.impl("fractional_max_pool2d_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, const at::Tensor &)>(&AtenXlaTypeDefault::fractional_max_pool2d_backward));
  m.impl("fractional_max_pool2d_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::fractional_max_pool2d_backward_out));
  m.impl("fractional_max_pool3d", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, const at::Tensor &)>(&AtenXlaTypeDefault::fractional_max_pool3d));
  m.impl("fractional_max_pool3d.output", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, const at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::fractional_max_pool3d_out));
  m.impl("fractional_max_pool3d_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, const at::Tensor &)>(&AtenXlaTypeDefault::fractional_max_pool3d_backward));
  m.impl("fractional_max_pool3d_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::fractional_max_pool3d_backward_out));
  m.impl("reflection_pad1d.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::reflection_pad1d_out));
  m.impl("reflection_pad1d_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef)>(&AtenXlaTypeDefault::reflection_pad1d_backward));
  m.impl("reflection_pad1d_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::reflection_pad1d_backward_out));
  m.impl("replication_pad3d.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::replication_pad3d_out));
  m.impl("replication_pad3d_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef)>(&AtenXlaTypeDefault::replication_pad3d_backward));
  m.impl("replication_pad3d_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::replication_pad3d_backward_out));
  m.impl("upsample_nearest3d.vec", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<at::IntArrayRef>, c10::optional<at::ArrayRef<double>>)>(&AtenXlaTypeDefault::upsample_nearest3d));
  m.impl("upsample_nearest3d_backward.vec", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<at::IntArrayRef>, at::IntArrayRef, c10::optional<at::ArrayRef<double>>)>(&AtenXlaTypeDefault::upsample_nearest3d_backward));
  m.impl("upsample_linear1d.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, bool, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::upsample_linear1d_out));
  m.impl("upsample_linear1d_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, bool, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::upsample_linear1d_backward_out));
  m.impl("upsample_bicubic2d.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, bool, c10::optional<double>, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::upsample_bicubic2d_out));
  m.impl("upsample_bicubic2d_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, bool, c10::optional<double>, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::upsample_bicubic2d_backward_out));
  m.impl("upsample_trilinear3d.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, bool, c10::optional<double>, c10::optional<double>, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::upsample_trilinear3d_out));
  m.impl("upsample_trilinear3d_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, bool, c10::optional<double>, c10::optional<double>, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::upsample_trilinear3d_backward_out));
  m.impl("upsample_nearest1d.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::upsample_nearest1d_out));
  m.impl("upsample_nearest1d_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::upsample_nearest1d_backward_out));
  m.impl("upsample_nearest3d.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, c10::optional<double>, c10::optional<double>, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::upsample_nearest3d_out));
  m.impl("upsample_nearest3d_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, c10::optional<double>, c10::optional<double>, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::upsample_nearest3d_backward_out));
  m.impl("logit_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, c10::optional<double>)>(&AtenXlaTypeDefault::logit_backward));
  m.impl("logit_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, c10::optional<double>, at::Tensor &)>(&AtenXlaTypeDefault::logit_backward_out));
  m.impl("slow_conv_transpose2d", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::slow_conv_transpose2d));
  m.impl("slow_conv_transpose2d.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::slow_conv_transpose2d_out));
  m.impl("slow_conv_transpose2d_backward.output_mask", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, const at::Tensor &, const at::Tensor &, std::array<bool,3>)>(&AtenXlaTypeDefault::slow_conv_transpose2d_backward));
  m.impl("slow_conv_transpose3d", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::slow_conv_transpose3d));
  m.impl("slow_conv_transpose3d.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::slow_conv_transpose3d_out));
  m.impl("slow_conv_transpose3d_backward.output_mask", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, const at::Tensor &, const at::Tensor &, std::array<bool,3>)>(&AtenXlaTypeDefault::slow_conv_transpose3d_backward));
  m.impl("thnn_conv2d_forward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::thnn_conv2d_forward));
  m.impl("thnn_conv2d_forward.output", static_cast<std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::thnn_conv2d_forward_out));
  m.impl("thnn_conv2d_backward.output_mask", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, const at::Tensor &, const at::Tensor &, std::array<bool,3>)>(&AtenXlaTypeDefault::thnn_conv2d_backward));
  m.impl("thnn_conv_depthwise2d_forward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::thnn_conv_depthwise2d_forward));
  m.impl("thnn_conv_depthwise2d_forward.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::thnn_conv_depthwise2d_forward_out));
  m.impl("thnn_conv_depthwise2d_backward.output_mask", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, std::array<bool,2>)>(&AtenXlaTypeDefault::thnn_conv_depthwise2d_backward));
  m.impl("conv_depthwise3d", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::conv_depthwise3d));
  m.impl("conv_depthwise3d_backward.grad_input", static_cast<std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::conv_depthwise3d_backward_out));
  m.impl("conv_depthwise3d_backward.output_mask", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, std::array<bool,3>)>(&AtenXlaTypeDefault::conv_depthwise3d_backward));
  m.impl("slow_conv3d_forward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::slow_conv3d_forward));
  m.impl("slow_conv3d_forward.output", static_cast<std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::slow_conv3d_forward_out));
  m.impl("slow_conv3d_backward.output_mask", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, const at::Tensor &, const at::Tensor &, std::array<bool,3>)>(&AtenXlaTypeDefault::slow_conv3d_backward));
  m.impl("slow_conv_dilated2d", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::slow_conv_dilated2d));
  m.impl("slow_conv_dilated2d_backward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, std::array<bool,3>)>(&AtenXlaTypeDefault::slow_conv_dilated2d_backward));
  m.impl("slow_conv_dilated3d", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, at::IntArrayRef, const c10::optional<at::Tensor> &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::slow_conv_dilated3d));
  m.impl("slow_conv_dilated3d_backward", static_cast<std::tuple<at::Tensor,at::Tensor,at::Tensor> (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, std::array<bool,3>)>(&AtenXlaTypeDefault::slow_conv_dilated3d_backward));
  m.impl("col2im", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::col2im));
  m.impl("col2im.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::col2im_out));
  m.impl("col2im_backward", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::col2im_backward));
  m.impl("col2im_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::col2im_backward_out));
  m.impl("im2col", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::im2col));
  m.impl("im2col.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::im2col_out));
  m.impl("im2col_backward", static_cast<at::Tensor (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef)>(&AtenXlaTypeDefault::im2col_backward));
  m.impl("im2col_backward.grad_input", static_cast<at::Tensor & (*)(const at::Tensor &, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::IntArrayRef, at::Tensor &)>(&AtenXlaTypeDefault::im2col_backward_out));
  m.impl("record_stream", static_cast<void (*)(at::Tensor &, at::Stream)>(&AtenXlaTypeDefault::record_stream));
  m.impl("isposinf.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::isposinf_out));
  m.impl("isneginf.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::isneginf_out));
  m.impl("special_entr.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::special_entr_out));
  m.impl("special_xlog1py.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::special_xlog1py_out));
  m.impl("special_i0e.out", static_cast<at::Tensor & (*)(const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::special_i0e_out));
  m.impl("linalg_cholesky_ex", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, bool)>(&AtenXlaTypeDefault::linalg_cholesky_ex));
  m.impl("linalg_cholesky_ex.L", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, bool, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::linalg_cholesky_ex_out));
  m.impl("linalg_lstsq.out", static_cast<std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> (*)(const at::Tensor &, const at::Tensor &, c10::optional<double>, c10::optional<c10::string_view>, at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::linalg_lstsq_out));
  m.impl("linalg_slogdet", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &)>(&AtenXlaTypeDefault::linalg_slogdet));
  m.impl("linalg_slogdet.out", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::linalg_slogdet_out));
  m.impl("linalg_eig", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &)>(&AtenXlaTypeDefault::linalg_eig));
  m.impl("linalg_eig.out", static_cast<std::tuple<at::Tensor &,at::Tensor &> (*)(const at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::linalg_eig_out));
  m.impl("linalg_householder_product", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &)>(&AtenXlaTypeDefault::linalg_householder_product));
  m.impl("linalg_householder_product.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::linalg_householder_product_out));
  m.impl("_linalg_inv_out_helper_", static_cast<at::Tensor & (*)(at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::_linalg_inv_out_helper_));
  m.impl("linalg_vector_norm", static_cast<at::Tensor (*)(const at::Tensor &, const at::Scalar &, c10::optional<at::IntArrayRef>, bool, c10::optional<at::ScalarType>)>(&AtenXlaTypeDefault::linalg_vector_norm));
  m.impl("linalg_vector_norm.out", static_cast<at::Tensor & (*)(const at::Tensor &, const at::Scalar &, c10::optional<at::IntArrayRef>, bool, c10::optional<at::ScalarType>, at::Tensor &)>(&AtenXlaTypeDefault::linalg_vector_norm_out));
  m.impl("_linalg_solve_out_helper_", static_cast<at::Tensor & (*)(at::Tensor &, at::Tensor &, at::Tensor &)>(&AtenXlaTypeDefault::_linalg_solve_out_helper_));
  m.impl("_linalg_qr_helper", static_cast<std::tuple<at::Tensor,at::Tensor> (*)(const at::Tensor &, c10::string_view)>(&AtenXlaTypeDefault::_linalg_qr_helper));
  m.impl("_test_optional_intlist", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<at::IntArrayRef>)>(&AtenXlaTypeDefault::_test_optional_intlist));
  m.impl("_test_optional_filled_intlist", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<at::IntArrayRef>)>(&AtenXlaTypeDefault::_test_optional_filled_intlist));
  m.impl("_test_optional_floatlist", static_cast<at::Tensor (*)(const at::Tensor &, c10::optional<at::ArrayRef<double>>)>(&AtenXlaTypeDefault::_test_optional_floatlist));
  m.impl("segment_reduce", static_cast<at::Tensor (*)(const at::Tensor &, c10::string_view, const c10::optional<at::Tensor> &, const c10::optional<at::Tensor> &, int64_t, bool, const c10::optional<at::Scalar> &)>(&AtenXlaTypeDefault::segment_reduce));
  m.impl("segment_reduce_backward", static_cast<at::Tensor (*)(const at::Tensor &, const at::Tensor &, const at::Tensor &, const c10::optional<at::Tensor> &)>(&AtenXlaTypeDefault::segment_reduce_backward));

}

}  // namespace torch_xla
