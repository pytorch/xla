diff --git a/torch/optim/optimizer.py b/torch/optim/optimizer.py
index 13770d73f..540a198ba 100644
--- a/torch/optim/optimizer.py
+++ b/torch/optim/optimizer.py
@@ -14,6 +14,11 @@ class _RequiredParameter(object):
 required = _RequiredParameter()
 
 
+def _is_tensor(t):
+    import torch_xla
+    return isinstance(t, torch.Tensor) or isinstance(t, torch_xla._XLAC.XLATensor)
+
+
 class Optimizer(object):
     r"""Base class for all optimizers.
 
@@ -32,7 +37,7 @@ class Optimizer(object):
     def __init__(self, params, defaults):
         self.defaults = defaults
 
-        if isinstance(params, torch.Tensor):
+        if _is_tensor(params):
             raise TypeError("params argument given to the optimizer should be "
                             "an iterable of Tensors or dicts, but got " +
                             torch.typename(params))
@@ -85,7 +90,7 @@ class Optimizer(object):
             return packed
         param_groups = [pack_group(g) for g in self.param_groups]
         # Remap state to use ids as keys
-        packed_state = {(id(k) if isinstance(k, torch.Tensor) else k): v
+        packed_state = {(id(k) if _is_tensor(k) else k): v
                         for k, v in self.state.items()}
         return {
             'state': packed_state,
@@ -121,7 +126,7 @@ class Optimizer(object):
 
         def cast(param, value):
             r"""Make a deep copy of value, casting all tensors to device of param."""
-            if isinstance(value, torch.Tensor):
+            if _is_tensor(value):
                 # Floating-point types are a bit special here. They are the only ones
                 # that are assumed to always match the type of params.
                 if param.is_floating_point():
@@ -184,7 +189,7 @@ class Optimizer(object):
         assert isinstance(param_group, dict), "param group must be a dict"
 
         params = param_group['params']
-        if isinstance(params, torch.Tensor):
+        if _is_tensor(params):
             param_group['params'] = [params]
         elif isinstance(params, set):
             raise TypeError('optimizer parameters need to be organized in ordered collections, but '
@@ -193,7 +198,7 @@ class Optimizer(object):
             param_group['params'] = list(params)
 
         for param in param_group['params']:
-            if not isinstance(param, torch.Tensor):
+            if not _is_tensor(param):
                 raise TypeError("optimizer can only optimize Tensors, "
                                 "but one of the params is " + torch.typename(param))
             if not param.is_leaf:
