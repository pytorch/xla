{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12bbb3d6",
   "metadata": {},
   "source": [
    "# Controlling Floating Point Precision\n",
    " **Author:** `Yaoshiang Ho`\n",
    "\n",
    " **Date created:** 2025/05/15\n",
    "\n",
    " **Last modified:** 2025/05/15\n",
    "\n",
    " In this tutorial, you will learn how to control the floating point\n",
    " precision of matrix multiplication (mat mul) operations\n",
    " when using certain accelerators with PyTorch/XLA, such as TPUs.\n",
    " You will also learn how to access torch's floating point info\n",
    " and how to visually inspect the floating point representation of numbers.\n",
    "\n",
    " **Acknowledgments**\n",
    "\n",
    " 1) This tutorial uses a diagram from HuggingFace.\n",
    " 2) This tutorial draws on the JAX documentation for matmul precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad22835",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    " Google TPUs are built with matrix multiplication optimized on silicon\n",
    " in a physical module called a Matrix Multiply Unit or MXU.\n",
    " To maintain speed, researchers identified an inexpensive tradeoff.\n",
    " [The research](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)\n",
    " showed that neural networks were able to train with less precision\n",
    " than FP32\n",
    " \\\"without having any noticeable impact on model accuracy\\\".\n",
    " The same was not true for range. Due to operations like norms,\n",
    " FP32's range was important to keep. The solution was bfloat16:\n",
    " the same range as FP32, with less precision.\n",
    "\n",
    " Nvidia V100 and newer GPUs also include specialized matrix multiplication\n",
    " units called TensorCores. These GPUs use a numerical format called\n",
    " TF32, which has the same range as FP32 and bfloat16, but an\n",
    " intermediate precision (10 bits of mantissa)\n",
    " because TF32 only has 19 total bits.\n",
    "\n",
    " Matrix multiplication operations performed on FP32 values will\n",
    " yield results in bfloat16 for TPUs and BF19 for Nvidia GPUs.\n",
    "\n",
    " ![bits layout](../_static/img/bit_layout.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f62e2f",
   "metadata": {},
   "source": [
    "## Higher precision math on lower precision hardware\n",
    "\n",
    " Even with the 7 mantissa bits of bfloat16, it is possible to calculate\n",
    " math in higher precision. This is done by breaking up a number into\n",
    " its components. To build intuition, imagine an MXU\n",
    " that supports 2 digits in a base-10 (decimal) number system.\n",
    " The goal is to multiply numbers with\n",
    " 4 digits of precision, for example, $9.111$ and $9.222$.\n",
    " In infinite precision, the product is $8.4021642e-1$. Notice that\n",
    " two numbers with 4 digits of precision generates twice as many\n",
    " digits of precision in the result.\n",
    "\n",
    " The simplest approach is to round the numbers to $9.1$ and $9.2$,\n",
    " resulting in $8.372e-1$. This is conceptually the \"default\" precision\n",
    " setting of PyTorch/XLA on TPU.\n",
    "\n",
    " The next approach is to break each number up into two parts,\n",
    " high and low (H and L):\n",
    " $(9.1 + 0.011) \\times (9.2 + 0.022)$.\n",
    " This equals $(H \\times H + H \\times L + L \\times H + L \\times L)$.\n",
    " The fourth term is negligible, so it is ignored. The first three matrix\n",
    " multiplications comprise the three-pass approach and it roughly\n",
    " doubles the effective precision.\n",
    "\n",
    " ![decomposing multiplication](../_static/img/decomposing_multiplication.png)\n",
    "\n",
    " Extending this approach again would yield roughly triple the precision.\n",
    " The idea is to break the number into\n",
    " into high, medium, and low (H, M, and L), generating nine possible terms:\n",
    " $(H + M + L) \\times (H + M + L) = HH + HM + MH + MM + HL + LH + ML + LM + LL$.\n",
    " The final three are ignored, and the first six comprise the six-pass approach.\n",
    " It is essentially equivalent to FP32, with some room for variances in the minor bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865ebbc",
   "metadata": {},
   "source": [
    "## PyTorch/XLA and TPUs\n",
    " PyTorch/XLA allows control of the one pass, three pass, and six pass\n",
    " approach in the `[torch_xla.backends.set_mat_mul_precision()]` function.\n",
    " The valid values are `default`, `high`, and `highest`. Now, you'll investigate\n",
    " the differences between these three settings.\n",
    "\n",
    " Warning: Although this notebook demonstrates different precision settings,\n",
    " it is recommended to only set the precision once at the beginning of your\n",
    " script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b815c82f",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    " Make sure you are running this tutorial on a TPU. You can\n",
    " access a TPU using Google Colab.\n",
    "\n",
    " Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21404537",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T22:43:53.762304Z",
     "iopub.status.busy": "2025-05-09T22:43:53.761947Z",
     "iopub.status.idle": "2025-05-09T22:43:56.756028Z",
     "shell.execute_reply": "2025-05-09T22:43:56.755525Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_xla.backends\n",
    "\n",
    "torch.set_printoptions(precision=20, sci_mode=False, linewidth=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc954a",
   "metadata": {},
   "source": [
    "Epsilon is the minimum difference between 1.0 and the next highest representable number. Retrieve the value out from torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531e53a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T22:43:56.757699Z",
     "iopub.status.busy": "2025-05-09T22:43:56.757418Z",
     "iopub.status.idle": "2025-05-09T22:43:56.760311Z",
     "shell.execute_reply": "2025-05-09T22:43:56.759882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfloat16 epsilon: 0.0078125\n",
      "return type of torch.finfo: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "eps = torch.finfo(torch.bfloat16).eps\n",
    "print(f\"bfloat16 epsilon: {eps}\")\n",
    "print(f\"return type of torch.finfo: {type(eps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15797f95",
   "metadata": {},
   "source": [
    "The epsilon is also defined as 1 / 2^p, where p is the number of bits in the mantissa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57fa5977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T22:43:56.761542Z",
     "iopub.status.busy": "2025-05-09T22:43:56.761345Z",
     "iopub.status.idle": "2025-05-09T22:43:56.763745Z",
     "shell.execute_reply": "2025-05-09T22:43:56.763334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0078125\n"
     ]
    }
   ],
   "source": [
    "print(1 / (2**7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411c084",
   "metadata": {},
   "source": [
    "Numbers in between may get rounded up to 1.0 + epsilon, or down to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00f5d314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T22:43:56.765098Z",
     "iopub.status.busy": "2025-05-09T22:43:56.764807Z",
     "iopub.status.idle": "2025-05-09T22:43:56.768824Z",
     "shell.execute_reply": "2025-05-09T22:43:56.768394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.00000000000000000000, 1.00000000000000000000, 1.00000000000000000000, 1.00781250000000000000, 1.00781250000000000000], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    torch.tensor(\n",
    "        [1.0, 1.0 + eps / 4.0, 1.0 + eps / 2, 1.0 + eps * 3 / 4, 1.0 + eps],\n",
    "        dtype=torch.bfloat16,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5f852",
   "metadata": {},
   "source": [
    "## Get ready to look directly at bits\n",
    "\n",
    " Set up tools to convert binary strings to FP32 numbers, and vice\n",
    " versa. Create a function to generate a random matrix.\n",
    "\n",
    " In general, when\n",
    " testing an MXU (or TensorCore), pass matrices to encourage XLA to\n",
    " use the MXU rather than the slower but more precise units for FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3dfab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T22:43:56.770080Z",
     "iopub.status.busy": "2025-05-09T22:43:56.769897Z",
     "iopub.status.idle": "2025-05-09T22:43:56.773942Z",
     "shell.execute_reply": "2025-05-09T22:43:56.773526Z"
    }
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "\n",
    "def binary_fraction_to_fp32(bstr: str) -> float:\n",
    "  fraction_bits = bstr[4:]  # skip \"0b1.\"\n",
    "  mantissa = 1.0\n",
    "  for i, bit in enumerate(fraction_bits):\n",
    "    mantissa += int(bit) * 2**-(i + 1)\n",
    "  return float(mantissa)\n",
    "\n",
    "\n",
    "def fp32_to_binary_fraction(fp32_float: float) -> str:\n",
    "  x_bytes = struct.pack(\">f\", fp32_float)  # Big-endian IEEE 754 float32\n",
    "  as_int = struct.unpack(\">I\", x_bytes)[0]  # Interpret bits as uint32\n",
    "  sign = (as_int >> 31) & 0b1\n",
    "  exponent = (as_int >> 23) & 0xFF\n",
    "  mantissa = as_int & 0x7FFFFF  # lower 23 bits\n",
    "  return f\"FORMAT:0b SIGN:{sign} EXPONENT:{exponent:08b} MANTISSA:{mantissa:023b} VALUE={fp32_float}\"\n",
    "\n",
    "\n",
    "def get_rand_matrix():\n",
    "  \"\"\"Returns a diagonal matrix of shape 1024, 1024, values between 0.999 and 1.111\"\"\"\n",
    "  eye = torch.eye(1024, dtype=torch.float32, device=\"xla\")\n",
    "  rand_ = torch.rand(\n",
    "      (1024, 1024), dtype=torch.float32, device=\"xla\") * 0.2 + 0.9\n",
    "  result = eye * rand_\n",
    "  assert torch.nonzero(result).size(0) == 1024, torch.nonzero(result).size(0)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb80ea1",
   "metadata": {},
   "source": [
    "## Examining a number\n",
    "\n",
    " Generate an FP32 number representing 1 + bf16_eps/2. This\n",
    " will put one extra bit out of reach of a bfloat16's mantissa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64debffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T22:43:56.775209Z",
     "iopub.status.busy": "2025-05-09T22:43:56.774980Z",
     "iopub.status.idle": "2025-05-09T22:43:56.777529Z",
     "shell.execute_reply": "2025-05-09T22:43:56.777102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32     : 1.00390625\n",
      "1 + eps/2: 1.00390625\n"
     ]
    }
   ],
   "source": [
    "one_plus_half_eps = binary_fraction_to_fp32(\"0b1.\" + \"0\" * 7 + \"1\" + \"0\" * 15)\n",
    "print(f\"FP32     : {one_plus_half_eps }\")\n",
    "print(f\"1 + eps/2: {1.0 + eps / 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b30036",
   "metadata": {},
   "source": [
    "Print the bits for FP32 and BF16. Notice that the 8th bit is lost.\n",
    " This reconfirms that BF16 cannot represent the 8th bit of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e7653f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T22:43:56.778777Z",
     "iopub.status.busy": "2025-05-09T22:43:56.778600Z",
     "iopub.status.idle": "2025-05-09T22:43:56.781376Z",
     "shell.execute_reply": "2025-05-09T22:43:56.780949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: FORMAT:0b SIGN:0 EXPONENT:01111111 MANTISSA:00000001000000000000000 VALUE=1.00390625\n",
      "BF16: FORMAT:0b SIGN:0 EXPONENT:01111111 MANTISSA:00000000000000000000000 VALUE=1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"FP32: {fp32_to_binary_fraction(one_plus_half_eps)}\")\n",
    "ones_bf16 = torch.tensor(\n",
    "    one_plus_half_eps, dtype=torch.bfloat16).to(torch.float32).item()\n",
    "print(f\"BF16: {fp32_to_binary_fraction(ones_bf16)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de36bf",
   "metadata": {},
   "source": [
    "## MXU\n",
    "\n",
    " Place your numbers of interest in a diagonal matrix. By putting them in\n",
    " a matrix, XLA will execute the math on the MXU. By making the matrices\n",
    " diagonal, the math will be equivalent to element-wise multiplication.\n",
    "\n",
    " Notice that the values are essentially rounded down to 1.0 before\n",
    " being multiplied, resulting in 1.0 as the output.\n",
    " This is the loss of precision that occurs in a TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ebdc29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T22:43:56.782605Z",
     "iopub.status.busy": "2025-05-09T22:43:56.782431Z",
     "iopub.status.idle": "2025-05-09T22:44:18.979070Z",
     "shell.execute_reply": "2025-05-09T22:44:18.978398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: FORMAT:0b SIGN:0 EXPONENT:01111111 MANTISSA:00000001000000000000000 VALUE=1.00390625\n",
      "Y: FORMAT:0b SIGN:0 EXPONENT:01111111 MANTISSA:00000001000000000000000 VALUE=1.00390625\n",
      "Z: FORMAT:0b SIGN:0 EXPONENT:01111111 MANTISSA:00000000000000000000000 VALUE=1.0\n"
     ]
    }
   ],
   "source": [
    "X = get_rand_matrix()\n",
    "Y = get_rand_matrix()\n",
    "X[0, 0] = one_plus_half_eps\n",
    "Y[0, 0] = one_plus_half_eps\n",
    "Z = torch.matmul(X, Y)\n",
    "print(f\"X: {fp32_to_binary_fraction(X[0][0].item())}\")\n",
    "print(f\"Y: {fp32_to_binary_fraction(Y[0][0].item())}\")\n",
    "print(f\"Z: {fp32_to_binary_fraction(Z[0][0].item())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05335a2b",
   "metadata": {},
   "source": [
    "## FP32 precision on bfloat16 hardware\n",
    "\n",
    " The 3 and 6 pass approaches generate more bits of precision.\n",
    " Turn on the highest precision mode (six passes) and run\n",
    " the experiment again. Notice that the TPU has calculated FP32 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e76e738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T22:44:18.980622Z",
     "iopub.status.busy": "2025-05-09T22:44:18.980441Z",
     "iopub.status.idle": "2025-05-09T22:44:19.352262Z",
     "shell.execute_reply": "2025-05-09T22:44:19.351575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_ref: FORMAT:0b SIGN:0 EXPONENT:01111111 MANTISSA:00000010000000010000000 VALUE=1.0078277587890625\n",
      "Z:     FORMAT:0b SIGN:0 EXPONENT:01111111 MANTISSA:00000010000000010000000 VALUE=1.0078277587890625\n"
     ]
    }
   ],
   "source": [
    "Z_ref = torch.matmul(\n",
    "    X.to(\"cpu\").to(torch.float32),\n",
    "    Y.to(\"cpu\").to(torch.float32))\n",
    "print(f\"Z_ref: {fp32_to_binary_fraction(Z_ref[0][0].item())}\")\n",
    "torch_xla.backends.set_mat_mul_precision(\"highest\")\n",
    "Z = torch.matmul(X, Y)\n",
    "print(f\"Z:     {fp32_to_binary_fraction(Z[0][0].item())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a914c7e",
   "metadata": {},
   "source": [
    "## Edge-case numbers\n",
    "\n",
    " In the previous example, you saw no difference between\n",
    " the six-pass and FP32 multiplication. Now, you'll use an edge\n",
    " case number to demonstrate a difference in the\n",
    " final bit between the six pass approach and full FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c8c6a10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T22:44:19.353765Z",
     "iopub.status.busy": "2025-05-09T22:44:19.353585Z",
     "iopub.status.idle": "2025-05-09T22:44:20.005015Z",
     "shell.execute_reply": "2025-05-09T22:44:20.004365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_ref: FORMAT:0b SIGN:0 EXPONENT:01111111 MANTISSA:01110000101000111101100 VALUE=1.440000057220459\n",
      "Z:     FORMAT:0b SIGN:0 EXPONENT:01111111 MANTISSA:01110000101000111101101 VALUE=1.4400001764297485\n"
     ]
    }
   ],
   "source": [
    "X = get_rand_matrix()\n",
    "Y = get_rand_matrix()\n",
    "X[0, 0] = 1.2\n",
    "Y[0, 0] = 1.2\n",
    "Z_ref = torch.matmul(\n",
    "    X.to(\"cpu\").to(torch.float32),\n",
    "    Y.to(\"cpu\").to(torch.float32))\n",
    "print(f\"Z_ref: {fp32_to_binary_fraction(Z_ref[0][0].item())}\")\n",
    "torch_xla.backends.set_mat_mul_precision(\"highest\")\n",
    "Z = torch.matmul(X, Y)\n",
    "print(f\"Z:     {fp32_to_binary_fraction(Z[0][0].item())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdd678",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    " In this tutorial, you learned how to control the floating point\n",
    " precision of your matrix multiplication (mat mul) operations.\n",
    " You also learned the internal algorithm used to generate\n",
    " higher precision through the three-pass and six-pass approaches."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
