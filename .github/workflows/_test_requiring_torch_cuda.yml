name: xla-test-requiring-torch-cuda
on:
  workflow_call:
    inputs:
      dev-image:
        required: true
        type: string
        description: Base image for builds
      runner:
        required: false
        type: string
        description: Runner type for the test
        default: linux.12xlarge
      collect-coverage:
        required: false
        type: boolean
        description: Set to true to collect coverage information
        default: false
      timeout-minutes:
        required: false
        type: number
        default: 30
        description: |
          Set the maximum (in minutes) how long the workflow should take to finish
            timeout-minutes:
      torch-commit:
        required: true
        type: string
        description: torch-commit

jobs:
  test:
    container:
      image: ${{ inputs.dev-image }}
      options: "--gpus all --shm-size 16g"
    strategy:
      matrix:
        include:
          - name: 'torch_with_cuda_python_tests'
            run_python_tests: 'python_tests'
            runner: ${{ inputs.runner }}
          - name: 'torch_with_cuda_triton_tests'
            run_triton_tests: 'triton_tests'
            runner: 'linux.g5.4xlarge.nvidia.gpu'
    runs-on: ${{ matrix.runner }}
    timeout-minutes: ${{ inputs.timeout-minutes }}
    env:
      USE_COVERAGE: ${{ inputs.collect-coverage && '1' || '0' }}
      COVERAGE_DIR: '/tmp/lcov'
      BAZEL_JOBS: ''  # Let bazel decide the parallelism based on the number of CPUs.
      BAZEL_REMOTE_CACHE: 1
      PJRT_DEVICE: 'CUDA'
    steps:
      - name: Checkout actions
        uses: actions/checkout@v4
        with:
          sparse-checkout: |
            .github/workflows/setup
          path: .actions
      - name: Setup
        uses: ./.actions/.github/workflows/setup
        with:
          torch-commit: ${{ inputs.torch-commit }}
          cuda: true
          wheels-artifact: torch-xla-wheels
          cuda-plugin-artifact: cuda-plugin
          cuda-torch-artifact: torch-with-cuda
      - name: Check GPU
        run: nvidia-smi
      - name: Install wheels
        shell: bash
        run: |
          pip install /tmp/wheels/*.whl
          # TODO: Add these in setup.py
          pip install fsspec
          pip install rich

          echo "Import check..."
          python -c "import torch, torch_xla, torchvision"
          echo "Import check done."
          echo "Check if CUDA is available for PyTorch..."
          python -c "import torch; assert torch.cuda.is_available()"
          echo "CUDA is available for PyTorch."
      - name: Checkout PyTorch Repo
        uses: actions/checkout@v4
        with:
          repository: pytorch/pytorch
          path: pytorch
          ref: ${{ inputs.torch-commit }}
      - name: Checkout PyTorch/XLA Repo
        uses: actions/checkout@v4
        with:
          path: pytorch/xla
      - name: Extra CI deps
        shell: bash
        run: |
          set -x
          pip install -U --pre jax jaxlib "jax-cuda12-plugin[with_cuda]" jax-cuda12-pjrt -f https://storage.googleapis.com/jax-releases/jax_nightly_releases.html
        if: ${{ matrix.run_triton_tests }}
      - name: Extra Coverage deps
        shell: bash
        run: |
          set -x
          pip install -U coverage coverage-lcov
        if: ${{ inputs.collect-coverage }}
      - name: Install Triton
        shell: bash
        run: |
          cd pytorch
          make triton
      - name: Python Tests
        shell: bash
        env:
          COVERAGE_FILE: "${{ env.COVERAGE_DIR }}/py-coverage"
          GCOV_PREFIX: "${{ env.COVERAGE_DIR }}/cpp-coverage"
        run: |
          set -xue

          TORCH_XLA_DIR=$(cd ~; dirname "$(python -c 'import torch_xla; print(torch_xla.__file__)')")

          function run_coverage {
            if [ "$USE_COVERAGE" != "0" ]; then
              coverage run --source="$TORCH_XLA_DIR" -p "$@"
            else
              python3 "$@"
            fi
          }

          run_coverage pytorch/xla/test/test_operations.py -v
          run_coverage pytorch/xla/test/dynamo/test_dynamo.py -v

          coverage combine
          coverage-lcov --data_file_path $COVERAGE_FILE --output_file_path $COVERAGE_FILE.info
        if: ${{ matrix.run_python_tests }}
      - name: Triton Tests
        shell: bash
        env:
          COVERAGE_FILE: "${{ env.COVERAGE_DIR }}/py-coverage"
          GCOV_PREFIX: "${{ env.COVERAGE_DIR }}/cpp-coverage"
          TRITON_PTXAS_PATH: "/usr/local/cuda-12.3/bin/ptxas"
        run: |
          set -x

          TORCH_XLA_DIR=$(cd ~; dirname "$(python -c 'import torch_xla; print(torch_xla.__file__)')")

          function run_coverage {
            if [ "$USE_COVERAGE" != "0" ]; then
              coverage run --source="$TORCH_XLA_DIR" -p "$@"
            else
              python3 "$@"
            fi
          }

          run_coverage pytorch/xla/test/test_triton.py

          coverage combine
          coverage-lcov --data_file_path $COVERAGE_FILE --output_file_path $COVERAGE_FILE.info
        if: ${{ matrix.run_triton_tests }}
      - name: Upload coverage results
        uses: actions/upload-artifact@v4
        with:
          name: "lcov-${{ matrix.name }}"
          path: "${{ env.COVERAGE_DIR }}"
        if: ${{ inputs.collect-coverage }}
