# Benchmarking

The two main benchmarking scripts are 
  - `experiment_runner.py` to run benchmark experiments, and 
  - `result_analyzer.py` to aggregate the benchmark result in CSV form.


## Reducing benchmark noise 

It is important to keep the benchmark runs safe from external effects 
to reduce noise. Do the following:

Sets the CPU statically to the highest tuneable frequency.
Prevent energy saving features to kick in.

```sudo cpupower frequency-set --governor performance```

Lock GPU clocks to lower frequency to reduce the chance of thermal throttling. Choose
FREQ based on your GPU info. To find out clock frequency on your device run:
`nvidia-smi -q -d CLOCK`, and look for Graphics/SM in Max Clocks section.
Setting the clock a couple hundrend MHz below, or ~80% of max
will most likely prevent thermal throttling effects.

```FREQ=... nvidia-smi --lock-gpu-clocks=$FREQ,$FREQ```

Disable autoboost selecting clock rate based on thermal, and power budget effects.
```CUDA_AUTO_BOOST=0```

## Experiment runner

Run the `experiment_runner.py` from the `pytorch` directory, which should be the
parent of the `xla` directory.

The following example runs the alexnet benchmark on GPU through the
Pytorch/XLA-dynamo path and through the Inductor-dynamo with 5 repetitions each.
The results will be stored in a json file in `experiment_results`.

```
cd pytorch
python xla/benchmarks/experiment_runner.py                   \
    --dynamo=openxla_eval --dynamo=openxla --dynamo=inductor \
    --xla=PJRT --xla=None                                    \
    --test=eval --test=train                                 \
    --suite-name=torchbench                                  \
    --accelerator=cuda                                       \
    --output-dirname=experiment_results                      \
    --repeat=5                                               \
    --print-subprocess                                       \
    --no-resume                                              \
    --filter="^alexnet$"
```

You can change the flags to add the configurations you are interested in. The
`experiment_runner.py` will expand the options to all supported configurations.
For example, in the case above, it will consider all the possible combinations
among the flags `--dynamo`, `--xla`, and `--test`, 4 of which are supported:

  - `dynamo=openxla_eval`, `xla=PJRT`, `test=eval`
  - `dynamo=openxla`, `xla=PJRT`, `test=train`
  - `dynamo=inductor`, `xla=None`, `test=eval`
  - `dynamo=inductor`, `xla=None`, `test=train`


## Result analyzer

Run the `result_analyzer.py` from the `pytorch` directory, which should be the
parent of the `xla` directory.

The following example analyzes the results generated by the above invocation of
`experiment_runner.py`. The aggregates are saved in CSV format in
`experiment_results/metric_report.csv`.

```
cd pytorch
python xla/benchmarks/result_analyzer.py --output-dirname=experiment_results
```
