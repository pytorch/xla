diff --git a/test/args_parse.py b/test/args_parse.py
index b4413b7a..208675a3 100644
--- a/test/args_parse.py
+++ b/test/args_parse.py
@@ -10,6 +10,12 @@ def parse_common_options(datadir=None,
                          batch_size=128,
                          num_epochs=10,
                          num_workers=4,
+                         prefetch_factor=8,
+                         pin_memory=False,
+                         persistent_workers=True,
+                         loader_prefetch_size=8,
+                         device_prefetch_size=4,
+                         cpu_to_device_transfer_threads=1,
                          log_steps=20,
                          lr=None,
                          momentum=None,
@@ -23,6 +29,12 @@ def parse_common_options(datadir=None,
   parser.add_argument('--batch_size', type=int, default=batch_size)
   parser.add_argument('--num_epochs', type=int, default=num_epochs)
   parser.add_argument('--num_workers', type=int, default=num_workers)
+  parser.add_argument("--prefetch_factor", type=int, default=prefetch_factor)
+  parser.add_argument('--pin_memory', type=bool, default=pin_memory)
+  parser.add_argument('--persistent_workers', type=bool, default=persistent_workers)
+  parser.add_argument("--loader_prefetch_size", type=int, default=loader_prefetch_size)
+  parser.add_argument("--device_prefetch_size", type=int, default=device_prefetch_size)
+  parser.add_argument("--cpu_to_device_transfer_threads", type=int, default=cpu_to_device_transfer_threads)
   parser.add_argument('--log_steps', type=int, default=log_steps)
   parser.add_argument('--profiler_port', type=int, default=profiler_port)
   parser.add_argument('--lr', type=float, default=lr)
@@ -42,4 +54,4 @@ def parse_common_options(datadir=None,
   # Setup import folders.
   xla_folder = os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0])))
   sys.path.append(os.path.join(os.path.dirname(xla_folder), 'test'))
-  return args
+  return args
\ No newline at end of file
diff --git a/torch_xla/distributed/parallel_loader.py b/torch_xla/distributed/parallel_loader.py
index 44b9f888..7cb6ccbd 100644
--- a/torch_xla/distributed/parallel_loader.py
+++ b/torch_xla/distributed/parallel_loader.py
@@ -1,3 +1,7 @@
+from __future__ import division
+from __future__ import print_function
+
+from six import iteritems, itervalues
 import threading
 import torch
 import torch_xla
@@ -69,9 +73,6 @@ class ParallelLoader(object):
       where the worker threads deposit tensors which have already been sent to
       devices.
       Default: 4
-    host_to_device_transfer_threads (int, optional): The number of threads that
-      work in parallel to transfer data from loader queue to device queue.
-      Default: 1
   """
 
   def __init__(self,
@@ -81,26 +82,24 @@ class ParallelLoader(object):
                batches_per_execution=1,
                loader_prefetch_size=8,
                device_prefetch_size=4,
-               host_to_device_transfer_threads=1,
-               input_sharding=None):
+               host_to_device_transfer_threads=1):
     self._loader = loader
     self._devices = [torch.device(x) for x in devices]
     self._batchdim = batchdim
     self._batches_per_execution = batches_per_execution
     self._done = False
     self._queues = dict()
-    self._input_sharding = input_sharding
     for device in self._devices:
       self._queues[device] = PerDeviceQueue(device, loader_prefetch_size,
                                             device_prefetch_size)
     thread = threading.Thread(target=self._loader_worker)
     thread.daemon = True
     thread.start()
-    for dqueue in self._queues.values():
+    for dqueue in itervalues(self._queues):
       for i in range(host_to_device_transfer_threads):
-        thread = threading.Thread(target=self._worker, args=(dqueue,))
-        thread.daemon = True
-        thread.start()
+          thread = threading.Thread(target=self._worker, args=(dqueue,))
+          thread.daemon = True
+          thread.start()
 
   def per_device_loader(self, device):
     """Retrieves the loader iterator object for the given device.
@@ -125,7 +124,7 @@ class ParallelLoader(object):
 
   def close(self):
     self._done = True
-    for dqueue in self._queues.values():
+    for dqueue in itervalues(self._queues):
       dqueue.queue.close()
       dqueue.loader_queue.close()
 
@@ -134,41 +133,45 @@ class ParallelLoader(object):
     return self._batches_per_execution
 
   def _loader_worker(self):
-    queues = list(self._queues.values())
-    data_iter = enumerate(self._loader)
-    batch = []
-    while not self._done:
-      try:
-        _, data = next(data_iter)
-      except StopIteration:
-        break
-      batch.append(data)
-      if len(batch) == len(self._devices):
-        for queue_no, device_batch in enumerate(batch):
-          queues[queue_no].loader_queue.put(device_batch)
+    with xp.Trace('_loader_worker'):
+        queues = list(self._queues.values())
+        data_iter = enumerate(self._loader)
         batch = []
-    for dqueue in queues:
-      dqueue.loader_queue.close_write()
+        while not self._done:
+          try:
+            _, data = next(data_iter)
+          except StopIteration:
+            break
+          batch.append(data)
+          if len(batch) == len(self._devices):
+            for queue_no, device_batch in enumerate(batch):
+              queues[queue_no].loader_queue.put(device_batch)
+            batch = []
+        for dqueue in queues:
+          dqueue.loader_queue.close_write()
 
   def _get_batch(self, dqueue):
-    batch = []
-    while dqueue.queue.max_size() > len(batch):
-      item = dqueue.loader_queue.get()
-      if item is None:
-        break
-      batch.append(item)
-    return batch
+    with xp.Trace('_get_batch'):
+        batch = []
+        while dqueue.queue.max_size() > len(batch):
+          item = dqueue.loader_queue.get()
+          if item is None:
+            break
+          batch.append(item)
+        return batch
 
   def _worker(self, dqueue):
-    device = torch.device(dqueue.device)
-    while True:
-      batch = self._get_batch(dqueue)
-      if not batch:
-        break
-      batch = xm.send_cpu_data_to_device(batch, device, self._input_sharding)
-      for data in batch:
-        dqueue.queue.put(data)
-    dqueue.queue.close_write()
+    with xp.Trace('_worker'):
+        device = torch.device(dqueue.device)
+        while True:
+          batch = self._get_batch(dqueue)
+          if not batch:
+            break
+          with xp.Trace('send_cpu_data_to_device'):
+              batch = xm.send_cpu_data_to_device(batch, device)
+          for data in batch:
+            dqueue.queue.put(data)
+        dqueue.queue.close_write()
 
 
 class MpDeviceLoader(object):
@@ -194,4 +197,4 @@ class MpDeviceLoader(object):
     return parallel_loader.per_device_loader(self._device)
 
   def __len__(self):
-    return len(self._loader)
+    return len(self._loader)
\ No newline at end of file
diff --git a/torch_xla/experimental/pjrt.py b/torch_xla/experimental/pjrt.py
index 73547300..d93442d2 100644
--- a/torch_xla/experimental/pjrt.py
+++ b/torch_xla/experimental/pjrt.py
@@ -4,17 +4,20 @@ import functools
 import itertools
 import logging
 import os
+import sys
+import tempfile
 from itertools import chain
 from typing import Callable, Dict, List, Optional, Tuple, TypeVar
 
 import torch
+import torch.distributed as dist
 import torch.nn as nn
 import torch_xla
 import torch_xla.core.xla_env_vars as xenv
 import torch_xla.core.xla_model as xm
 import torch_xla.distributed.xla_backend
 import torch_xla.utils.utils as xu
-from torch_xla.experimental import tpu, gpu
+from torch_xla.experimental import tpu
 
 R = TypeVar('R')
 FN = TypeVar('FN')
@@ -31,48 +34,14 @@ def set_device_type(pjrt_device: str) -> None:
   os.environ[xenv.PJRT_DEVICE] = pjrt_device
 
 
-def _maybe_select_default_device():
-  # Skip if runtime is already configured
-  if xu.getenv_as(
-      xenv.PJRT_SELECT_DEFAULT_DEVICE, str, '1'
-  ) == '0' or xenv.PJRT_DEVICE in os.environ or xenv.GPU_NUM_DEVICES in os.environ or any(
-      env.startswith('XRT_') for env in os.environ):
-    return
-
-  logging.warning(
-      'XRT configuration not detected. Defaulting to preview PJRT '
-      'runtime. To silence this warning and continue using PJRT, '
-      'explicitly set PJRT_DEVICE to a supported device or configure XRT. To '
-      'disable default device selection, set PJRT_SELECT_DEFAULT_DEVICE=0')
-  # TODO: Update this link in the release branch
-  logging.warning('For more information about the status of PJRT, see '
-                  'https://github.com/pytorch/xla/blob/master/docs/pjrt.md')
-  # Check for libtpu _and_ the TPU device
-  if torch_xla._found_libtpu and os.path.exists('/dev/accel0'):
-    logging.warning('libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.')
-    os.environ[xenv.PJRT_DEVICE] = 'TPU'
-  else:
-    logging.warning('Defaulting to PJRT_DEVICE=CPU')
-    os.environ[xenv.PJRT_DEVICE] = 'CPU'
-  # TODO(wcromar): Detect GPU device too
-
-
 def device_type() -> Optional[str]:
-  """Returns the currrent PjRt device type.
-
-  Selects a default device if none has been configured
-  """
-  _maybe_select_default_device()
+  """Returns the currrent PjRt device type."""
   pjrt_device = xu.getenv_as(xenv.PJRT_DEVICE, str)
-  return pjrt_device.split('_')[0] if pjrt_device else pjrt_device
+  return 'TPU' if pjrt_device and pjrt_device.startswith('TPU') else pjrt_device
 
 
 def using_pjrt() -> bool:
-  """Returns whether this process is using PjRt runtime.
-
-  Selects a default device if none has been configured.
-  """
-  _maybe_select_default_device()
+  """Returns whether this process is using PjRt runtime."""
   return device_type() is not None
 
 
@@ -122,7 +91,7 @@ def xla_device(n: Optional[int] = None,
 @requires_pjrt
 def local_process_count() -> int:
   """Returns the number of processes running on this host."""
-  return xu.getenv_as(xenv.PJRT_LOCAL_PROCESS_COUNT, int, defval=1)
+  return xu.getenv_as('LOCAL_WORLD_SIZE', int, defval=1)
 
 
 @requires_pjrt
@@ -169,7 +138,7 @@ def local_ordinal() -> int:
   """Returns local ordinal of this thread within this host.
 
   Local ordinal is in range [0, local_device_count)."""
-  local_rank = xu.getenv_as(xenv.PJRT_LOCAL_PROCESS_RANK, int, 0)
+  local_rank = xu.getenv_as('LOCAL_RANK', int, 0)
   devices_per_process = addressable_device_count()
   return local_rank * devices_per_process + xla_device().index
 
@@ -211,9 +180,11 @@ def _merge_replica_results(
 
 
 @requires_pjrt
-def _run_thread_per_device(
-    local_rank: int, local_world_size: int, fn: Callable[[], R],
-    initializer_fn: Callable[[int, int], None]) -> Dict[int, R]:
+def _run_thread_per_device(local_rank: int,
+                           local_world_size: int,
+                           fn: Callable[[], R],
+                           initializer_fn: Callable[[int, int], None],
+                           master_port: int = 12355) -> Dict[int, R]:
   """Runs `fn` in a separate thread on each addressable device.
 
   Args:
@@ -237,8 +208,24 @@ def _run_thread_per_device(
 
     return fn()
 
+  # TODO(wcromar): remove this when the TPU master IP becomes predictable
+  def _discover_tpu_master_worker_ip(device: torch.device):
+    torch_xla._XLAC._xla_set_default_device(device)
+
+    return tpu.discover_master_worker_ip()
+
   with concurrent.futures.ThreadPoolExecutor(
       max_workers=num_threads) as executor:
+    if os.getenv('PJRT_INIT_TORCH_DISTRIBUTED', '0') == '1':
+      if device_type() == 'TPU':
+        # HACK: need to call with each device since it relies on an XLA collective
+        master_ip = next(executor.map(_discover_tpu_master_worker_ip, devices))
+        init_method = f'tcp://{master_ip}:{master_port}'
+      else:
+        init_method = None
+
+      init_pjrt_process_group(init_method=init_method)
+
     device_ordinals = [
         torch_xla._XLAC._xla_get_device_ordinal(d) for d in devices
     ]
@@ -249,7 +236,11 @@ def _run_thread_per_device(
 
 
 @requires_pjrt
-def _run_singleprocess(fn: Callable[..., R], *args, **kwargs) -> Dict[int, R]:
+def _run_singleprocess(fn: Callable[..., R],
+                       *args,
+                       start_method: str = 'spawn',
+                       master_port: int = 12355,
+                       **kwargs) -> Dict[int, R]:
   """Runs `fn` on a single device core.
 
   Spawns one process on a single physical device (e.g. TPU chip).
@@ -257,25 +248,28 @@ def _run_singleprocess(fn: Callable[..., R], *args, **kwargs) -> Dict[int, R]:
   Args:
     fn: Function to run on the device devices
     args: args to pass to `fn`
+    start_method: The Python `multiprocessing` process creation method.
+      Default: `spawn`
     kwargs: kwargs to pass to `fn`
 
   Returns:
     the result of calling `fn`.
   """
-  os.environ.setdefault(xenv.PJRT_LOCAL_PROCESS_COUNT, '1')
+  os.environ.setdefault('LOCAL_WORLD_SIZE', '1')
 
   if device_type() == 'TPU':
     tpu.configure_one_chip_topology()
 
   xm.set_replication(xm.xla_device(), [])
+  init_pjrt_process_group()
 
-  return fn(*args, **kwargs)
+  return fn()
 
 
 @requires_pjrt
 def _initialize_multiprocess(local_rank: int, local_world_size: int):
-  os.environ.setdefault(xenv.PJRT_LOCAL_PROCESS_RANK, str(local_rank))
-  os.environ.setdefault(xenv.PJRT_LOCAL_PROCESS_COUNT, str(local_world_size))
+  #os.environ.setdefault('LOCAL_RANK', str(local_rank))
+  os.environ.setdefault('LOCAL_WORLD_SIZE', str(local_world_size))
 
   if device_type() == 'TPU':
     tpu.configure_topology(local_rank, local_world_size)
@@ -303,9 +297,6 @@ def _run_multiprocess(fn: Callable[..., R],
   """
   if device_type() == 'TPU':
     num_processes = tpu.num_local_processes()
-  elif device_type() == 'GPU':
-    num_processes = gpu.num_local_processes()
-    gpu.initialize_distributed_runtime(num_processes)
   else:
     num_processes = 1
 
@@ -323,9 +314,6 @@ def _run_multiprocess(fn: Callable[..., R],
         itertools.chain.from_iterable(
             result.items() for result in process_results))
 
-  if device_type() == 'GPU':
-    gpu.shutdown_distributed_runtime()
-
   return _merge_replica_results(replica_results)
 
 
@@ -358,7 +346,7 @@ def spawn(fn: Callable,
   spawn_fn = _SpawnFn(fn, *args)
 
   if nprocs == 1:
-    return _run_singleprocess(spawn_fn)
+    return _run_singleprocess(spawn_fn, start_method=start_method)
   elif nprocs is not None:
     logging.warning('Unsupported nprocs (%d), ignoring...' % nprocs)
 
@@ -367,13 +355,12 @@ def spawn(fn: Callable,
 
 @requires_pjrt
 def _initialize_single_process(local_rank: int, local_world_size: int):
-  os.environ.setdefault(xenv.PJRT_LOCAL_PROCESS_RANK, str(local_rank))
-  os.environ.setdefault(xenv.PJRT_LOCAL_PROCESS_COUNT, str(local_world_size))
+  os.environ.setdefault('LOCAL_RANK', str(local_rank))
+  os.environ.setdefault('LOCAL_WORLD_SIZE', str(local_world_size))
 
 
 def spawn_threads(fn: Callable, args: Tuple = ()) -> None:
   """Run function in one process with one thread per addressable device."""
-  assert device_type() != 'GPU', "spawn_threads does not support GPU device"
   spawn_fn = _SpawnFn(fn, *args)
   _run_thread_per_device(
       local_rank=0,
@@ -443,3 +430,61 @@ def rendezvous(tag: str, payload: bytes,
   xm.mark_step()
 
   return [bytes(p.cpu().tolist()) for p in payloads]
+
+
+def init_pjrt_process_group(init_method: Optional[str] = None, **kwargs):
+  if not init_method and process_count() == 1:
+    init_method = f'file://{tempfile.mktemp()}'
+
+  dist.init_process_group(
+      'xla',
+      init_method,
+      rank=process_index(),
+      world_size=process_count(),
+      **kwargs)
+
+
+class DistributedDataParallel(nn.Module):
+  """Emulate DistributedDataParallel on TPUs.
+
+  Very experimental! There may still be correctness and performance issues to
+  work out. This class may be removed at any time.
+
+  torch.nn.parallel.DistributedDataParallel has additional overhead for gradient
+  bucketing that does not benefit TPUs. This implemenation may give better
+  performance on TPUs.
+
+  Compatible with multithreaded workloads required for multi-client execution on
+  TPU v2/v3.
+
+  Does not support model parallelism.
+  """
+
+  @staticmethod
+  def _reduce_grad(grad: torch.Tensor) -> torch.Tensor:
+    """Average gradients across replicas."""
+    return xm.all_reduce(xm.REDUCE_SUM, grad, scale=1. / global_device_count())
+
+  def __init__(self,
+               module: nn.Module,
+               *,
+               broadcast_buffers: bool = True,
+               **kwargs):
+    super().__init__()
+
+    if kwargs:
+      logging.warn('Ignoring DDP arguments: %s', str(kwargs.keys()))
+
+    assert dist.is_available() and dist.get_backend() == 'xla'
+    self._module = module
+    self._broadcast_buffers = broadcast_buffers
+
+    for p in module.parameters():
+      p.register_hook(self._reduce_grad)
+
+  def forward(self, x: torch.Tensor):
+    if self._broadcast_buffers:
+      for b in self._module.buffers():
+        dist.broadcast(b, src=0)
+
+    return self._module(x)
\ No newline at end of file
diff --git a/torch_xla/experimental/tpu.py b/torch_xla/experimental/tpu.py
index 99d9cf9e..32867940 100644
--- a/torch_xla/experimental/tpu.py
+++ b/torch_xla/experimental/tpu.py
@@ -1,10 +1,8 @@
 import functools
-import glob
 import operator
 import os
 import re
 from typing import Dict, NamedTuple, Optional, List, Tuple
-from typing_extensions import TypedDict
 import requests
 import yaml
 
@@ -30,17 +28,18 @@ _ACCELERATOR_TYPE_TO_HOST_BOUNDS = {
     'v3-512': '8,8,1',
     'v3-1024': '8,16,1',
     'v3-2048': '16,16,1',
+    # v5
+    'v5lite-4': '1,1,1',
+    'v5litepod-4': '1,1,1',
+    'v5litepod-8': '1,2,1',
+    'v5litepod-16': '2,2,1',
+    'v5litepod-64': '4,4,1',
+    'v5litepod-128': '4,8,1',
+    'v5litepod-256': '8,8,1',
     # Get v4 host bounds from TPU metadata
 }
 
 
-class TpuEnv(TypedDict):
-  accelerator_type: str
-  tpu_process_bounds: str
-  tpu_chips_per_process_bound: str
-  worker_id: int
-
-
 class MeshShape(NamedTuple):
   """Represents a TPU mesh shape (e.g. '2,2,1' or '1,1,1')"""
   x: int
@@ -71,24 +70,18 @@ def _get_metadata(key: str) -> str:
   return resp.text
 
 
-def process_bounds_size() -> Optional[int]:
-  """Returns number of processes across all TPU hosts, or None if unknown."""
+def process_bounds_size(default: int = 1) -> int:
+  """Returns number of processes across all TPU hosts."""
   process_bounds = xu.getenv_as(xenv.TPU_PROCESS_BOUNDS, str)
-  return MeshShape.from_string(process_bounds).size if process_bounds else None
-
 
-def num_available_chips() -> int:
-  """Returns the number of local chips in /dev/"""
-  return len(glob.glob('/dev/accel?'))
+  return MeshShape.from_string(
+      process_bounds).size if process_bounds else default
 
 
-def num_local_processes() -> int:
+def num_local_processes(local_chips: int = 4) -> int:
   """Returns number of processes to create on this host."""
-  local_chips = num_available_chips()
-  total_processes = process_bounds_size()
   # Don't create more processes than local chips
-  return local_chips if not total_processes else min(local_chips,
-                                                     total_processes)
+  return min(local_chips, process_bounds_size(default=local_chips))
 
 
 def task_id() -> Optional[int]:
@@ -96,28 +89,10 @@ def task_id() -> Optional[int]:
   return xu.getenv_as(xenv.CLOUD_TPU_TASK_ID, int)
 
 
-def _using_env_vars() -> bool:
-  return xu.getenv_as(xenv.TPU_SKIP_MDS_QUERY, str, False)
-
-
-def build_tpu_env_from_vars() -> TpuEnv:
-  metadata = dict()
-  metadata[xenv.ACCELERATOR_TYPE] = xu.getenv_as(xenv.TPU_ACCELERATOR_TYPE, str)
-  metadata[xenv.TPU_PROCESS_BOUNDS] = xu.getenv_as(
-      xenv.TPU_PROCESS_BOUNDS, str, xu.getenv_as(xenv.TPU_HOST_BOUNDS, str))
-  metadata[xenv.TPU_CHIPS_PER_PROCESS_BOUNDS] = xu.getenv_as(
-      xenv.TPU_CHIPS_PER_PROCESS_BOUNDS, str,
-      xu.getenv_as(xenv.TPU_CHIPS_PER_HOST_BOUNDS, str))
-  metadata[xenv.WORKER_ID] = xu.getenv_as(xenv.CLOUD_TPU_TASK_ID, str,
-                                          xu.getenv_as(xenv.TPU_WORKER_ID, str))
-  return metadata
-
-
-def get_tpu_env() -> TpuEnv:
+def get_tpu_env() -> Dict[str, str]:
   """Fetches and parses `tpu-env` metadata field."""
-  if _using_env_vars():
-    return build_tpu_env_from_vars()
   metadata = _get_metadata('tpu-env')
+
   return yaml.load(metadata, yaml.Loader)
 
 
@@ -127,22 +102,19 @@ def version() -> int:
   except requests.HTTPError as e:
     raise EnvironmentError('Failed to get TPU metadata') from e
 
-  match = re.match(r'^v(\d)([A-Za-z]?){7}-(\d+)$', env[xenv.ACCELERATOR_TYPE])
+  match = re.match(r'^v(\d)-(\d+)$', env['ACCELERATOR_TYPE'])
   return int(match.groups()[0])
 
 
 def get_worker_ips() -> List[str]:
   """Returns ordered list of TPU worker IPs from TPU metadata."""
-  if _using_env_vars():
-    hostnames_string = xu.getenv_as(xenv.TPU_WORKER_HOSTNAMES, str, '')
-    # String has the format 'host-name-1,host-name-2,...,host-name-n'
-    hostnames = hostnames_string.split(',')
-  else:
-    hostnames_string = _get_metadata('worker-network-endpoints')
-    # Workers have format 'hostname:uid:ip,hostname:uid:ip,...'
-    workers = hostnames_string.split(',')
-    hostnames = [worker.split(':')[2] for worker in workers]
-  return hostnames if len(hostnames) > 1 else ['localhost']
+  metadata = _get_metadata('worker-network-endpoints')
+
+  # Workers have format 'hostname:uid:ip,hostname:uid:ip,...'
+  workers = metadata.split(',')
+  ips = [worker.split(':')[2] for worker in workers]
+
+  return ips if len(ips) > 1 else ['localhost']
 
 
 def configure_one_chip_topology() -> None:
@@ -171,8 +143,8 @@ def configure_topology(local_rank: int,
   """
   tpu_env = get_tpu_env()
 
-  accelerator_type = tpu_env[xenv.ACCELERATOR_TYPE]
-  if version() >= 4:
+  accelerator_type = tpu_env['ACCELERATOR_TYPE']
+  if tpu_env['ACCELERATOR_TYPE'].startswith('v4'):
     # Process bounds with 4 chips per process
     default_process_bounds = MeshShape.from_string(
         tpu_env[xenv.TPU_PROCESS_BOUNDS])
@@ -192,7 +164,7 @@ def configure_topology(local_rank: int,
                         ','.join(str(dim) for dim in process_bounds))
 
   # Assume each TPU has the same number of local processes with the same ports
-  worker_id = int(tpu_env[xenv.WORKER_ID])
+  worker_id = int(tpu_env['WORKER_ID'])
   os.environ.setdefault(xenv.CLOUD_TPU_TASK_ID,
                         str(worker_id * local_world_size + local_rank))
 
@@ -222,10 +194,10 @@ def discover_master_worker_ip(use_localhost: bool = True) -> str:
     return 'localhost'
 
   tpu_env = get_tpu_env()
-  current_worker_id = int(tpu_env[xenv.WORKER_ID])
+  current_worker_id = int(tpu_env['WORKER_ID'])
   t = torch.tensor([current_worker_id], device=xm.xla_device())
   xm.collective_broadcast([t])
   xm.mark_step()
 
   master_worker_id = int(t.cpu())
-  return worker_ips[master_worker_id]
+  return worker_ips[master_worker_id]
