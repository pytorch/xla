[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  __init__ (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py:533)
  _wrap (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/wrap.py:147)
  recursive_wrap (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/wrap.py:213)
  recursive_wrap (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/wrap.py:196)
  _auto_wrap (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py:1635)
  __init__ (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py:372)
  <lambda> (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:235)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:247)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (968f7a417173a2e1b368916bd85f25f6, 6cc1fa14189af4e9e77e72c207f4cba, 968f7a417173a2e1b368916bd85f25f6)

## BEGIN_GRAPH
HloModule IrToHlo.9, entry_computation_layout={(bf16[13,320]{1,0})->(bf16[1]{0}, bf16[13,320]{1,0}, bf16[1]{0})}

ENTRY %IrToHlo.9 (p0.4: bf16[13,320]) -> (bf16[1], bf16[13,320], bf16[1]) {
  %constant.1 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=659}
  %reshape.2 = bf16[1]{0} reshape(bf16[] %constant.1), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=659}
  %broadcast.3 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.2), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=659}
  %p0.4 = bf16[13,320]{1,0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=699}
  %constant.5 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %reshape.6 = bf16[1]{0} reshape(bf16[] %constant.5), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %broadcast.7 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.6), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  ROOT %tuple.8 = (bf16[1]{0}, bf16[13,320]{1,0}, bf16[1]{0}) tuple(bf16[1]{0} %broadcast.3, bf16[13,320]{1,0} %p0.4, bf16[1]{0} %broadcast.7)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  __init__ (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py:533)
  _wrap (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/wrap.py:147)
  recursive_wrap (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/wrap.py:213)
  recursive_wrap (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/wrap.py:196)
  _auto_wrap (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py:1635)
  __init__ (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py:372)
  <lambda> (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:235)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:247)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (968f7a417173a2e1b368916bd85f25f6, ae42e279c4c63b619343c590a92f1385, 968f7a417173a2e1b368916bd85f25f6)

## BEGIN_GRAPH
HloModule IrToHlo.9, entry_computation_layout={(bf16[3,50]{1,0})->(bf16[1]{0}, bf16[3,50]{1,0}, bf16[1]{0})}

ENTRY %IrToHlo.9 (p0.4: bf16[3,50]) -> (bf16[1], bf16[3,50], bf16[1]) {
  %constant.1 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=659}
  %reshape.2 = bf16[1]{0} reshape(bf16[] %constant.1), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=659}
  %broadcast.3 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.2), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=659}
  %p0.4 = bf16[3,50]{1,0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=699}
  %constant.5 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %reshape.6 = bf16[1]{0} reshape(bf16[] %constant.5), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %broadcast.7 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.6), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  ROOT %tuple.8 = (bf16[1]{0}, bf16[3,50]{1,0}, bf16[1]{0}) tuple(bf16[1]{0} %broadcast.3, bf16[3,50]{1,0} %p0.4, bf16[1]{0} %broadcast.7)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  __init__ (/workspaces/work/pytorch/xla/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py:533)
  <lambda> (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:235)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:247)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (968f7a417173a2e1b368916bd85f25f6, dc225278bdcd07ff78e6dbc512e1882f, 968f7a417173a2e1b368916bd85f25f6, fa1dbc4df7b527815cd75d199f4e6b33, 968f7a417173a2e1b368916bd85f25f6, fa1dbc4df7b527815cd75d199f4e6b33, 968f7a417173a2e1b368916bd85f25f6, fa1dbc4df7b527815cd75d199f4e6b33, 968f7a417173a2e1b368916bd85f25f6, e7a9b81c682cb1be25eea014981e9d78, 968f7a417173a2e1b368916bd85f25f6, 364c1607fa7a0b6e13de590292a6f9a3, 968f7a417173a2e1b368916bd85f25f6, 364c1607fa7a0b6e13de590292a6f9a3, 968f7a417173a2e1b368916bd85f25f6, 364c1607fa7a0b6e13de590292a6f9a3, 968f7a417173a2e1b368916bd85f25f6, abfeb1fd0d3ffcc80670084f69703128, abfeb1fd0d3ffcc80670084f69703128, eb03aa3a8858b23facf7cc6922ab39b6, ce75e646308080112098812aa3c9dcde, ce75e646308080112098812aa3c9dcde, eb03aa3a8858b23facf7cc6922ab39b6)

## BEGIN_GRAPH
HloModule IrToHlo.43, entry_computation_layout={(bf16[3,1,5,5]{3,2,0,1},bf16[3]{0},bf16[3]{0},bf16[3]{0},bf16[5,10,5,5]{1,3,2,0},bf16[5]{0},bf16[5]{0},bf16[5]{0},bf16[10]{0},bf16[10]{0},bf16[20]{0},bf16[20]{0})->(bf16[1]{0}, bf16[3,1,5,5]{3,2,0,1}, bf16[1]{0}, bf16[3]{0}, bf16[1]{0}, /*index=5*/bf16[3]{0}, bf16[1]{0}, bf16[3]{0}, bf16[1]{0}, bf16[5,10,5,5]{1,3,2,0}, /*index=10*/bf16[1]{0}, bf16[5]{0}, bf16[1]{0}, bf16[5]{0}, bf16[1]{0}, /*index=15*/bf16[5]{0}, bf16[1]{0}, bf16[10]{0}, bf16[10]{0}, s64[], /*index=20*/bf16[20]{0}, bf16[20]{0}, s64[])}

ENTRY %IrToHlo.43 (p0.4: bf16[3,1,5,5], p1.8: bf16[3], p2.12: bf16[3], p3.16: bf16[3], p4.20: bf16[5,10,5,5], p5.24: bf16[5], p6.28: bf16[5], p7.32: bf16[5], p8.36: bf16[10], p9.37: bf16[10], p10.39: bf16[20], p11.40: bf16[20]) -> (bf16[1], bf16[3,1,5,5], bf16[1], bf16[3], bf16[1], /*index=5*/bf16[3], bf16[1], bf16[3], bf16[1], bf16[5,10,5,5], /*index=10*/bf16[1], bf16[5], bf16[1], bf16[5], bf16[1], /*index=15*/bf16[5], bf16[1], bf16[10], bf16[10], s64[], /*index=20*/bf16[20], bf16[20], s64[]) {
  %constant.1 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=659}
  %reshape.2 = bf16[1]{0} reshape(bf16[] %constant.1), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=659}
  %broadcast.3 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.2), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=659}
  %p0.4 = bf16[3,1,5,5]{3,2,0,1} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=699}
  %constant.5 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %reshape.6 = bf16[1]{0} reshape(bf16[] %constant.5), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %broadcast.7 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.6), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %p1.8 = bf16[3]{0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=699}
  %constant.9 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %reshape.10 = bf16[1]{0} reshape(bf16[] %constant.9), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %broadcast.11 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.10), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %p2.12 = bf16[3]{0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=699}
  %constant.13 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %reshape.14 = bf16[1]{0} reshape(bf16[] %constant.13), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %broadcast.15 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.14), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %p3.16 = bf16[3]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=699}
  %constant.17 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %reshape.18 = bf16[1]{0} reshape(bf16[] %constant.17), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %broadcast.19 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.18), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %p4.20 = bf16[5,10,5,5]{1,3,2,0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=699}
  %constant.21 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %reshape.22 = bf16[1]{0} reshape(bf16[] %constant.21), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %broadcast.23 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.22), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %p5.24 = bf16[5]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=699}
  %constant.25 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %reshape.26 = bf16[1]{0} reshape(bf16[] %constant.25), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %broadcast.27 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.26), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %p6.28 = bf16[5]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=699}
  %constant.29 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %reshape.30 = bf16[1]{0} reshape(bf16[] %constant.29), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %broadcast.31 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.30), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %p7.32 = bf16[5]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=699}
  %constant.33 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %reshape.34 = bf16[1]{0} reshape(bf16[] %constant.33), metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %broadcast.35 = bf16[1]{0} broadcast(bf16[1]{0} %reshape.34), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="_shard_parameters_@xla_fully_sharded_data_parallel.py" source_line=718}
  %p8.36 = bf16[10]{0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_cast_buffers@xla_fully_sharded_data_parallel.py" source_line=780}
  %p9.37 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_cast_buffers@xla_fully_sharded_data_parallel.py" source_line=780}
  %constant.38 = s64[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_cast_buffers@xla_fully_sharded_data_parallel.py" source_line=780}
  %p10.39 = bf16[20]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_cast_buffers@xla_fully_sharded_data_parallel.py" source_line=780}
  %p11.40 = bf16[20]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_cast_buffers@xla_fully_sharded_data_parallel.py" source_line=780}
  %constant.41 = s64[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_cast_buffers@xla_fully_sharded_data_parallel.py" source_line=780}
  ROOT %tuple.42 = (bf16[1]{0}, bf16[3,1,5,5]{3,2,0,1}, bf16[1]{0}, bf16[3]{0}, bf16[1]{0}, /*index=5*/bf16[3]{0}, bf16[1]{0}, bf16[3]{0}, bf16[1]{0}, bf16[5,10,5,5]{1,3,2,0}, /*index=10*/bf16[1]{0}, bf16[5]{0}, bf16[1]{0}, bf16[5]{0}, bf16[1]{0}, /*index=15*/bf16[5]{0}, bf16[1]{0}, bf16[10]{0}, bf16[10]{0}, s64[], /*index=20*/bf16[20]{0}, bf16[20]{0}, s64[]) tuple(bf16[1]{0} %broadcast.3, bf16[3,1,5,5]{3,2,0,1} %p0.4, bf16[1]{0} %broadcast.7, bf16[3]{0} %p1.8, bf16[1]{0} %broadcast.11, /*index=5*/bf16[3]{0} %p2.12, bf16[1]{0} %broadcast.15, bf16[3]{0} %p3.16, bf16[1]{0} %broadcast.19, bf16[5,10,5,5]{1,3,2,0} %p4.20, /*index=10*/bf16[1]{0} %broadcast.23, bf16[5]{0} %p5.24, bf16[1]{0} %broadcast.27, bf16[5]{0} %p6.28, bf16[1]{0} %broadcast.31, /*index=15*/bf16[5]{0} %p7.32, bf16[1]{0} %broadcast.35, bf16[10]{0} %p8.36, bf16[10]{0} %p9.37, s64[] %constant.38, /*index=20*/bf16[20]{0} %p10.39, bf16[20]{0} %p11.40, s64[] %constant.41)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (a3cfffa0e2f592d1d8c40672a1f0ddf1, c6ce42013343ea1bf8649be89f0d7bb8, bc7c721ba69dc561850a50a0ce180028, e00de3593c815d38a56aba7fe87d57d1, 75fc66096e7436cfbd93af5ad09aee7c, 6c358e1ac4dd5645c1d14ec880cf2fa1, 5958cd1df2a7d3ad8c4142fb24d928f9, 6ce7121b6a29bd731935f6c0dc63834c, d898c48e8aec5490b82ecbb38ec60cd5)

## BEGIN_GRAPH
HloModule IrToHlo.283, entry_computation_layout={(f32[],bf16[3,1,5,5]{3,2,0,1},bf16[3]{0},bf16[3]{0},bf16[3]{0},bf16[5,10,5,5]{1,3,2,0},bf16[5]{0},bf16[5]{0},bf16[5]{0},bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[10]{0},bf16[10]{0},f32[128,1,28,28]{0,3,2,1})->(bf16[10,1,5,5]{3,2,1,0}, bf16[10]{0}, bf16[10]{0}, bf16[10]{0}, bf16[20,10,5,5]{3,2,1,0}, /*index=5*/bf16[20]{0}, bf16[20]{0}, bf16[20]{0}, bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.25 (x.26: bf16[], y.27: bf16[]) -> bf16[] {
  %x.26 = bf16[] parameter(0)
  %y.27 = bf16[] parameter(1)
  ROOT %add.28 = bf16[] add(bf16[] %x.26, bf16[] %y.27)
}

%AddComputation.41 (x.42: bf16[], y.43: bf16[]) -> bf16[] {
  %x.42 = bf16[] parameter(0)
  %y.43 = bf16[] parameter(1)
  ROOT %add.44 = bf16[] add(bf16[] %x.42, bf16[] %y.43)
}

%AddComputation.57 (x.58: bf16[], y.59: bf16[]) -> bf16[] {
  %x.58 = bf16[] parameter(0)
  %y.59 = bf16[] parameter(1)
  ROOT %add.60 = bf16[] add(bf16[] %x.58, bf16[] %y.59)
}

%AddComputation.73 (x.74: bf16[], y.75: bf16[]) -> bf16[] {
  %x.74 = bf16[] parameter(0)
  %y.75 = bf16[] parameter(1)
  ROOT %add.76 = bf16[] add(bf16[] %x.74, bf16[] %y.75)
}

%AddComputation.89 (x.90: bf16[], y.91: bf16[]) -> bf16[] {
  %x.90 = bf16[] parameter(0)
  %y.91 = bf16[] parameter(1)
  ROOT %add.92 = bf16[] add(bf16[] %x.90, bf16[] %y.91)
}

%AddComputation.105 (x.106: bf16[], y.107: bf16[]) -> bf16[] {
  %x.106 = bf16[] parameter(0)
  %y.107 = bf16[] parameter(1)
  ROOT %add.108 = bf16[] add(bf16[] %x.106, bf16[] %y.107)
}

%AddComputation.121 (x.122: bf16[], y.123: bf16[]) -> bf16[] {
  %x.122 = bf16[] parameter(0)
  %y.123 = bf16[] parameter(1)
  ROOT %add.124 = bf16[] add(bf16[] %x.122, bf16[] %y.123)
}

%AddComputation.137 (x.138: bf16[], y.139: bf16[]) -> bf16[] {
  %x.138 = bf16[] parameter(0)
  %y.139 = bf16[] parameter(1)
  ROOT %add.140 = bf16[] add(bf16[] %x.138, bf16[] %y.139)
}

%AddComputation.152 (x.153: bf16[], y.154: bf16[]) -> bf16[] {
  %x.153 = bf16[] parameter(0)
  %y.154 = bf16[] parameter(1)
  ROOT %add.155 = bf16[] add(bf16[] %x.153, bf16[] %y.154)
}

%max_BF16.177 (lhs.178: bf16[], rhs.179: bf16[]) -> bf16[] {
  %lhs.178 = bf16[] parameter(0)
  %rhs.179 = bf16[] parameter(1)
  ROOT %maximum.180 = bf16[] maximum(bf16[] %lhs.178, bf16[] %rhs.179)
}

%ge_BF16.183 (lhs.184: bf16[], rhs.185: bf16[]) -> pred[] {
  %lhs.184 = bf16[] parameter(0)
  %rhs.185 = bf16[] parameter(1)
  ROOT %compare.186 = pred[] compare(bf16[] %lhs.184, bf16[] %rhs.185), direction=GE
}

%max_BF16.187 (lhs.188: bf16[], rhs.189: bf16[]) -> bf16[] {
  %lhs.188 = bf16[] parameter(0)
  %rhs.189 = bf16[] parameter(1)
  ROOT %maximum.190 = bf16[] maximum(bf16[] %lhs.188, bf16[] %rhs.189)
}

%min_U32.201 (lhs.202: u32[], rhs.203: u32[]) -> u32[] {
  %lhs.202 = u32[] parameter(0)
  %rhs.203 = u32[] parameter(1)
  ROOT %minimum.204 = u32[] minimum(u32[] %lhs.202, u32[] %rhs.203)
}

%max_BF16.221 (lhs.222: bf16[], rhs.223: bf16[]) -> bf16[] {
  %lhs.222 = bf16[] parameter(0)
  %rhs.223 = bf16[] parameter(1)
  ROOT %maximum.224 = bf16[] maximum(bf16[] %lhs.222, bf16[] %rhs.223)
}

%ge_BF16.227 (lhs.228: bf16[], rhs.229: bf16[]) -> pred[] {
  %lhs.228 = bf16[] parameter(0)
  %rhs.229 = bf16[] parameter(1)
  ROOT %compare.230 = pred[] compare(bf16[] %lhs.228, bf16[] %rhs.229), direction=GE
}

%max_BF16.231 (lhs.232: bf16[], rhs.233: bf16[]) -> bf16[] {
  %lhs.232 = bf16[] parameter(0)
  %rhs.233 = bf16[] parameter(1)
  ROOT %maximum.234 = bf16[] maximum(bf16[] %lhs.232, bf16[] %rhs.233)
}

%min_U32.245 (lhs.246: u32[], rhs.247: u32[]) -> u32[] {
  %lhs.246 = u32[] parameter(0)
  %rhs.247 = u32[] parameter(1)
  ROOT %minimum.248 = u32[] minimum(u32[] %lhs.246, u32[] %rhs.247)
}

%MaxComputation.265 (x.266: bf16[], y.267: bf16[]) -> bf16[] {
  %x.266 = bf16[] parameter(0)
  %y.267 = bf16[] parameter(1)
  ROOT %maximum.268 = bf16[] maximum(bf16[] %x.266, bf16[] %y.267)
}

%AddComputation.274 (x.275: bf16[], y.276: bf16[]) -> bf16[] {
  %x.275 = bf16[] parameter(0)
  %y.276 = bf16[] parameter(1)
  ROOT %add.277 = bf16[] add(bf16[] %x.275, bf16[] %y.276)
}

ENTRY %IrToHlo.283 (p0.1: f32[], p1.2: bf16[3,1,5,5], p2.18: bf16[3], p3.34: bf16[3], p4.50: bf16[3], p5.66: bf16[5,10,5,5], p6.82: bf16[5], p7.98: bf16[5], p8.114: bf16[5], p9.130: bf16[13,320], p10.145: bf16[3,50], p11.164: bf16[20], p12.165: bf16[20], p13.166: bf16[10], p14.167: bf16[10], p15.168: f32[128,1,28,28]) -> (bf16[10,1,5,5], bf16[10], bf16[10], bf16[10], bf16[20,10,5,5], /*index=5*/bf16[20], bf16[20], bf16[20], bf16[128,10]) {
  %p10.145 = bf16[3,50]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.146 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.147 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p10.145, bf16[] %constant.146), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p9.130 = bf16[13,320]{1,0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.131 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.132 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p9.130, bf16[] %constant.131), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p8.114 = bf16[5]{0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.115 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.116 = bf16[20]{0} pad(bf16[5]{0} %p8.114, bf16[] %constant.115), padding=0_15, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p7.98 = bf16[5]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.99 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.100 = bf16[20]{0} pad(bf16[5]{0} %p7.98, bf16[] %constant.99), padding=0_15, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p6.82 = bf16[5]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.83 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.84 = bf16[20]{0} pad(bf16[5]{0} %p6.82, bf16[] %constant.83), padding=0_15, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p5.66 = bf16[5,10,5,5]{1,3,2,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.67 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.68 = bf16[20,10,5,5]{3,2,1,0} pad(bf16[5,10,5,5]{1,3,2,0} %p5.66, bf16[] %constant.67), padding=0_15x0_0x0_0x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p4.50 = bf16[3]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.51 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.52 = bf16[12]{0} pad(bf16[3]{0} %p4.50, bf16[] %constant.51), padding=0_9, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p3.34 = bf16[3]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.35 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.36 = bf16[12]{0} pad(bf16[3]{0} %p3.34, bf16[] %constant.35), padding=0_9, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p2.18 = bf16[3]{0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.19 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.20 = bf16[12]{0} pad(bf16[3]{0} %p2.18, bf16[] %constant.19), padding=0_9, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[3,1,5,5]{3,2,0,1} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[12,1,5,5]{3,2,1,0} pad(bf16[3,1,5,5]{3,2,0,1} %p1.2, bf16[] %constant.3), padding=0_9x0_0x0_0x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[12,1,5,5]{3,2,1,0}, bf16[]) tuple(bf16[12,1,5,5]{3,2,1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[12,1,5,5]{3,2,1,0} get-tuple-element((bf16[12,1,5,5]{3,2,1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[12,1,5,5]{3,2,1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[12,1,5,5]{0,3,2,1}, bf16[]) all-reduce(bf16[12,1,5,5]{3,2,1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[12,1,5,5]{0,3,2,1}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.21 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.22 = (bf16[12]{0}, bf16[]) tuple(bf16[12]{0} %pad.20, bf16[] %convert.21), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[12]{0} get-tuple-element((bf16[12]{0}, bf16[]) %tuple.22), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.24 = bf16[] get-tuple-element((bf16[12]{0}, bf16[]) %tuple.22), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.29 = (bf16[12]{0}, bf16[]) all-reduce(bf16[12]{0} %get-tuple-element.23, bf16[] %get-tuple-element.24), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.25, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.31 = bf16[] get-tuple-element((bf16[12]{0}, bf16[]) %all-reduce.29), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.32 = f32[] convert(bf16[] %get-tuple-element.31), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.37 = bf16[] convert(f32[] %convert.32), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.38 = (bf16[12]{0}, bf16[]) tuple(bf16[12]{0} %pad.36, bf16[] %convert.37), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.39 = bf16[12]{0} get-tuple-element((bf16[12]{0}, bf16[]) %tuple.38), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.40 = bf16[] get-tuple-element((bf16[12]{0}, bf16[]) %tuple.38), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.45 = (bf16[12]{0}, bf16[]) all-reduce(bf16[12]{0} %get-tuple-element.39, bf16[] %get-tuple-element.40), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.41, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.47 = bf16[] get-tuple-element((bf16[12]{0}, bf16[]) %all-reduce.45), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.48 = f32[] convert(bf16[] %get-tuple-element.47), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.53 = bf16[] convert(f32[] %convert.48), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.54 = (bf16[12]{0}, bf16[]) tuple(bf16[12]{0} %pad.52, bf16[] %convert.53), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.55 = bf16[12]{0} get-tuple-element((bf16[12]{0}, bf16[]) %tuple.54), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.56 = bf16[] get-tuple-element((bf16[12]{0}, bf16[]) %tuple.54), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.61 = (bf16[12]{0}, bf16[]) all-reduce(bf16[12]{0} %get-tuple-element.55, bf16[] %get-tuple-element.56), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.57, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.63 = bf16[] get-tuple-element((bf16[12]{0}, bf16[]) %all-reduce.61), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.64 = f32[] convert(bf16[] %get-tuple-element.63), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.69 = bf16[] convert(f32[] %convert.64), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.70 = (bf16[20,10,5,5]{3,2,1,0}, bf16[]) tuple(bf16[20,10,5,5]{3,2,1,0} %pad.68, bf16[] %convert.69), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.71 = bf16[20,10,5,5]{3,2,1,0} get-tuple-element((bf16[20,10,5,5]{3,2,1,0}, bf16[]) %tuple.70), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.72 = bf16[] get-tuple-element((bf16[20,10,5,5]{3,2,1,0}, bf16[]) %tuple.70), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.77 = (bf16[20,10,5,5]{0,1,3,2}, bf16[]) all-reduce(bf16[20,10,5,5]{3,2,1,0} %get-tuple-element.71, bf16[] %get-tuple-element.72), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.73, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.79 = bf16[] get-tuple-element((bf16[20,10,5,5]{0,1,3,2}, bf16[]) %all-reduce.77), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.80 = f32[] convert(bf16[] %get-tuple-element.79), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.85 = bf16[] convert(f32[] %convert.80), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.86 = (bf16[20]{0}, bf16[]) tuple(bf16[20]{0} %pad.84, bf16[] %convert.85), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.87 = bf16[20]{0} get-tuple-element((bf16[20]{0}, bf16[]) %tuple.86), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.88 = bf16[] get-tuple-element((bf16[20]{0}, bf16[]) %tuple.86), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.93 = (bf16[20]{0}, bf16[]) all-reduce(bf16[20]{0} %get-tuple-element.87, bf16[] %get-tuple-element.88), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.89, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.95 = bf16[] get-tuple-element((bf16[20]{0}, bf16[]) %all-reduce.93), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.96 = f32[] convert(bf16[] %get-tuple-element.95), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.101 = bf16[] convert(f32[] %convert.96), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.102 = (bf16[20]{0}, bf16[]) tuple(bf16[20]{0} %pad.100, bf16[] %convert.101), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.103 = bf16[20]{0} get-tuple-element((bf16[20]{0}, bf16[]) %tuple.102), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.104 = bf16[] get-tuple-element((bf16[20]{0}, bf16[]) %tuple.102), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.109 = (bf16[20]{0}, bf16[]) all-reduce(bf16[20]{0} %get-tuple-element.103, bf16[] %get-tuple-element.104), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.105, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.111 = bf16[] get-tuple-element((bf16[20]{0}, bf16[]) %all-reduce.109), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.112 = f32[] convert(bf16[] %get-tuple-element.111), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.117 = bf16[] convert(f32[] %convert.112), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.118 = (bf16[20]{0}, bf16[]) tuple(bf16[20]{0} %pad.116, bf16[] %convert.117), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.119 = bf16[20]{0} get-tuple-element((bf16[20]{0}, bf16[]) %tuple.118), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.120 = bf16[] get-tuple-element((bf16[20]{0}, bf16[]) %tuple.118), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.125 = (bf16[20]{0}, bf16[]) all-reduce(bf16[20]{0} %get-tuple-element.119, bf16[] %get-tuple-element.120), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.121, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.127 = bf16[] get-tuple-element((bf16[20]{0}, bf16[]) %all-reduce.125), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.128 = f32[] convert(bf16[] %get-tuple-element.127), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.133 = bf16[] convert(f32[] %convert.128), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.134 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.132, bf16[] %convert.133), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.135 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.134), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.136 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.134), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.141 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.135, bf16[] %get-tuple-element.136), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.137, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.143 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.141), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.144 = f32[] convert(bf16[] %get-tuple-element.143), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.148 = bf16[] convert(f32[] %convert.144), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.149 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.147, bf16[] %convert.148), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.150 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.149), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.151 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.149), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.156 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.150, bf16[] %get-tuple-element.151), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.152, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.158 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.156), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.159 = f32[] convert(bf16[] %get-tuple-element.158), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.168 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.169 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.168), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %get-tuple-element.14 = bf16[12,1,5,5]{0,3,2,1} get-tuple-element((bf16[12,1,5,5]{0,3,2,1}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.17 = bf16[10,1,5,5]{3,2,1,0} slice(bf16[12,1,5,5]{0,3,2,1} %get-tuple-element.14), slice={[0:10], [0:1], [0:5], [0:5]}, metadata={op_type="xla__select" op_name="xla__select" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.170 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.169, bf16[10,1,5,5]{3,2,1,0} %slice.17), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %get-tuple-element.30 = bf16[12]{0} get-tuple-element((bf16[12]{0}, bf16[]) %all-reduce.29), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.33 = bf16[10]{0} slice(bf16[12]{0} %get-tuple-element.30), slice={[0:10]}, metadata={op_type="xla__select" op_name="xla__select" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.171 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %slice.33), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.172 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.171), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.173 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.170, bf16[128,10,24,24]{1,3,2,0} %transpose.172), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.174 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.175 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.173, bf16[] %constant.174), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.176 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.181 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.175, bf16[] %constant.176), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.177, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.182 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.191 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.175, bf16[128,10,12,12]{3,2,1,0} %reduce-window.181, bf16[] %constant.182), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.183, scatter=%max_BF16.187, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.198 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.182), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.199 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.191, bf16[128,10,24,24]{3,2,1,0} %broadcast.198), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.192 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.193 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.192), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.194 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.193), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.195 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.196 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.194, u32[] %constant.195), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.197 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.195), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.200 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.199, u32[128,10,24,24]{3,2,1,0} %pad.196, u32[128,10,24,24]{3,2,1,0} %broadcast.197), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.205 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.200, u32[] %constant.195), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.201, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p13.166 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.210 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.211 = bf16[10]{0} broadcast(bf16[] %constant.210), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.212 = bf16[10]{0} add(bf16[10]{0} %p13.166, bf16[10]{0} %broadcast.211), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.213 = bf16[10]{0} rsqrt(bf16[10]{0} %add.212), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.206 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.207 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.206), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.208 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.181, bf16[128,10,12,12]{3,2,1,0} %broadcast.207), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.46 = bf16[12]{0} get-tuple-element((bf16[12]{0}, bf16[]) %all-reduce.45), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.49 = bf16[10]{0} slice(bf16[12]{0} %get-tuple-element.46), slice={[0:10]}, metadata={op_type="xla__select" op_name="xla__select" source_file="batch_norm@functional.py" source_line=2455}
  %get-tuple-element.62 = bf16[12]{0} get-tuple-element((bf16[12]{0}, bf16[]) %all-reduce.61), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.65 = bf16[10]{0} slice(bf16[12]{0} %get-tuple-element.62), slice={[0:10]}, metadata={op_type="xla__select" op_name="xla__select" source_file="batch_norm@functional.py" source_line=2455}
  %p14.167 = bf16[10]{0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.209 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.208, bf16[10]{0} %slice.49, bf16[10]{0} %slice.65, bf16[10]{0} %p14.167, bf16[10]{0} %p13.166), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %get-tuple-element.78 = bf16[20,10,5,5]{0,1,3,2} get-tuple-element((bf16[20,10,5,5]{0,1,3,2}, bf16[]) %all-reduce.77), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.81 = bf16[20,10,5,5]{3,2,1,0} slice(bf16[20,10,5,5]{0,1,3,2} %get-tuple-element.78), slice={[0:20], [0:10], [0:5], [0:5]}, metadata={op_type="xla__select" op_name="xla__select" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.214 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.209, bf16[20,10,5,5]{3,2,1,0} %slice.81), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %get-tuple-element.94 = bf16[20]{0} get-tuple-element((bf16[20]{0}, bf16[]) %all-reduce.93), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.97 = bf16[20]{0} slice(bf16[20]{0} %get-tuple-element.94), slice={[0:20]}, metadata={op_type="xla__select" op_name="xla__select" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.215 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %slice.97), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.216 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.215), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.217 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.214, bf16[128,20,8,8]{1,3,2,0} %transpose.216), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.218 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.219 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.217, bf16[] %constant.218), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.220 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.225 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.219, bf16[] %constant.220), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.221, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.226 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.235 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.219, bf16[128,20,4,4]{3,2,1,0} %reduce-window.225, bf16[] %constant.226), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.227, scatter=%max_BF16.231, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.242 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.226), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.243 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.235, bf16[128,20,8,8]{3,2,1,0} %broadcast.242), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.236 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.237 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.236), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.238 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.237), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.239 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.240 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.238, u32[] %constant.239), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.241 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.239), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.244 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.243, u32[128,20,8,8]{3,2,1,0} %pad.240, u32[128,20,8,8]{3,2,1,0} %broadcast.241), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.249 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.244, u32[] %constant.239), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.245, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p11.164 = bf16[20]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.254 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.255 = bf16[20]{0} broadcast(bf16[] %constant.254), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.256 = bf16[20]{0} add(bf16[20]{0} %p11.164, bf16[20]{0} %broadcast.255), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.257 = bf16[20]{0} rsqrt(bf16[20]{0} %add.256), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %get-tuple-element.110 = bf16[20]{0} get-tuple-element((bf16[20]{0}, bf16[]) %all-reduce.109), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.113 = bf16[20]{0} slice(bf16[20]{0} %get-tuple-element.110), slice={[0:20]}, metadata={op_type="xla__select" op_name="xla__select" source_file="batch_norm@functional.py" source_line=2455}
  %get-tuple-element.126 = bf16[20]{0} get-tuple-element((bf16[20]{0}, bf16[]) %all-reduce.125), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.129 = bf16[20]{0} slice(bf16[20]{0} %get-tuple-element.126), slice={[0:20]}, metadata={op_type="xla__select" op_name="xla__select" source_file="batch_norm@functional.py" source_line=2455}
  %constant.250 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.251 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.250), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.252 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.225, bf16[128,20,4,4]{3,2,1,0} %broadcast.251), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.165 = bf16[20]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.253 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.252, bf16[20]{0} %slice.113, bf16[20]{0} %slice.129, bf16[20]{0} %p12.165, bf16[20]{0} %p11.164), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.258 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.253), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.142 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.141), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.162 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.142), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.163 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.162), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.259 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.258, bf16[320,50]{0,1} %transpose.163), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.260 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.261 = bf16[128,50]{1,0} broadcast(bf16[] %constant.260), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.262 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.259, bf16[128,50]{1,0} %broadcast.261), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.157 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.156), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.160 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.157), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.161 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.160), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.263 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.262, bf16[50,10]{0,1} %transpose.161), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.264 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.269 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.263, bf16[] %constant.264), dimensions={1}, to_apply=%MaxComputation.265, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.270 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.269), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.271 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.263, bf16[128,10]{1,0} %broadcast.270), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.272 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.271), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.273 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.278 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.272, bf16[] %constant.273), dimensions={1}, to_apply=%AddComputation.274, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.279 = bf16[128]{0} log(bf16[128]{0} %reduce.278), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.280 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.279), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.281 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.271, bf16[128,10]{1,0} %broadcast.280), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.282 = (bf16[10,1,5,5]{3,2,1,0}, bf16[10]{0}, bf16[10]{0}, bf16[10]{0}, bf16[20,10,5,5]{3,2,1,0}, /*index=5*/bf16[20]{0}, bf16[20]{0}, bf16[20]{0}, bf16[128,10]{1,0}) tuple(bf16[10,1,5,5]{3,2,1,0} %slice.17, bf16[10]{0} %slice.33, bf16[10]{0} %slice.49, bf16[10]{0} %slice.65, bf16[20,10,5,5]{3,2,1,0} %slice.81, /*index=5*/bf16[20]{0} %slice.97, bf16[20]{0} %slice.113, bf16[20]{0} %slice.129, bf16[128,10]{1,0} %subtract.281)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (da856b219d15c7e57855f2640108b4ec)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[128,1,28,28]{0,3,2,1})->(bf16[128,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[128,1,28,28]) -> (bf16[128,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[128,1,28,28]{0,3,2,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[128,1,28,28]{0,3,2,1} convert(f32[128,1,28,28]{0,3,2,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[128,10,24,24]{3,2,1,0} convolution(bf16[128,1,28,28]{0,3,2,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[128,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[128,10,24,24]{1,3,2,0} transpose(bf16[128,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[128,10,24,24]{3,2,1,0} add(bf16[128,10,24,24]{3,2,1,0} %convolution.50, bf16[128,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[128,10,24,24]{3,2,1,0} pad(bf16[128,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[128,10,12,12]{3,2,1,0} reduce-window(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[128,10,24,24]{3,2,1,0} select-and-scatter(bf16[128,10,24,24]{3,2,1,0} %pad.55, bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[128,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[128,10,24,24]{3,2,1,0} compare(bf16[128,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[128,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[128,10,24,24]{3,2,1,0} pad(u32[128,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[128,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[128,10,24,24]{3,2,1,0} select(pred[128,10,24,24]{3,2,1,0} %compare.79, u32[128,10,24,24]{3,2,1,0} %pad.76, u32[128,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[128,10,12,12]{3,2,1,0} reduce-window(u32[128,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[128,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[128,10,12,12]{3,2,1,0} maximum(bf16[128,10,12,12]{3,2,1,0} %reduce-window.61, bf16[128,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[128,10,12,12]{3,2,1,0} batch-norm-inference(bf16[128,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[128,20,8,8]{3,2,1,0} convolution(bf16[128,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[128,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[128,20,8,8]{1,3,2,0} transpose(bf16[128,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[128,20,8,8]{3,2,1,0} add(bf16[128,20,8,8]{3,2,1,0} %convolution.94, bf16[128,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[128,20,8,8]{3,2,1,0} pad(bf16[128,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[128,20,4,4]{3,2,1,0} reduce-window(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[128,20,8,8]{3,2,1,0} select-and-scatter(bf16[128,20,8,8]{3,2,1,0} %pad.99, bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[128,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[128,20,8,8]{3,2,1,0} compare(bf16[128,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[128,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[128,20,8,8]{3,2,1,0} pad(u32[128,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[128,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[128,20,8,8]{3,2,1,0} select(pred[128,20,8,8]{3,2,1,0} %compare.123, u32[128,20,8,8]{3,2,1,0} %pad.120, u32[128,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[128,20,4,4]{3,2,1,0} reduce-window(u32[128,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[128,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[128,20,4,4]{3,2,1,0} maximum(bf16[128,20,4,4]{3,2,1,0} %reduce-window.105, bf16[128,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[128,20,4,4]{3,2,1,0} batch-norm-inference(bf16[128,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[128,320]{1,0} reshape(bf16[128,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[128,50]{1,0} dot(bf16[128,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[128,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[128,50]{1,0} maximum(bf16[128,50]{1,0} %dot.139, bf16[128,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[128,10]{1,0} dot(bf16[128,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[128]{0} reduce(bf16[128,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %dot.143, bf16[128,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[128,10]{1,0} exponential(bf16[128,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[128]{0} reduce(bf16[128,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[128]{0} log(bf16[128]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[128,10]{1,0} broadcast(bf16[128]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[128,10]{1,0} subtract(bf16[128,10]{1,0} %subtract.151, bf16[128,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[128,10]{1,0}) tuple(bf16[128,10]{1,0} %subtract.161)
}


## END_GRAPH


[ScheduleSyncTensorsGraph]
TensorsGraphInfo:
  mark_step (/workspaces/work/pytorch/xla/torch_xla/core/xla_model.py:949)
  next (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:42)
  __next__ (/workspaces/work/pytorch/xla/torch_xla/distributed/parallel_loader.py:30)
  inference_loop_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:260)
  inference_mnist (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:291)
  _mp_fn (/workspaces/work/pytorch/xla/test/fsdp_mnist_quant_test.py:302)
  __call__ (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:341)
  _thread_fn (/workspaces/work/pytorch/xla/torch_xla/experimental/pjrt.py:238)
  run (/usr/local/lib/python3.8/concurrent/futures/thread.py:57)
  _worker (/usr/local/lib/python3.8/concurrent/futures/thread.py:80)
  run (/usr/local/lib/python3.8/threading.py:870)
  _bootstrap_inner (/usr/local/lib/python3.8/threading.py:932)
  _bootstrap (/usr/local/lib/python3.8/threading.py:890)

Hashes: (d703b40cf25aaa000e93b16d13b3559)

## BEGIN_GRAPH
HloModule IrToHlo.163, entry_computation_layout={(f32[],bf16[13,320]{1,0},bf16[3,50]{1,0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20]{0},bf16[20,10,5,5]{0,1,3,2},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10]{0},bf16[10,1,5,5]{0,3,2,1},f32[16,1,28,28]{3,2,0,1})->(bf16[16,10]{1,0})}

%AddComputation.9 (x.10: bf16[], y.11: bf16[]) -> bf16[] {
  %x.10 = bf16[] parameter(0)
  %y.11 = bf16[] parameter(1)
  ROOT %add.12 = bf16[] add(bf16[] %x.10, bf16[] %y.11)
}

%AddComputation.24 (x.25: bf16[], y.26: bf16[]) -> bf16[] {
  %x.25 = bf16[] parameter(0)
  %y.26 = bf16[] parameter(1)
  ROOT %add.27 = bf16[] add(bf16[] %x.25, bf16[] %y.26)
}

%max_BF16.57 (lhs.58: bf16[], rhs.59: bf16[]) -> bf16[] {
  %lhs.58 = bf16[] parameter(0)
  %rhs.59 = bf16[] parameter(1)
  ROOT %maximum.60 = bf16[] maximum(bf16[] %lhs.58, bf16[] %rhs.59)
}

%ge_BF16.63 (lhs.64: bf16[], rhs.65: bf16[]) -> pred[] {
  %lhs.64 = bf16[] parameter(0)
  %rhs.65 = bf16[] parameter(1)
  ROOT %compare.66 = pred[] compare(bf16[] %lhs.64, bf16[] %rhs.65), direction=GE
}

%max_BF16.67 (lhs.68: bf16[], rhs.69: bf16[]) -> bf16[] {
  %lhs.68 = bf16[] parameter(0)
  %rhs.69 = bf16[] parameter(1)
  ROOT %maximum.70 = bf16[] maximum(bf16[] %lhs.68, bf16[] %rhs.69)
}

%min_U32.81 (lhs.82: u32[], rhs.83: u32[]) -> u32[] {
  %lhs.82 = u32[] parameter(0)
  %rhs.83 = u32[] parameter(1)
  ROOT %minimum.84 = u32[] minimum(u32[] %lhs.82, u32[] %rhs.83)
}

%max_BF16.101 (lhs.102: bf16[], rhs.103: bf16[]) -> bf16[] {
  %lhs.102 = bf16[] parameter(0)
  %rhs.103 = bf16[] parameter(1)
  ROOT %maximum.104 = bf16[] maximum(bf16[] %lhs.102, bf16[] %rhs.103)
}

%ge_BF16.107 (lhs.108: bf16[], rhs.109: bf16[]) -> pred[] {
  %lhs.108 = bf16[] parameter(0)
  %rhs.109 = bf16[] parameter(1)
  ROOT %compare.110 = pred[] compare(bf16[] %lhs.108, bf16[] %rhs.109), direction=GE
}

%max_BF16.111 (lhs.112: bf16[], rhs.113: bf16[]) -> bf16[] {
  %lhs.112 = bf16[] parameter(0)
  %rhs.113 = bf16[] parameter(1)
  ROOT %maximum.114 = bf16[] maximum(bf16[] %lhs.112, bf16[] %rhs.113)
}

%min_U32.125 (lhs.126: u32[], rhs.127: u32[]) -> u32[] {
  %lhs.126 = u32[] parameter(0)
  %rhs.127 = u32[] parameter(1)
  ROOT %minimum.128 = u32[] minimum(u32[] %lhs.126, u32[] %rhs.127)
}

%MaxComputation.145 (x.146: bf16[], y.147: bf16[]) -> bf16[] {
  %x.146 = bf16[] parameter(0)
  %y.147 = bf16[] parameter(1)
  ROOT %maximum.148 = bf16[] maximum(bf16[] %x.146, bf16[] %y.147)
}

%AddComputation.154 (x.155: bf16[], y.156: bf16[]) -> bf16[] {
  %x.155 = bf16[] parameter(0)
  %y.156 = bf16[] parameter(1)
  ROOT %add.157 = bf16[] add(bf16[] %x.155, bf16[] %y.156)
}

ENTRY %IrToHlo.163 (p0.1: f32[], p1.2: bf16[13,320], p2.17: bf16[3,50], p3.36: bf16[20], p4.37: bf16[20], p5.38: bf16[20], p6.39: bf16[20], p7.40: bf16[20], p8.41: bf16[20,10,5,5], p9.42: bf16[10], p10.43: bf16[10], p11.44: bf16[10], p12.45: bf16[10], p13.46: bf16[10], p14.47: bf16[10,1,5,5], p15.48: f32[16,1,28,28]) -> (bf16[16,10]) {
  %p2.17 = bf16[3,50]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.18 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.19 = bf16[12,50]{1,0} pad(bf16[3,50]{1,0} %p2.17, bf16[] %constant.18), padding=0_9x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p1.2 = bf16[13,320]{1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %constant.3 = bf16[] constant(0), metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %pad.4 = bf16[52,320]{1,0} pad(bf16[13,320]{1,0} %p1.2, bf16[] %constant.3), padding=0_39x0_0, metadata={op_type="aten__constant_pad_nd" op_name="aten__constant_pad_nd" source_file="_all_gather_using_all_reduce@xla_model.py" source_line=653}
  %p0.1 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_get_all_reduce_token@xla_model.py" source_line=484}
  %convert.5 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.6 = (bf16[52,320]{1,0}, bf16[]) tuple(bf16[52,320]{1,0} %pad.4, bf16[] %convert.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.7 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.8 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %tuple.6), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.13 = (bf16[52,320]{1,0}, bf16[]) all-reduce(bf16[52,320]{1,0} %get-tuple-element.7, bf16[] %get-tuple-element.8), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.15 = bf16[] get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.16 = f32[] convert(bf16[] %get-tuple-element.15), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.20 = bf16[] convert(f32[] %convert.16), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %tuple.21 = (bf16[12,50]{1,0}, bf16[]) tuple(bf16[12,50]{1,0} %pad.19, bf16[] %convert.20), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.22 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.23 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %tuple.21), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %all-reduce.28 = (bf16[12,50]{1,0}, bf16[]) all-reduce(bf16[12,50]{1,0} %get-tuple-element.22, bf16[] %get-tuple-element.23), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %get-tuple-element.30 = bf16[] get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %convert.31 = f32[] convert(bf16[] %get-tuple-element.30), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %p15.48 = f32[16,1,28,28]{3,2,0,1} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %convert.49 = bf16[16,1,28,28]{3,2,0,1} convert(f32[16,1,28,28]{3,2,0,1} %p15.48), metadata={op_type="xla__cast" op_name="xla__cast" source_file="fn@xla_fully_sharded_data_parallel.py" source_line=1722}
  %p14.47 = bf16[10,1,5,5]{0,3,2,1} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.50 = bf16[16,10,24,24]{3,2,1,0} convolution(bf16[16,1,28,28]{3,2,0,1} %convert.49, bf16[10,1,5,5]{0,3,2,1} %p14.47), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p13.46 = bf16[10]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.51 = bf16[16,24,24,10]{3,2,1,0} broadcast(bf16[10]{0} %p13.46), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.52 = bf16[16,10,24,24]{1,3,2,0} transpose(bf16[16,24,24,10]{3,2,1,0} %broadcast.51), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.53 = bf16[16,10,24,24]{3,2,1,0} add(bf16[16,10,24,24]{3,2,1,0} %convolution.50, bf16[16,10,24,24]{1,3,2,0} %transpose.52), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.54 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.55 = bf16[16,10,24,24]{3,2,1,0} pad(bf16[16,10,24,24]{3,2,1,0} %add.53, bf16[] %constant.54), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.56 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.61 = bf16[16,10,12,12]{3,2,1,0} reduce-window(bf16[16,10,24,24]{3,2,1,0} %pad.55, bf16[] %constant.56), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.57, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.62 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.71 = bf16[16,10,24,24]{3,2,1,0} select-and-scatter(bf16[16,10,24,24]{3,2,1,0} %pad.55, bf16[16,10,12,12]{3,2,1,0} %reduce-window.61, bf16[] %constant.62), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.63, scatter=%max_BF16.67, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.78 = bf16[16,10,24,24]{3,2,1,0} broadcast(bf16[] %constant.62), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.79 = pred[16,10,24,24]{3,2,1,0} compare(bf16[16,10,24,24]{3,2,1,0} %select-and-scatter.71, bf16[16,10,24,24]{3,2,1,0} %broadcast.78), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.72 = u32[576]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.73 = u32[24,24]{1,0} reshape(u32[576]{0} %iota.72), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.74 = u32[16,10,24,24]{3,2,1,0} broadcast(u32[24,24]{1,0} %reshape.73), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.75 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.76 = u32[16,10,24,24]{3,2,1,0} pad(u32[16,10,24,24]{3,2,1,0} %broadcast.74, u32[] %constant.75), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.77 = u32[16,10,24,24]{3,2,1,0} broadcast(u32[] %constant.75), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.80 = u32[16,10,24,24]{3,2,1,0} select(pred[16,10,24,24]{3,2,1,0} %compare.79, u32[16,10,24,24]{3,2,1,0} %pad.76, u32[16,10,24,24]{3,2,1,0} %broadcast.77), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.85 = u32[16,10,12,12]{3,2,1,0} reduce-window(u32[16,10,24,24]{3,2,1,0} %select.80, u32[] %constant.75), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.81, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p9.42 = bf16[10]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.90 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.91 = bf16[10]{0} broadcast(bf16[] %constant.90), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.92 = bf16[10]{0} add(bf16[10]{0} %p9.42, bf16[10]{0} %broadcast.91), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.93 = bf16[10]{0} rsqrt(bf16[10]{0} %add.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.86 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.87 = bf16[16,10,12,12]{3,2,1,0} broadcast(bf16[] %constant.86), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.88 = bf16[16,10,12,12]{3,2,1,0} maximum(bf16[16,10,12,12]{3,2,1,0} %reduce-window.61, bf16[16,10,12,12]{3,2,1,0} %broadcast.87), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p12.45 = bf16[10]{0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p11.44 = bf16[10]{0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p10.43 = bf16[10]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.89 = bf16[16,10,12,12]{3,2,1,0} batch-norm-inference(bf16[16,10,12,12]{3,2,1,0} %maximum.88, bf16[10]{0} %p12.45, bf16[10]{0} %p11.44, bf16[10]{0} %p10.43, bf16[10]{0} %p9.42), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %p8.41 = bf16[20,10,5,5]{0,1,3,2} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %convolution.94 = bf16[16,20,8,8]{3,2,1,0} convolution(bf16[16,10,12,12]{3,2,1,0} %batch-norm-inference.89, bf16[20,10,5,5]{0,1,3,2} %p8.41), window={size=5x5}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %p7.40 = bf16[20]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=459}
  %broadcast.95 = bf16[16,8,8,20]{3,2,1,0} broadcast(bf16[20]{0} %p7.40), dimensions={3}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %transpose.96 = bf16[16,20,8,8]{1,3,2,0} transpose(bf16[16,8,8,20]{3,2,1,0} %broadcast.95), dimensions={0,3,1,2}, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %add.97 = bf16[16,20,8,8]{3,2,1,0} add(bf16[16,20,8,8]{3,2,1,0} %convolution.94, bf16[16,20,8,8]{1,3,2,0} %transpose.96), metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=459}
  %constant.98 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.99 = bf16[16,20,8,8]{3,2,1,0} pad(bf16[16,20,8,8]{3,2,1,0} %add.97, bf16[] %constant.98), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.100 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.105 = bf16[16,20,4,4]{3,2,1,0} reduce-window(bf16[16,20,8,8]{3,2,1,0} %pad.99, bf16[] %constant.100), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_BF16.101, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.106 = bf16[] constant(-inf), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select-and-scatter.115 = bf16[16,20,8,8]{3,2,1,0} select-and-scatter(bf16[16,20,8,8]{3,2,1,0} %pad.99, bf16[16,20,4,4]{3,2,1,0} %reduce-window.105, bf16[] %constant.106), window={size=1x1x2x2 stride=1x1x2x2}, select=%ge_BF16.107, scatter=%max_BF16.111, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.122 = bf16[16,20,8,8]{3,2,1,0} broadcast(bf16[] %constant.106), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %compare.123 = pred[16,20,8,8]{3,2,1,0} compare(bf16[16,20,8,8]{3,2,1,0} %select-and-scatter.115, bf16[16,20,8,8]{3,2,1,0} %broadcast.122), direction=NE, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %iota.116 = u32[64]{0} iota(), iota_dimension=0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reshape.117 = u32[8,8]{1,0} reshape(u32[64]{0} %iota.116), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.118 = u32[16,20,8,8]{3,2,1,0} broadcast(u32[8,8]{1,0} %reshape.117), dimensions={2,3}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %constant.119 = u32[] constant(4294967295), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %pad.120 = u32[16,20,8,8]{3,2,1,0} pad(u32[16,20,8,8]{3,2,1,0} %broadcast.118, u32[] %constant.119), padding=0_0x0_0x0_0x0_0, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %broadcast.121 = u32[16,20,8,8]{3,2,1,0} broadcast(u32[] %constant.119), dimensions={}, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %select.124 = u32[16,20,8,8]{3,2,1,0} select(pred[16,20,8,8]{3,2,1,0} %compare.123, u32[16,20,8,8]{3,2,1,0} %pad.120, u32[16,20,8,8]{3,2,1,0} %broadcast.121), metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %reduce-window.129 = u32[16,20,4,4]{3,2,1,0} reduce-window(u32[16,20,8,8]{3,2,1,0} %select.124, u32[] %constant.119), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%min_U32.125, metadata={op_type="aten__max_pool2d" op_name="aten__max_pool2d" source_file="_max_pool2d@functional.py" source_line=782}
  %p3.36 = bf16[20]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %constant.134 = bf16[] constant(1.001e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %broadcast.135 = bf16[20]{0} broadcast(bf16[] %constant.134), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %add.136 = bf16[20]{0} add(bf16[20]{0} %p3.36, bf16[20]{0} %broadcast.135), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %rsqrt.137 = bf16[20]{0} rsqrt(bf16[20]{0} %add.136), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %constant.130 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.131 = bf16[16,20,4,4]{3,2,1,0} broadcast(bf16[] %constant.130), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.132 = bf16[16,20,4,4]{3,2,1,0} maximum(bf16[16,20,4,4]{3,2,1,0} %reduce-window.105, bf16[16,20,4,4]{3,2,1,0} %broadcast.131), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %p6.39 = bf16[20]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p5.38 = bf16[20]{0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %p4.37 = bf16[20]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2455}
  %batch-norm-inference.133 = bf16[16,20,4,4]{3,2,1,0} batch-norm-inference(bf16[16,20,4,4]{3,2,1,0} %maximum.132, bf16[20]{0} %p6.39, bf16[20]{0} %p5.38, bf16[20]{0} %p4.37, bf16[20]{0} %p3.36), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2455}
  %reshape.138 = bf16[16,320]{1,0} reshape(bf16[16,20,4,4]{3,2,1,0} %batch-norm-inference.133), metadata={op_type="aten__view" op_name="aten__view" source_file="forward@utils.py" source_line=92}
  %get-tuple-element.14 = bf16[52,320]{1,0} get-tuple-element((bf16[52,320]{1,0}, bf16[]) %all-reduce.13), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.34 = bf16[50,320]{1,0} slice(bf16[52,320]{1,0} %get-tuple-element.14), slice={[0:50], [0:320]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.35 = bf16[320,50]{0,1} transpose(bf16[50,320]{1,0} %slice.34), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.139 = bf16[16,50]{1,0} dot(bf16[16,320]{1,0} %reshape.138, bf16[320,50]{0,1} %transpose.35), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.140 = bf16[] constant(0), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %broadcast.141 = bf16[16,50]{1,0} broadcast(bf16[] %constant.140), dimensions={}, metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %maximum.142 = bf16[16,50]{1,0} maximum(bf16[16,50]{1,0} %dot.139, bf16[16,50]{1,0} %broadcast.141), metadata={op_type="aten__relu" op_name="aten__relu" source_file="relu@functional.py" source_line=1457}
  %get-tuple-element.29 = bf16[12,50]{1,0} get-tuple-element((bf16[12,50]{1,0}, bf16[]) %all-reduce.28), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="all_reduce@xla_model.py" source_line=592}
  %slice.32 = bf16[10,50]{1,0} slice(bf16[12,50]{1,0} %get-tuple-element.29), slice={[0:10], [0:50]}, metadata={op_type="xla__select" op_name="xla__select" source_file="forward@utils.py" source_line=92}
  %transpose.33 = bf16[50,10]{0,1} transpose(bf16[10,50]{1,0} %slice.32), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="forward@utils.py" source_line=92}
  %dot.143 = bf16[16,10]{1,0} dot(bf16[16,50]{1,0} %maximum.142, bf16[50,10]{0,1} %transpose.33), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="forward@utils.py" source_line=92}
  %constant.144 = bf16[] constant(-inf), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.149 = bf16[16]{0} reduce(bf16[16,10]{1,0} %dot.143, bf16[] %constant.144), dimensions={1}, to_apply=%MaxComputation.145, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.150 = bf16[16,10]{1,0} broadcast(bf16[16]{0} %reduce.149), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.151 = bf16[16,10]{1,0} subtract(bf16[16,10]{1,0} %dot.143, bf16[16,10]{1,0} %broadcast.150), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %exponential.152 = bf16[16,10]{1,0} exponential(bf16[16,10]{1,0} %subtract.151), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %constant.153 = bf16[] constant(0), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %reduce.158 = bf16[16]{0} reduce(bf16[16,10]{1,0} %exponential.152, bf16[] %constant.153), dimensions={1}, to_apply=%AddComputation.154, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %log.159 = bf16[16]{0} log(bf16[16]{0} %reduce.158), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %broadcast.160 = bf16[16,10]{1,0} broadcast(bf16[16]{0} %log.159), dimensions={0}, metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  %subtract.161 = bf16[16,10]{1,0} subtract(bf16[16,10]{1,0} %subtract.151, bf16[16,10]{1,0} %broadcast.160), metadata={op_type="aten__log_softmax" op_name="aten__log_softmax" source_file="log_softmax@functional.py" source_line=1932}
  ROOT %tuple.162 = (bf16[16,10]{1,0}) tuple(bf16[16,10]{1,0} %subtract.161)
}


## END_GRAPH


