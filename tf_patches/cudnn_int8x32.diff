# TODO: Try removing with the next pin update. See https://github.com/pytorch/xla/pull/4615#issuecomment-1428883781
diff --git a/tensorflow/compiler/xla/debug_options_flags.cc b/tensorflow/compiler/xla/debug_options_flags.cc
index fac64573c54..dec0f1823c1 100644
--- a/tensorflow/compiler/xla/debug_options_flags.cc
+++ b/tensorflow/compiler/xla/debug_options_flags.cc
@@ -110,7 +110,6 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {
       DebugOptions::PARTITIONING_ALGORITHM_NOOP);

   opts.set_xla_gpu_enable_triton_gemm(false);
-  opts.set_xla_gpu_enable_cudnn_int8x32_convolution_reordering(false);
   return opts;
 }

@@ -882,13 +881,6 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,
                 bool_setter_for(&DebugOptions::set_xla_gpu_enable_triton_gemm),
                 debug_options->xla_gpu_enable_triton_gemm(),
                 "Use Triton-based matrix multiplication."));
-  flag_list->push_back(tsl::Flag(
-      "xla_gpu_enable_cudnn_int8x32_convolution_reordering",
-      bool_setter_for(
-          &DebugOptions::
-              set_xla_gpu_enable_cudnn_int8x32_convolution_reordering),
-      debug_options->xla_gpu_enable_cudnn_int8x32_convolution_reordering(),
-      "Enable cuDNN frontend for int8x32 convolutions with reordered filter."));
 }  // NOLINT(readability/fn_size)

 // Allocates flag_values and flag_objects; this function must not be called more
diff --git a/tensorflow/compiler/xla/service/gpu/BUILD b/tensorflow/compiler/xla/service/gpu/BUILD
index 7fff6bdad13..b21105a42d7 100644
--- a/tensorflow/compiler/xla/service/gpu/BUILD
+++ b/tensorflow/compiler/xla/service/gpu/BUILD
@@ -1658,11 +1658,8 @@ cc_library(
     srcs = ["cudnn_vectorize_convolutions.cc"],
     hdrs = ["cudnn_vectorize_convolutions.h"],
     deps = [
-        ":backend_configs_cc",
-        ":cublas_cudnn",
         ":cudnn_support_utils",
         ":stream_executor_util",
-        "//tensorflow/compiler/xla:shape_util",
         "//tensorflow/compiler/xla:statusor",
         "//tensorflow/compiler/xla/client:xla_builder",
         "//tensorflow/compiler/xla/hlo/ir:hlo",
diff --git a/tensorflow/compiler/xla/service/gpu/conv_layout_normalization.cc b/tensorflow/compiler/xla/service/gpu/conv_layout_normalization.cc
index 7254e02f513..c2ab437b48e 100644
--- a/tensorflow/compiler/xla/service/gpu/conv_layout_normalization.cc
+++ b/tensorflow/compiler/xla/service/gpu/conv_layout_normalization.cc
@@ -16,23 +16,22 @@ limitations under the License.
 #include "tensorflow/compiler/xla/service/gpu/conv_layout_normalization.h"

 #include <optional>
-#include <tuple>
 #include <vector>

 #include "tensorflow/compiler/xla/hlo/ir/hlo_casting_utils.h"
-#include "tensorflow/compiler/xla/hlo/ir/hlo_instructions.h"
 #include "tensorflow/compiler/xla/hlo/ir/hlo_module.h"
-#include "tensorflow/compiler/xla/layout_util.h"
 #include "tensorflow/compiler/xla/service/gpu/cublas_cudnn.h"
 #include "tensorflow/compiler/xla/service/hlo_creation_utils.h"
-#include "tensorflow/compiler/xla/shape_util.h"

 namespace xla {
 namespace gpu {
-namespace {

-StatusOr<HloInstruction*> UpdateLayoutForCudnnConvolution(
-    HloCustomCallInstruction* hlo) {
+StatusOr<std::optional<HloInstruction*>>
+NormalizeLayoutForCustomCallConvolution(HloCustomCallInstruction* hlo) {
+  if (!IsCustomCallToDnnConvolution(*hlo)) {
+    return {std::nullopt};
+  }
+
   HloInstruction* lhs = hlo->mutable_operand(0);
   HloInstruction* rhs = hlo->mutable_operand(1);
   const ConvolutionDimensionNumbers& dim_numbers =
@@ -162,64 +161,8 @@ StatusOr<HloInstruction*> UpdateLayoutForCudnnConvolution(
   } else {
     bc_to_orig = MakeBitcastHlo(normalized_conv, hlo->shape());
   }
-  return bc_to_orig;
-}
-
-// Normalize the layout of cuDNN int8x32 filter reordering custom call
-// (implemented by calling `cudnnReorderFilterAndBias`), which should be
-// followed by a convolution.
-// Both the input and the output shape for the filter operand must have the
-// NCHW_VECT_C layout.
-HloInstruction* UpdateLayoutForCudnnConvolutionReordering(
-    HloCustomCallInstruction* hlo) {
-  // The custom call may have either one (filter) or two (filter and bias)
-  // operands. The number of outputs matches the number of inputs.
-  Shape const* filter_shape;
-  Shape const* bias_shape;
-  std::tie(filter_shape, bias_shape) =
-      hlo->shape().IsTuple() ? std::make_tuple(&hlo->shape().tuple_shapes(0),
-                                               &hlo->shape().tuple_shapes(1))
-                             : std::make_tuple(&hlo->shape(), nullptr);
-
-  // Transpose the filter to match the expected layout (NCHW_VECT_C).
-  // This bias is 1D, so the shape doesn't need to be updated.
-  auto new_filter_shape =
-      ShapeUtil::MakeShapeWithDescendingLayoutAndSamePhysicalLayout(
-          *filter_shape);
-  auto dimensions = LayoutUtil::MakeLayoutFromMajorToMinor(
-      filter_shape->layout().minor_to_major());
-  HloInstruction* transpose = hlo->AddInstruction(
-      HloInstruction::CreateTranspose(new_filter_shape, hlo->mutable_operand(0),
-                                      dimensions.minor_to_major()));
-
-  // Create a replacement custom-call with layout-normalized inputs.
-  HloInstruction* custom_call;
-  if (bias_shape != nullptr) {
-    custom_call =
-        hlo->parent()->AddInstruction(HloInstruction::CreateCustomCall(
-            ShapeUtil::MakeTupleShape({new_filter_shape, *bias_shape}),
-            {transpose, hlo->mutable_operand(1)}, hlo->custom_call_target()));
-  } else {
-    custom_call =
-        hlo->parent()->AddInstruction(HloInstruction::CreateCustomCall(
-            new_filter_shape, {transpose}, hlo->custom_call_target()));
-  }
-  return MakeBitcastHlo(custom_call, hlo->shape());
-}
-
-}  // namespace

-StatusOr<std::optional<HloInstruction*>> NormalizeLayoutForGpuCustomCalls(
-    HloCustomCallInstruction* hlo) {
-  if (IsCustomCallToDnnConvolution(*hlo)) {
-    TF_ASSIGN_OR_RETURN(HloInstruction * bc_to_orig,
-                        UpdateLayoutForCudnnConvolution(hlo));
-    return std::make_optional(bc_to_orig);
-  }
-  if (IsCudnnConvolutionReorder(*hlo)) {
-    return std::make_optional(UpdateLayoutForCudnnConvolutionReordering(hlo));
-  }
-  return {std::nullopt};
+  return std::make_optional(bc_to_orig);
 }

 }  // end namespace gpu
diff --git a/tensorflow/compiler/xla/service/gpu/conv_layout_normalization.h b/tensorflow/compiler/xla/service/gpu/conv_layout_normalization.h
index bb38a299435..c6305784f94 100644
--- a/tensorflow/compiler/xla/service/gpu/conv_layout_normalization.h
+++ b/tensorflow/compiler/xla/service/gpu/conv_layout_normalization.h
@@ -28,8 +28,8 @@ limitations under the License.
 namespace xla {
 namespace gpu {

-StatusOr<std::optional<HloInstruction*>> NormalizeLayoutForGpuCustomCalls(
-    HloCustomCallInstruction*);
+StatusOr<std::optional<HloInstruction*>>
+NormalizeLayoutForCustomCallConvolution(HloCustomCallInstruction*);

 }  // end namespace gpu
 }  // end namespace xla
diff --git a/tensorflow/compiler/xla/service/gpu/cudnn_vectorize_convolutions.cc b/tensorflow/compiler/xla/service/gpu/cudnn_vectorize_convolutions.cc
index f9af729cf36..7511e81a423 100644
--- a/tensorflow/compiler/xla/service/gpu/cudnn_vectorize_convolutions.cc
+++ b/tensorflow/compiler/xla/service/gpu/cudnn_vectorize_convolutions.cc
@@ -16,21 +16,16 @@ limitations under the License.
 #include "tensorflow/compiler/xla/service/gpu/cudnn_vectorize_convolutions.h"

 #include <optional>
-#include <string>
 #include <vector>

 #include "tensorflow/compiler/xla/client/xla_builder.h"
 #include "tensorflow/compiler/xla/hlo/ir/hlo_casting_utils.h"
 #include "tensorflow/compiler/xla/hlo/ir/hlo_instructions.h"
-#include "tensorflow/compiler/xla/service/gpu/backend_configs.pb.h"
-#include "tensorflow/compiler/xla/service/gpu/cublas_cudnn.h"
 #include "tensorflow/compiler/xla/service/gpu/cudnn_support_utils.h"
 #include "tensorflow/compiler/xla/service/gpu/stream_executor_util.h"
-#include "tensorflow/compiler/xla/shape_util.h"

 namespace xla {
 namespace gpu {
-namespace {

 // Finds convolutions that this pass may be able to transform, namely int8_t
 // cudnn forward or forward-bias-activation convolutions
@@ -254,37 +249,6 @@ static ConvolutionDimensionNumbers VectorizeDnums(
   return dnums;
 }

-// Reorders the convolution's filter and bias (if present) according to
-// cudnnReorderFilterAndBias.  Also marks that the filter + bias are reordered
-// in the conv's backend-config.
-Status ReorderInt8NchwVect(HloCustomCallInstruction* conv, XlaOp* operands) {
-  // Update convolution backend config.
-  TF_ASSIGN_OR_RETURN(auto config,
-                      conv->backend_config<CudnnConvBackendConfig>());
-  config.set_reordered_int8_nchw_vect(true);
-  TF_RETURN_IF_ERROR(conv->set_backend_config(config));
-
-  XlaBuilder& builder = *operands->builder();
-  Shape filter_shape = builder.GetShape(operands[1]).value();
-
-  if (conv->operand_count() > 2) {
-    // Reorder filter and bias.
-    Shape bias_shape = builder.GetShape(operands[2]).value();
-    XlaOp reorder = CustomCall(
-        &builder, std::string(kCudnnConvReorderFilterAndBiasCallTarget),
-        {operands[1], operands[2]},
-        ShapeUtil::MakeTupleShape({filter_shape, bias_shape}));
-    operands[1] = GetTupleElement(reorder, 0);
-    operands[2] = GetTupleElement(reorder, 1);
-  } else {
-    // Reorder just the filter.
-    operands[1] =
-        CustomCall(&builder, std::string(kCudnnConvReorderFilterCallTarget),
-                   {operands[1]}, filter_shape);
-  }
-  return OkStatus();
-}
-
 // Tries to vectorize an already-vectorized convolution.
 //
 // That is, given a convolution of shape [N, C/k, H, W, k], changes it to have
@@ -371,13 +335,6 @@ static StatusOr<bool> TryRevectorizeConv(
         conv->ToString());
   }

-  // Reorder filter and bias for the int8x32 convolutions.
-  const auto& debug_options = conv->GetModule()->config().debug_options();
-  if (input_shape.element_type() == xla::S8 && vect_size == 32 &&
-      debug_options.xla_gpu_enable_cudnn_int8x32_convolution_reordering()) {
-    TF_RETURN_IF_ERROR(ReorderInt8NchwVect(conv, new_operands.data()));
-  }
-
   // The custom-call returns a tuple (new_output_shape, u8[0]), where the second
   // value in the tuple represents the convolution's scratch memory.
   DimensionVector new_output_dims(output_shape.dimensions().begin(),
@@ -502,13 +459,6 @@ static StatusOr<bool> TryVectorizeConv(
         conv->ToString());
   }

-  // Reorder filter and bias for the int8x32 convolutions.
-  const auto& debug_options = conv->GetModule()->config().debug_options();
-  if (input_shape.element_type() == xla::S8 && vect_size == 32 &&
-      debug_options.xla_gpu_enable_cudnn_int8x32_convolution_reordering()) {
-    TF_RETURN_IF_ERROR(ReorderInt8NchwVect(conv, new_operands.data()));
-  }
-
   // The custom-call returns a tuple (new_output_shape, u8[0]), where the second
   // value in the tuple represents the convolution's scratch memory.
   Shape new_output_shape = SplitShapeAtDim(
@@ -545,8 +495,6 @@ static StatusOr<bool> TryVectorizeConv(
   return true;
 }

-}  // namespace
-
 StatusOr<bool> CudnnVectorizeConvolutions::Run(
     HloModule* module,
     const absl::flat_hash_set<absl::string_view>& execution_threads) {
diff --git a/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc b/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc
index 9e4d8c95bb0..9b242ce3b04 100644
--- a/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc
+++ b/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc
@@ -836,7 +836,8 @@ Status GpuCompiler::OptimizeHloPostLayoutAssignment(
     pipeline.AddPass<GemmBroadcastFoldingRewriter>();

     if (debug_options.xla_gpu_normalize_layouts()) {
-      pipeline.AddPass<LayoutNormalization>(&NormalizeLayoutForGpuCustomCalls);
+      pipeline.AddPass<LayoutNormalization>(
+          &NormalizeLayoutForCustomCallConvolution);
       pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(options);
     }
     pipeline.AddPass<BroadcastCanonicalizer>();
diff --git a/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc b/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc
index cb2aa3f2ac1..4c995b4f142 100644
--- a/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc
+++ b/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc
@@ -3485,7 +3485,7 @@ std::tuple<int, int> GetTensorVectorSizeAndDim(
 tsl::StatusOr<cudnn_frontend::Tensor> CreateCudnnTensor(
     absl::Span<const int64_t> dims, absl::Span<const int64_t> strides,
     int64_t uid, dnn::DataType dtype, int64_t vec_count, int64_t vec_dim,
-    bool is_virtual = false, bool is_reordered_nchw_vect = false) {
+    bool is_virtual = false) {
   auto tensor = cudnn_frontend::TensorBuilder()
                     .setDim(dims.size(), dims.data())
                     .setStride(strides.size(), strides.data())
@@ -3494,9 +3494,6 @@ tsl::StatusOr<cudnn_frontend::Tensor> CreateCudnnTensor(
                     .setDataType(ToCudnnDataType(dtype))
                     .setVectorCountAndDimension(vec_count, vec_dim)
                     .setVirtual(is_virtual)
-                    .setReorderType(is_reordered_nchw_vect
-                                        ? CUDNN_TENSOR_REORDERING_INT8x32
-                                        : CUDNN_TENSOR_REORDERING_NONE)
                     .build();
   RETURN_MSG_IF_CUDNN_ERROR(tensor);
   return tensor;
@@ -3523,6 +3520,11 @@ GetCudnnOperationGraph(dnn::ConvolutionKind kind, dnn::DataType input_type,
   std::vector<int64_t> input_strides = input_descriptor.vectorized_strides(
       dnn::DataLayout::kBatchDepthYX, vector_size, vector_dim);

+  if (vector_size == 32) {
+    return tsl::errors::Internal(
+        "cuDNN frontend doesn't support Tx32 at the moment.");
+  }
+
   TF_ASSIGN_OR_RETURN(auto tensor_x,
                       CreateCudnnTensor(input_dims, input_strides, 'x',
                                         input_type, vector_size, vector_dim));
@@ -3547,13 +3549,9 @@ GetCudnnOperationGraph(dnn::ConvolutionKind kind, dnn::DataType input_type,
   std::vector<int64_t> filter_strides = filter_descriptor.vectorized_strides(
       dnn::FilterLayout::kOutputInputYX, vector_size, vector_dim);

-  TF_ASSIGN_OR_RETURN(
-      auto tensor_w,
-      CreateCudnnTensor(
-          filter_dims, filter_strides, 'w', input_type, vector_size, vector_dim,
-          /*is_virtual=*/false,
-          /*is_reordered_nchw_vect=*/filter_descriptor.layout() ==
-              dnn::FilterLayout::kOutputInputYX32_CudnnReordered));
+  TF_ASSIGN_OR_RETURN(auto tensor_w,
+                      CreateCudnnTensor(filter_dims, filter_strides, 'w',
+                                        input_type, vector_size, vector_dim));

   // conv_desc.
   auto mode = convolution_descriptor.convolution_not_crosscorr()
@@ -3657,6 +3655,11 @@ GetCudnnFusedOperationGraph(
   std::vector<int64_t> input_strides = input_descriptor.vectorized_strides(
       dnn::DataLayout::kBatchDepthYX, vector_size, vector_dim);

+  if (vector_size == 32) {
+    return tsl::errors::Internal(
+        "cuDNN frontend doesn't support Tx32 at the moment.");
+  }
+
   TF_ASSIGN_OR_RETURN(auto tensor_x,
                       CreateCudnnTensor(input_dims, input_strides, 'x',
                                         input_type, vector_size, vector_dim));
@@ -3681,13 +3684,9 @@ GetCudnnFusedOperationGraph(
       dnn::FilterLayout::kOutputInputYX, vector_size, vector_dim);
   std::vector<int64_t> filter_strides = filter_descriptor.vectorized_strides(
       dnn::FilterLayout::kOutputInputYX, vector_size, vector_dim);
-  TF_ASSIGN_OR_RETURN(
-      auto tensor_w,
-      CreateCudnnTensor(
-          filter_dims, filter_strides, 'w', input_type, vector_size, vector_dim,
-          /*is_virtual=*/false,
-          /*is_reordered_nchw_vect=*/filter_descriptor.layout() ==
-              dnn::FilterLayout::kOutputInputYX32_CudnnReordered));
+  TF_ASSIGN_OR_RETURN(auto tensor_w,
+                      CreateCudnnTensor(filter_dims, filter_strides, 'w',
+                                        input_type, vector_size, vector_dim));

   // For the purposes of the cudnn graph, say that the bias tensor has the same
   // layout as the output tensor.  It doesn't actually matter, because bias is a
@@ -4822,20 +4821,17 @@ tsl::Status CudnnSupport::GetConvolveRunners(
     const dnn::ConvolutionDescriptor& convolution_descriptor, bool use_fallback,
     ScratchAllocator* /*scratch_allocator*/,
     std::vector<std::unique_ptr<const dnn::ConvRunner>>* out_exec_plans) {
+  // All current versions of the frontend API lack support for Tx32
+  // convolutions.
+  const bool is_unsupported_x32 =
+      input_descriptor.layout() == dnn::kBatchDepthYX32;
+
   // cuDNN frontend support became sufficiently stable to use in 8.1.
   // TODO(awpr): remove this condition once support for cuDNN 8.0 is dropped.
   const bool is_pre_frontend_cudnn = CUDNN_VERSION < 8100;

-  // cuDNN frontend support for Tx32 convolutions added in 8.3.
-  // If the filter is not reordered, do not use frontend (it is slow).
-  const bool is_disabled_x32 =
-      input_descriptor.layout() == dnn::kBatchDepthYX32 &&
-      (CUDNN_VERSION < 8300 ||
-       filter_descriptor.layout() !=
-           dnn::FilterLayout::kOutputInputYX32_CudnnReordered);
-
   const bool actually_use_cudnn_frontend =
-      use_cudnn_frontend && !is_pre_frontend_cudnn && !is_disabled_x32;
+      use_cudnn_frontend && !is_pre_frontend_cudnn && !is_unsupported_x32;

   if (use_cudnn_frontend && !actually_use_cudnn_frontend) {
     // This will happen once per unique conv configuration/shape that gets
@@ -4847,8 +4843,8 @@ tsl::Status CudnnSupport::GetConvolveRunners(
               << "  filter: " << filter_descriptor.ToString() << "\n"
               << "  " << convolution_descriptor.ToString() << "\n"
               << "  ... because "
-              << (is_disabled_x32
-                      ? "Tx32 convolutions are disabled."
+              << (is_unsupported_x32
+                      ? "Tx32 convolutions are unsupported."
                       : "the current cuDNN version does not support it.");
   }

@@ -4933,12 +4929,6 @@ CudnnSupport::ConvolveRunnerFromDesc(
         ToCudnnDataType(GetConvAccumulatorType(input_type)));
     conv.set_use_tensor_op_math(algorithm_desc.tensor_ops_enabled());

-    if (filter_descriptor.layout() ==
-        dnn::FilterLayout::kOutputInputYX32_CudnnReordered) {
-      CHECK_CUDNN_OK(
-          cudnnSetConvolutionReorderType(conv.handle(), CUDNN_NO_REORDER));
-    }
-
     TF_ASSIGN_OR_RETURN(
         auto runner,
         CudnnLegacyConvRunner::Create(
@@ -5200,12 +5190,6 @@ CudnnSupport::FusedConvolveRunnerFromDesc(
         ToCudnnDataType(GetConvAccumulatorType(input_type)));
     conv.set_use_tensor_op_math(algorithm_desc.tensor_ops_enabled());

-    if (filter_descriptor.layout() ==
-        dnn::FilterLayout::kOutputInputYX32_CudnnReordered) {
-      CHECK_CUDNN_OK(
-          cudnnSetConvolutionReorderType(conv.handle(), CUDNN_NO_REORDER));
-    }
-
     // CUDNN v6 only supports CUDNN_NOT_PROPAGATE_NAN as the reluNanOpt for
     // activation descriptor. Note that this will change the nan propagation
     // behavior from separate conv, bias, and relu (which by default is
@@ -5275,26 +5259,23 @@ tsl::Status CudnnSupport::GetFusedConvolveRunners(
       false;
 #endif

+  // All current versions of the frontend API lack support for Tx32
+  // convolutions.
+  const bool is_unsupported_x32 =
+      input_descriptor.layout() == dnn::kBatchDepthYX32;
+
   // cuDNN frontend support became sufficiently stable to use in 8.1.
   // TODO(awpr): remove this condition once support for cuDNN 8.0 is dropped.
   const bool is_pre_frontend_cudnn = CUDNN_VERSION < 8100;

-  // cuDNN frontend support for Tx32 convolutions added in 8.3.
-  // If the filter is not reordered, do not use frontend (it is slow).
-  const bool is_disabled_x32 =
-      input_descriptor.layout() == dnn::kBatchDepthYX32 &&
-      (CUDNN_VERSION < 8300 ||
-       filter_descriptor.layout() !=
-           dnn::FilterLayout::kOutputInputYX32_CudnnReordered);
-
   const bool actually_use_cudnn_frontend =
       use_cudnn_frontend && !is_pre_frontend_cudnn &&
-      !is_broken_identity_fused_conv && !is_disabled_x32;
+      !is_broken_identity_fused_conv && !is_unsupported_x32;

   if (use_cudnn_frontend && !actually_use_cudnn_frontend) {
     const char* reason = "the current cuDNN version does not support it.";
-    if (is_disabled_x32) {
-      reason = "Tx32 convolutions are disabled.";
+    if (is_unsupported_x32) {
+      reason = "Tx32 convolutions are unsupported.";
     } else if (is_broken_identity_fused_conv) {
       reason = "it uses an identity activation.";
     }
diff --git a/tensorflow/compiler/xla/tests/BUILD b/tensorflow/compiler/xla/tests/BUILD
index a7ece361f17..efa6e41c4a0 100644
--- a/tensorflow/compiler/xla/tests/BUILD
+++ b/tensorflow/compiler/xla/tests/BUILD
@@ -1298,7 +1298,7 @@ xla_test(
     srcs = ["convolution_cudnn_test.cc"],
     backend_tags = {"gpu": [
         "gpu",
-        "requires-gpu-sm80",
+        "requires-gpu-sm70",
     ]},
     backends = ["gpu"],
     deps = [
diff --git a/tensorflow/compiler/xla/tests/convolution_cudnn_test.cc b/tensorflow/compiler/xla/tests/convolution_cudnn_test.cc
index 04b9126302d..596f2be8f77 100644
--- a/tensorflow/compiler/xla/tests/convolution_cudnn_test.cc
+++ b/tensorflow/compiler/xla/tests/convolution_cudnn_test.cc
@@ -61,60 +61,5 @@ ENTRY TestComputation {
   EXPECT_TRUE(RunAndCompare(kHlo, ErrorSpec{0, 0}));
 }

-XLA_TEST_F(ConvolutionHloTest, TestCudnnConvInt8x32BiasNonConst) {
-  // Test two GPU compiled HLOs, first version with vectorization disabled,
-  // second with vectorization enabled. The reference implementation
-  // (Interpreter) does not support the fused conv-add-relu-clamp operation,
-  // thus cannot be used.
-  if (!backend()
-           .default_stream_executor()
-           ->GetDeviceDescription()
-           .cuda_compute_capability()
-           .IsAtLeast(8)) {
-    return;
-  }
-  constexpr char kHloBase[] = R"(
-HloModule TestModule, entry_computation_layout={(s8[4,48,48,64]{3,2,1,0},s8[64,3,3,64]{3,2,1,0},s8[64]{0})->s8[4,48,48,64]{3,2,1,0}}
-
-ENTRY TestComputation {
-  input = s8[4,48,48,64]{3,2,1,0} parameter(0)
-  filter = s8[64,3,3,64]{3,2,1,0} parameter(1)
-  bias = s8[64]{0} parameter(2)
-  convert.1 = f32[64]{0} convert(bias)
-  cudnn-conv-bias-activation.3 = (s8[4,48,48,64]{3,2,1,0}, u8[0]{0}) custom-call(input, filter, convert.1),
-      window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target="__cudnn$convBiasActivationForward",
-      backend_config="{\"activation_mode\":\"2\",\"conv_result_scale\":1,\"side_input_scale\":0,\"algorithm\":{
-        \"algo_id\":\"38\",\"math_type\":\"DEFAULT_MATH\",\"tuning_knobs\":{\"14\":\"5\",\"13\":\"1\",\"23\":\"0\",\"2\":\"1\"},
-        \"is_cudnn_frontend\":true,\"workspace_size\":\"0\"}}"
-  ROOT get-tuple-element.1 = s8[4,48,48,64]{3,2,1,0} get-tuple-element(cudnn-conv-bias-activation.3), index=0
-})";
-  constexpr char kHloVectorized[] = R"(
-HloModule TestModule, entry_computation_layout={(s8[4,48,48,64]{3,2,1,0},s8[64,3,3,64]{3,2,1,0},s8[64]{0})->s8[4,48,48,64]{3,2,1,0}}
-
-ENTRY TestComputation {
-  input = s8[4,48,48,64]{3,2,1,0} parameter(0)
-  bitcast.36 = s8[4,48,48,2,32]{4,3,2,1,0} bitcast(input)
-  transpose = s8[4,2,48,48,32]{4,3,2,1,0} transpose(bitcast.36), dimensions={0,3,1,2,4}
-  filter = s8[64,3,3,64]{3,2,1,0} parameter(1)
-  bitcast.18 = s8[64,3,3,2,32]{4,3,2,1,0} bitcast(filter)
-  transpose.3 = s8[64,2,3,3,32]{4,3,2,1,0} transpose(bitcast.18), dimensions={0,3,1,2,4}
-  bias = s8[64]{0} parameter(2)
-  convert.2 = f32[64]{0} convert(bias)
-  custom-call.3 = (s8[64,2,3,3,32]{4,3,2,1,0}, f32[64]{0}) custom-call(transpose.3, convert.2), custom_call_target="__cudnn$convReorderFilterAndBias"
-  get-tuple-element.2 = s8[64,2,3,3,32]{4,3,2,1,0} get-tuple-element(custom-call.3), index=0
-  get-tuple-element.3 = f32[64]{0} get-tuple-element(custom-call.3), index=1
-  cudnn-conv-bias-activation.4 = (s8[4,2,48,48,32]{4,3,2,1,0}, u8[51328]{0}) custom-call(transpose, get-tuple-element.2, get-tuple-element.3),
-      window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target="__cudnn$convBiasActivationForward",
-      backend_config="{\"activation_mode\":\"2\",\"conv_result_scale\":1,\"side_input_scale\":0,\"algorithm\":{
-        \"algo_id\":\"7\",\"math_type\":\"DEFAULT_MATH\",\"tuning_knobs\":{\"7\":\"3\",\"2\":\"0\",\"5\":\"4\",\"6\":\"4\",\"4\":\"2\",\"21\":\"0\"},
-        \"is_cudnn_frontend\":true,\"workspace_size\":\"51328\"},\"reordered_int8_nchw_vect\":true}"
-  get-tuple-element.6 = s8[4,2,48,48,32]{4,3,2,1,0} get-tuple-element(cudnn-conv-bias-activation.4), index=0
-  transpose.4 = s8[4,48,48,2,32]{4,3,2,1,0} transpose(get-tuple-element.6), dimensions={0,2,3,1,4}
-  ROOT bitcast.1 = s8[4,48,48,64]{3,2,1,0} bitcast(transpose.4)
-})";
-  EXPECT_TRUE(RunAndCompareTwoModules(kHloBase, kHloVectorized, ErrorSpec{0, 0},
-                                      /*run_hlo_passes=*/false));
-}
-
 }  // namespace
 }  // namespace xla
diff --git a/tensorflow/compiler/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc b/tensorflow/compiler/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
index a25931e6fce..33de736599c 100644
--- a/tensorflow/compiler/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
+++ b/tensorflow/compiler/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
@@ -1233,10 +1233,10 @@ LhloDialectEmitter::EmitDnnConvolutionReorderVectorized(
     }

     CHECK_EQ(shape.rank(), 5);
-    CHECK_EQ(shape.dimensions(4), 32);
+    CHECK_EQ(shape.dimensions_minor(0), 32);
     llvm::SmallVector<int64_t, 4> nchw = {
-        shape.dimensions(0), shape.dimensions(1) * 32, shape.dimensions(2),
-        shape.dimensions(3)};
+        shape.dimensions_minor(4), shape.dimensions_minor(3) * 32,
+        shape.dimensions_minor(2), shape.dimensions_minor(1)};
     op->setAttr("filter_dims", GetI64DenseElementsAttr(nchw));

     return op.getOperation();
diff --git a/tensorflow/compiler/xla/xla.proto b/tensorflow/compiler/xla/xla.proto
index 78e4706edfe..9f5b65a5858 100644
--- a/tensorflow/compiler/xla/xla.proto
+++ b/tensorflow/compiler/xla/xla.proto
@@ -462,9 +462,7 @@ message DebugOptions {

   bool xla_gpu_enable_triton_gemm = 188;

-  bool xla_gpu_enable_cudnn_int8x32_convolution_reordering = 189;
-
-  // Next id: 190
+  // Next id: 189

   // Extra options to pass to the compilation backend (e.g. LLVM); specific
   // interpretation of these values is left to the backend.
