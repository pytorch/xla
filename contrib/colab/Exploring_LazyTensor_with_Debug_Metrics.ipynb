{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06e73109",
      "metadata": {
        "id": "06e73109"
      },
      "source": [
        "# Setup PyTorch/XLA Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f6b658",
      "metadata": {
        "id": "89f6b658"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Environment variable for profiling / debug\n",
        "os.environ['PT_XLA_DEBUG'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WJ5qowFp1yJP",
      "metadata": {
        "id": "WJ5qowFp1yJP"
      },
      "outputs": [],
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==1.13.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.13-cp38-cp38-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5VDH-vKP1q9u",
      "metadata": {
        "id": "5VDH-vKP1q9u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aa70226",
      "metadata": {
        "id": "8aa70226"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e55acb83",
      "metadata": {
        "id": "e55acb83"
      },
      "source": [
        "## Create XLA Tensor \n",
        "\n",
        "For illustration, perform operations with XLA tensor(s), and view HLO Graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20d93911",
      "metadata": {
        "id": "20d93911"
      },
      "outputs": [],
      "source": [
        "dev = xm.xla_device()\n",
        "\n",
        "x1 = torch.rand((3, 3)).to(dev)\n",
        "x2 = torch.rand((3, 8)).to(dev)\n",
        "\n",
        "y1 = torch.einsum('bs,st->bt', x1, x2)\n",
        "y1 = y1 + x2\n",
        "print(torch_xla._XLAC._get_xla_tensors_text([y1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RdNpPCL_eYUp",
      "metadata": {
        "id": "RdNpPCL_eYUp"
      },
      "source": [
        "Notice that XLA Tensors are \"Lazy\", i.e. The operations have been recorded but no computation/execution actually is done until required.\n",
        "\n",
        "The execution is done when a LazyTensor Barrier is inserted.\n",
        "The easiest way to insert a barrier is mark_step() call:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b85ebc0",
      "metadata": {
        "id": "6b85ebc0"
      },
      "source": [
        "## Exploring LazyTensor with Debug Metrics\n",
        "Report the metrics and counters, and notice that no compilation has been performed yet, nor the graph has been executed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e09e1ff",
      "metadata": {
        "id": "7e09e1ff"
      },
      "outputs": [],
      "source": [
        "# Print all available metrics \n",
        "print(f\"Available metrics:\\n {met.metric_names()}\")\n",
        "# Print all available counters\n",
        "print(f\"Available counters:\\n {met.counter_names()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c137c446",
      "metadata": {
        "id": "c137c446"
      },
      "source": [
        "## Graph Execution Scenarios - 1\n",
        "\n",
        "The simplest where LazyTensor barrier is inserted triggers execution of graph(s) recorded so far is to call the mark_step API explicitly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee9a743c",
      "metadata": {
        "id": "ee9a743c"
      },
      "outputs": [],
      "source": [
        "xm.mark_step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kOohNpTYoOrR",
      "metadata": {
        "id": "kOohNpTYoOrR"
      },
      "source": [
        "Let's review the available metrics after the mark step call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f043601d",
      "metadata": {
        "id": "f043601d"
      },
      "outputs": [],
      "source": [
        "# Print all available metrics \n",
        "print(f\"Available metrics:\\n {met.metric_names()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qjRW_OKjoasv",
      "metadata": {
        "id": "qjRW_OKjoasv"
      },
      "source": [
        "Note that we see the CompileTime metric available now. This metrics can provide the details of Compilation Times distribution for all the graph compilations executed so far. However, here we are only interested in the number of times the compilations happens, we can report it as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce416d37",
      "metadata": {
        "id": "ce416d37"
      },
      "outputs": [],
      "source": [
        "met.metric_data('CompileTime')[:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d45d908",
      "metadata": {
        "id": "4d45d908"
      },
      "source": [
        "## Execution Scenario - 2\n",
        "Another scenario, where the LazyTensor Barrier is inserted is when PyTorch/XLA encounters an OP with no XLA lowering. Let's examine this scenario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86050e62",
      "metadata": {
        "id": "86050e62"
      },
      "outputs": [],
      "source": [
        "y1 = y1.view(3, 1, 2, 4)\n",
        "# Example op with no XLA lowering\n",
        "unfold = nn.Unfold(kernel_size=(2, 3))\n",
        "y2 =  unfold(y1)\n",
        "y4 = y2 * 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee2c95d",
      "metadata": {
        "id": "1ee2c95d"
      },
      "source": [
        "Notice that an additional compilation is triggered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c546dcb3",
      "metadata": {
        "id": "c546dcb3"
      },
      "outputs": [],
      "source": [
        "met.metric_data('CompileTime')[:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JEjG0VU6pi3Z",
      "metadata": {
        "id": "JEjG0VU6pi3Z"
      },
      "source": [
        "Notice also the counters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nwRyaPm3pha5",
      "metadata": {
        "id": "nwRyaPm3pha5"
      },
      "outputs": [],
      "source": [
        "print(f\"Available counters:\\n {met.counter_names()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e2bfa93",
      "metadata": {
        "id": "8e2bfa93"
      },
      "source": [
        "## PyTorch/XLA Profiler\n",
        "In the remainder of this notebook we will explore how PyTorch/XLA profiler can help surface these metrics insights without writing any additional line of code.\n",
        "\n",
        "Note: We alter the lower level variables to display the debug info which will by default be printed on your terminal (can be captured in the logfile). It is intended for educational purposes and is not the recommended way to use the profiler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276316b9",
      "metadata": {
        "id": "276316b9"
      },
      "outputs": [],
      "source": [
        "from torch_xla.debug.frame_parser_util import process_frames"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c985a52",
      "metadata": {
        "id": "8c985a52"
      },
      "source": [
        "Example stack trace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "223f64c3",
      "metadata": {
        "id": "223f64c3"
      },
      "outputs": [],
      "source": [
        "debug_file = torch_xla._tmp_fname\n",
        "process_frames(debug_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5905ace9",
      "metadata": {
        "id": "5905ace9"
      },
      "outputs": [],
      "source": [
        "y4 = y4.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ec76730",
      "metadata": {
        "id": "9ec76730"
      },
      "source": [
        "## Device to host transfer\n",
        "Now let's create a device to host transfer scenario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d70c301d",
      "metadata": {
        "id": "d70c301d"
      },
      "outputs": [],
      "source": [
        "print(y4[0].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba90288",
      "metadata": {
        "id": "7ba90288"
      },
      "outputs": [],
      "source": [
        "# Print all available counters\n",
        "print(f\"Available counters:\\n {met.counter_names()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5145c93f",
      "metadata": {
        "id": "5145c93f"
      },
      "outputs": [],
      "source": [
        "print(met.counter_value('aten::_local_scalar_dense'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53b620cd",
      "metadata": {
        "id": "53b620cd"
      },
      "outputs": [],
      "source": [
        "process_frames(debug_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F_S_eXK1qfAd",
      "metadata": {
        "id": "F_S_eXK1qfAd"
      },
      "source": [
        "Notice that device to host transfer are reported in terms of _local_scalar_dense op. In the usual seting PyTorch/XLA profiler would provide you the full stack-trace leading to lines in your code which are causing device to host transfers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6An3HV34q0La",
      "metadata": {
        "id": "6An3HV34q0La"
      },
      "source": [
        "# Summary\n",
        "In this notebook we have explored the LazyTensor behavior with some basic metrics and briefly also experiemented with some of the functionalities of PyTorch/XLA profiler. To explore other features of Pytorch/XLA profiler please review:\n",
        "- [Blog Posts](https://cloud.google.com/blog/topics/developers-practitioners/pytorchxla-performance-debugging-tpu-vm-part-1)\n",
        "- [Contrib Notebooks](https://github.com/pytorch/xla.git)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Exploring LazyTensor-with-Debug-Metrics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
