{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[PT Devcon 2019] PyTorch/TPU ResNet18/CIFAR10",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX1hxqUQn47M",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch/TPU ResNet18/CIFAR10 Demo\n",
        "\n",
        "This colab example corresponds to the implementation under [test_train_cifar.py](https://github.com/pytorch/xla/blob/master/test/test_train_cifar.py) and is TF/XRT 1.15 compatible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOlFL-Sn5cq",
        "colab_type": "text"
      },
      "source": [
        "<h3>  &nbsp;&nbsp;Use Colab Cloud TPU&nbsp;&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a></h3>\n",
        "\n",
        "* On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "* The cell below makes sure you have access to a TPU on Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLQPoJ6Fn8wF",
        "colab_type": "text"
      },
      "source": [
        "### [RUNME] Install Colab TPU compatible PyTorch/TPU wheels and dependencies\n",
        "This may take up to ~2 minutes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O53lrJMDn9Rd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIST_BUCKET=\"gs://tpu-pytorch/wheels\"\n",
        "TORCH_WHEEL=\"torch-1.15-cp36-cp36m-linux_x86_64.whl\"\n",
        "TORCH_XLA_WHEEL=\"torch_xla-1.15-cp36-cp36m-linux_x86_64.whl\"\n",
        "TORCHVISION_WHEEL=\"torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\"\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!sudo apt-get install libomp5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rroH9yiAn-XE",
        "colab_type": "text"
      },
      "source": [
        "### Define Parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMdPRFXIn_jH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "data_dir = \"/tmp/cifar\" #@param {type:\"string\"}\n",
        "batch_size = 128 #@param {type:\"integer\"}\n",
        "num_workers = 4 #@param {type:\"integer\"}\n",
        "learning_rate = 0.1 #@param {type:\"number\"}\n",
        "momentum = 0.9 #@param {type:\"number\"}\n",
        "num_epochs = 20 #@param {type:\"integer\"}\n",
        "num_cores = 8 #@param [8, 1] {type:\"raw\"}\n",
        "log_steps = 20 #@param {type:\"integer\"}\n",
        "metrics_debug = False #@param {type:\"boolean\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Micd3xZvoA-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion * planes:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(\n",
        "              in_planes,\n",
        "              self.expansion * planes,\n",
        "              kernel_size=1,\n",
        "              stride=stride,\n",
        "              bias=False), nn.BatchNorm2d(self.expansion * planes))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "  def __init__(self, block, num_blocks, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_planes = 64\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "    self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride):\n",
        "    strides = [stride] + [1] * (num_blocks - 1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_planes, planes, stride))\n",
        "      self.in_planes = planes * block.expansion\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = F.avg_pool2d(out, 4)\n",
        "    out = torch.flatten(out, 1)\n",
        "    out = self.linear(out)\n",
        "    return F.log_softmax(out, dim=1)\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "  return ResNet(BasicBlock, [2, 2, 2, 2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vMl96KLoCq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "norm = transforms.Normalize(\n",
        "    mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
        "inv_norm = transforms.Normalize(\n",
        "    mean=(-0.4914/0.2023, -0.4822/0.1994, -0.4465/0.2010),\n",
        "    std=(1/0.2023, 1/0.1994, 1/0.2010))  # For visualizations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    norm,\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    norm,\n",
        "])\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root=data_dir,\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root=data_dir,\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers)\n",
        "\n",
        "devices = (\n",
        "    xm.get_xla_supported_devices(\n",
        "        max_devices=num_cores) if num_cores != 0 else [])\n",
        "print(\"Devices: {}\".format(devices))\n",
        "# Scale learning rate to num cores\n",
        "learning_rate = learning_rate * max(len(devices), 1)\n",
        "# Pass [] as device_ids to run using the PyTorch/CPU engine.\n",
        "model_parallel = dp.DataParallel(ResNet18, device_ids=devices)\n",
        "\n",
        "def train_loop_fn(model, loader, device, context):\n",
        "  loss_fn = nn.NLLLoss()\n",
        "  optimizer = context.getattr_or(\n",
        "      'optimizer',\n",
        "      lambda: optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                        momentum=momentum, weight_decay=5e-4))\n",
        "  tracker = xm.RateTracker()\n",
        "\n",
        "  model.train()\n",
        "  for x, (data, target) in loader:\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = loss_fn(output, target)\n",
        "    loss.backward()\n",
        "    xm.optimizer_step(optimizer)\n",
        "    tracker.add(batch_size)\n",
        "    if x % log_steps == 0:\n",
        "      print('[{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}\\n'.format(\n",
        "          device, x, loss.item(), tracker.rate(),\n",
        "          tracker.global_rate(), time.asctime()), flush=True)\n",
        "\n",
        "def test_loop_fn(model, loader, device, context):\n",
        "  total_samples = 0\n",
        "  correct = 0\n",
        "  model.eval()\n",
        "  data, pred, target = None, None, None\n",
        "  for x, (data, target) in loader:\n",
        "    output = model(data)\n",
        "    pred = output.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    total_samples += data.size()[0]\n",
        "\n",
        "  accuracy = 100.0 * correct / total_samples\n",
        "  print('[Epoch {}] Global accuracy: {:.2f}%\\n'.format(epoch, accuracy))\n",
        "  return accuracy, data, pred, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2nL4HmloEyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start training\n",
        "data, target, pred = None, None, None\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  model_parallel(train_loop_fn, train_loader)\n",
        "  results = model_parallel(test_loop_fn, test_loader)\n",
        "  results = np.array(results)  # for ease of slicing\n",
        "  accuracies, data, pred, target = \\\n",
        "    [results[:,i] for i in range(results.shape[-1])]\n",
        "  accuracy = sum(accuracies) / len(accuracies)\n",
        "  print('Global accuracy: {:.2f}%'.format(accuracy))\n",
        "  if metrics_debug:\n",
        "    print(torch_xla._XLAC._xla_metrics_report())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wt7wEVJoFmf",
        "colab_type": "text"
      },
      "source": [
        "## Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSdVUMPjoGhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "CIFAR10_LABELS = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Retrive tensors that is on TPU cores\n",
        "M = 4\n",
        "N = 6\n",
        "images, labels, preds = data[0][:M*N].cpu(), \\\n",
        "  target[0][:M*N].cpu(), pred[0][:M*N].cpu()\n",
        "\n",
        "fig, ax = plt.subplots(M, N, figsize=(16, 9))\n",
        "plt.suptitle('Correct / Predicted Labels (Red text for incorrect ones)')\n",
        "\n",
        "for i, ax in enumerate(fig.axes):\n",
        "  img, label, prediction = images[i], labels[i], preds[i]\n",
        "  img = inv_norm(img)\n",
        "  img = img.permute(1, 2, 0) # (C, M, N) -> (M, N, C)\n",
        "  ax.axis('off')\n",
        "  label, prediction = label.item(), prediction.item()\n",
        "  if label == prediction:\n",
        "    ax.set_title(u'\\u2713', color='blue', fontsize=22)\n",
        "  else:\n",
        "    ax.set_title(\n",
        "        'X {}/{}'.format(CIFAR10_LABELS[label],\n",
        "                         CIFAR10_LABELS[prediction]), color='red')\n",
        "  ax.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
