{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wu3njRHLcIcr"
   },
   "source": [
    "# Using The CachedDataset In Resource Constrained Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKMmNL09cakQ"
   },
   "source": [
    "Many times a ML model training happen on exactly the same dataset, with exactly the same transofrmations happening on the raw data.\n",
    "\n",
    "When the transformations applied to the raw data require considerable amount of CPU and/or RAM resources, and when the environment is scarse on those resources, it is possible to trade CPU/RAM with storage/network by using a *CachedDataset*.\n",
    "\n",
    "A *CachedDataset* wraps any existing *PyTorch* *Dataset*, by transparently caching the training samples, so that after the dataset is fully cached, there won't be any more CPU/RAM resources used to process it.\n",
    "\n",
    "A *CachedDataset* can also reveal itself useful even in cases where there is enough CPU/RAM available, as if the raw data processing performed from the input pipeline is heavy, there will still benefit in loading from storage the cooked data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sPJVqAKyml5W"
   },
   "outputs": [],
   "source": [
    "VERSION = \"2.0\"  #@param [\"1.13\", \"nightly\", \"20220315\"]  # or YYYYMMDD format\n",
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "!python pytorch-xla-env-setup.py --version $VERSION\n",
    "import os \n",
    "os.environ['LD_LIBRARY_PATH']='/usr/local/lib'\n",
    "!echo $LD_LIBRARY_PATH\n",
    "\n",
    "!sudo ln -s /usr/local/lib/libmkl_intel_lp64.so /usr/local/lib/libmkl_intel_lp64.so.1\n",
    "!sudo ln -s /usr/local/lib/libmkl_intel_thread.so /usr/local/lib/libmkl_intel_thread.so.1\n",
    "!sudo ln -s /usr/local/lib/libmkl_core.so /usr/local/lib/libmkl_core.so.1\n",
    "\n",
    "!ldconfig\n",
    "!ldd /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07WeJ77alpkR"
   },
   "source": [
    "A *CachedDataset* can be used transparently, by wrapping an existing *PyTorch* *Dataset*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEb7ZOKRlovh"
   },
   "outputs": [],
   "source": [
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.utils.cached_dataset as xcd\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def _mp_fn(index):\n",
    "  train_dataset = datasets.MNIST(\n",
    "      '/tmp/mnist-data',\n",
    "      train=True,\n",
    "      download=True,\n",
    "      transform=transforms.Compose(\n",
    "              [transforms.ToTensor(),\n",
    "               transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "  train_dataset = xcd.CachedDataset(train_dataset, '/tmp/cached-mnist-data')\n",
    "  # Here it follow the normal model code ...\n",
    "\n",
    "\n",
    "xmp.spawn(_mp_fn, args=(), start_method='fork', nprocs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZNDoQHstfsSu"
   },
   "source": [
    "Example use of populating a CachedDataset whose cache folder can be exported to other locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQ2_OcQxMEI8"
   },
   "outputs": [],
   "source": [
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.utils.cached_dataset as xcd\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def _mp_fn(index):\n",
    "  train_dataset = datasets.MNIST(\n",
    "      '/tmp/mnist-data',\n",
    "      train=True,\n",
    "      download=True,\n",
    "      transform=transforms.Compose(\n",
    "              [transforms.ToTensor(),\n",
    "               transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "  cached_dataset = xcd.CachedDataset(train_dataset, '/tmp/cached-mnist-data')\n",
    "  print('Warming up ...')  \n",
    "  cached_dataset.warmup()\n",
    "  print('Done!')\n",
    "\n",
    "\n",
    "xmp.spawn(_mp_fn, args=(), start_method='fork', nprocs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xE4KFwWak5Sp"
   },
   "source": [
    "The *CachedDataset* generated in **/tmp/cached-mnist-data** can then be packed and use in other setups.\n",
    "\n",
    "A *CachedDataset* uses the PyTorch serialization to save samples, so it is portable in every machine where PyTorch is.\n",
    "\n",
    "Simply use *tar* to pack it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kL9sxjZako-o"
   },
   "outputs": [],
   "source": [
    "!tar czf cached-mnist.tar.gz /tmp/cached-mnist-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UInwgBTAmR1p"
   },
   "source": [
    "The fully cached *CachedDataset* can then be used in other machines, even without the need of instantiating the existing *Dataset* (simply pass *None* as source *Dataset* object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-X8u5qauoXGk"
   },
   "outputs": [],
   "source": [
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.utils.cached_dataset as xcd\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "def _mp_fn(index):\n",
    "  train_dataset = xcd.CachedDataset(None, '/tmp/cached-mnist-data')\n",
    "  # Here it follow the normal model code ...\n",
    "\n",
    "\n",
    "xmp.spawn(_mp_fn, args=(), start_method='fork', nprocs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2g56bHZoum1"
   },
   "source": [
    "The XLA CachedDataset implementation natively supports GCS (Google Cloud Storage) as storage destination/source.\n",
    "\n",
    "Simply prefix the paths with gs:// and make sure the proper environment is setup to access GCS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2aEnnjkpKY0"
   },
   "outputs": [],
   "source": [
    "!export GOOGLE_APPLICATION_CREDENTIALS=/PATH/TO/CREDENTIALS_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ur7GGOsdpQNI"
   },
   "outputs": [],
   "source": [
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.utils.cached_dataset as xcd\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def _mp_fn(index):\n",
    "  train_dataset = datasets.MNIST(\n",
    "      '/tmp/mnist-data',\n",
    "      train=True,\n",
    "      download=True,\n",
    "      transform=transforms.Compose(\n",
    "              [transforms.ToTensor(),\n",
    "               transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "  train_dataset = xcd.CachedDataset(train_dataset, 'gs://my_bucket/cached-mnist-data')\n",
    "  # Here it follow the normal model code ...\n",
    "\n",
    "\n",
    "xmp.spawn(_mp_fn, args=(), start_method='fork', nprocs=1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CachedDataset",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
